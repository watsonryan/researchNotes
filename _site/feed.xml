<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research Notes</title>
    <description></description>
    <link>https://watsonryan.github.io/researchNotes/</link>
    <atom:link href="https://watsonryan.github.io/researchNotes/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 06 Nov 2018 15:12:34 -0500</pubDate>
    <lastBuildDate>Tue, 06 Nov 2018 15:12:34 -0500</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Sensitivity of Posterior Distribution to Hyperparameters</title>
        <description>&lt;bf&gt; References: &lt;/bf&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1709.02536.pdf&quot;&gt; 1) Giordano, Ryan, Tamara Broderick, and Michael I. Jordan. “Covariances, robustness, and variational bayes.” arXiv preprint arXiv:1709.02536 (2017).&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://projecteuclid.org/euclid.aos/1033066205&quot;&gt; 2) Gustafson, Paul. “Local sensitivity of posterior expectations.” The Annals of Statistics 24.1 (1996): 174-195.&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;problem-setup&quot;&gt;Problem Setup&lt;/h1&gt;

&lt;p&gt;Lets say that we would like to estimate $\mathbf{\theta} \in \mathbb{R}^i$ given ${\mathbf{X}}_{j=1}^J$. When conducting this analysis within a Bayesian framework, we generally inject subjective into the analysis through the definition of prior and likelihood functions, which are simplifications of the true model. This subjectivity takes the form of hyperparameters, $\alpha \in \mathbf{A} \subseteq \mathbb{R}^k$, which affects the posterior distribution as defined below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{\theta} | \mathbf{X}, \mathbf{\alpha}) = \frac{p( \mathbf{X} | \mathbf{\theta}, \mathbf{\alpha}) p( \mathbf{\theta} | \mathbf{\alpha})}{p( \mathbf{X} | \mathbf{\alpha})}&lt;/script&gt;

&lt;p&gt;Utilizing the provided posterior distribution, we’re generally interested in calculating the expectation of some function ( $g(\theta)$ ,e.g., the mean ), $\mathbb{E}_{p\alpha}[g(\theta)]$. When conducting this calculation, we would to have a measure of the amount of influence that a permutation of $\alpha$ has on the expectation (i.e., how robust is the posterior distribution to changes in the provided hyperparamters).&lt;/p&gt;

&lt;h1 id=&quot;defining-sensitivity&quot;&gt;Defining Sensitivity&lt;/h1&gt;

&lt;p&gt;An intuitive way to calculate the global sensitivity of our estimator to variation in $\alpha$ would be to calculate the extrema of $\mathbb{E}_{p\alpha}[g(\theta)]$ over all $\alpha \in \mathbf{A}$; however, this is intractable in general. So, instead, we can calculate the local sensitivity around an origin, $\alpha_o \in \mathbf{A}$ to small variations in $\alpha$, as defined below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_{\alpha o} = \frac{d \mathbb{E}_{p\alpha}[g(\theta)] }{d \alpha} \quad  \Big|_{\alpha_o}&lt;/script&gt;

&lt;p&gt;This local measure of sensitivity can be extended to an approximate global measure (to the first order) as provided below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}_{p\alpha}[g(\theta)] \approx \mathbb{E}_{p\alpha o }[g(\theta)] + S^T_{\alpha o }(\alpha - \alpha_o)&lt;/script&gt;

&lt;h1 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Start working on software implementation&lt;br /&gt;
– Will conduct analysis on sensitivity of clustering to Wishart hyperparamters.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 06 Nov 2018 00:00:00 -0500</pubDate>
        <link>https://watsonryan.github.io/researchNotes/_projects/dirichletrobustposegraph/2018/11/06/Update.html</link>
        <guid isPermaLink="true">https://watsonryan.github.io/researchNotes/_projects/dirichletrobustposegraph/2018/11/06/Update.html</guid>
        
        <category>dirichletRobustPoseGraph</category>
        
        <category>Sensitivity Analysis</category>
        
        
        <category>_projects</category>
        
        <category>dirichletRobustPoseGraph</category>
        
      </item>
    
      <item>
        <title>Variational Inference Characterization</title>
        <description>&lt;p&gt;To transition our robust optimization approach from a batch estimation technique to one that can run incremental, we need a fast, efficient clustering technique. One such technique is an approximate inference methodology known as variational inference. Specifically, we will utilize variational inference with a Dirichlet prior over the categorical distribution.&lt;/p&gt;

&lt;p&gt;We have implemented the Dirichlet variational Bayes clustering approach, and would like to begin quantifying its ability to accurately characterize the provided data. As an initial test, we will run our algorithm over a unimodal Gaussian data set (i.e., $X \sim \mathcal{N}(10,2)$), where we are interested in the accuracy and run-time of our estimator as the cardinality of the data-set is varied, as provided in Fig. 1.&lt;/p&gt;

&lt;p&gt;From Fig. 1, we can see that the run-time of the variational Bayes clustering seems to increase exponentially with the cardinality of the data-set. Additionally, we can note that our mean estimate seems to converge rather quickly to steady-state accuracy level. However, what’s disconcerting – specifically for our problem – is that an accurate characterization of the covariance matrix seems to require a rather large data-set.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/lUaWcjHXwaGt6cVBEv_LNMHkNAg-dG-iep3IdVh42uwZ5NOkYRF6hoYVkuLJhju273b7wuIrddCuyaVqmUBFRs3ycqasvIQdGSeniSqVrNqDMiwV6jZBgyL1aDRtd6AfU9Qi6nZA2bwOG35ZZx9PqHKZLJT_CVn7WFet490pyaSDpMv5E-JDT59nLAI26wcLSC4lpPauFxcPvhldgvD6favnNYsnukJSLGOCXz4-tZdWP73Ubr6rt3fn2R6xPU3lQNmwmz_yonUH_833icztL4v6cwxWwtGFJh8GXESXla19yNYLspg8vOwZpM5MlrjwvNntSm5p7RSReHd3AGrDPJVB1F9navNApzFnQoY56PV3mNY5Kiyyv-Gn6rTR897Zy5Czf6Rxeh3nSTrVBUhRugeSDliAxZOyPMRpJQUxK0U0MiPMBgQh_gVMXzuV2UdPGMC2itaaUMz3Uyqlz3qahEMHkin_2otTimTicCaadBfvDgyVDxRBfaitXMTIsy32KB3jIqUaq6I3Veb34IgXCo7xtH_oi9zA4_3olR5mKdSikBgblBY-lHkHOXvZp69LXHgNnhqImx1UYyL2Bl_ruGnwN1aO0LmqIrecaZXXsh5hm2j2gftkaWvdr-VOFr8=w1920-h909-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/lUaWcjHXwaGt6cVBEv_LNMHkNAg-dG-iep3IdVh42uwZ5NOkYRF6hoYVkuLJhju273b7wuIrddCuyaVqmUBFRs3ycqasvIQdGSeniSqVrNqDMiwV6jZBgyL1aDRtd6AfU9Qi6nZA2bwOG35ZZx9PqHKZLJT_CVn7WFet490pyaSDpMv5E-JDT59nLAI26wcLSC4lpPauFxcPvhldgvD6favnNYsnukJSLGOCXz4-tZdWP73Ubr6rt3fn2R6xPU3lQNmwmz_yonUH_833icztL4v6cwxWwtGFJh8GXESXla19yNYLspg8vOwZpM5MlrjwvNntSm5p7RSReHd3AGrDPJVB1F9navNApzFnQoY56PV3mNY5Kiyyv-Gn6rTR897Zy5Czf6Rxeh3nSTrVBUhRugeSDliAxZOyPMRpJQUxK0U0MiPMBgQh_gVMXzuV2UdPGMC2itaaUMz3Uyqlz3qahEMHkin_2otTimTicCaadBfvDgyVDxRBfaitXMTIsy32KB3jIqUaq6I3Veb34IgXCo7xtH_oi9zA4_3olR5mKdSikBgblBY-lHkHOXvZp69LXHgNnhqImx1UYyL2Bl_ruGnwN1aO0LmqIrecaZXXsh5hm2j2gftkaWvdr-VOFr8=w1920-h909-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Initial characterization of Variational Bayes clustering on uni-modal Gaussian data.  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;One way that we maybe able to improve our covariance characterization is through the use of &lt;a href=&quot;https://arxiv.org/pdf/1506.04088.pdf&quot;&gt;Linear Response Variational Bayes&lt;/a&gt;. However, this approach assumes that the mean estimate provided by the estimator is accurate. To validate that this is true for our estimator, we will generate data from the same distribution as above; however, this time, we will run 100 test at each data-set cardinality. This will provide us with a rough estimate of the variance of our mean estimation as a function of data-set cardinality, as provided in Fig. 2.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/66zSOOV_wc2sKHrKbKnmLCR5zPiR-1g-7oNDvgp_zwE-Vwoxyy1JoMTALkQrC1rL4Vo9l-sb6T-jWP21kD-R8Mcz-usX8G1bQv5Y8-q0loF-zNWchlrxtWKt_oCFyq2Bk0EEEl7iefGqjXQu5hQ4inRQcfpvZrgywv0pGa0PYSYjdsGq_rVJn_fO-ri4xz426D3p0rgwLBh65iUwi_cY4DpgnL8NQrh5p5mTMGkjbrxcf79i9x7UkxcJ3r2XUo4CBHYMu07tEeTAjObY1BF5_J_PQWq15As_CKrRohXBO61psCMgL7pI051wmJDfuY9Juz3ouIijx13jTRkeBg-62U_3T59eY9ZK5PSwWLBq8abIlW3nKX5feJy_yi7UORs-CvxdB0pnjSzptlXbss_8GJVSDGZgqML0qOxynqhfU4BSyhJCLVHnA_G2jpX1E0V6EHI1qqhsNo10HT9oTuXZFAW9sKHpO0kW5pNw59LF65-cOzlB7U0CRWc0UgjgD_aa7oXbT4CP_ofBmjUf_qT677g5pIwgqx1oPyVTgkzlyjfn1m_KUBF2McrtIeg9_HZQpGOLlFh4GenWR8Mcs7OwFUb7HezPmkJgWD0FVXrAkPFUv-gdtnP233fHutwBc0E=w1920-h909-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/66zSOOV_wc2sKHrKbKnmLCR5zPiR-1g-7oNDvgp_zwE-Vwoxyy1JoMTALkQrC1rL4Vo9l-sb6T-jWP21kD-R8Mcz-usX8G1bQv5Y8-q0loF-zNWchlrxtWKt_oCFyq2Bk0EEEl7iefGqjXQu5hQ4inRQcfpvZrgywv0pGa0PYSYjdsGq_rVJn_fO-ri4xz426D3p0rgwLBh65iUwi_cY4DpgnL8NQrh5p5mTMGkjbrxcf79i9x7UkxcJ3r2XUo4CBHYMu07tEeTAjObY1BF5_J_PQWq15As_CKrRohXBO61psCMgL7pI051wmJDfuY9Juz3ouIijx13jTRkeBg-62U_3T59eY9ZK5PSwWLBq8abIlW3nKX5feJy_yi7UORs-CvxdB0pnjSzptlXbss_8GJVSDGZgqML0qOxynqhfU4BSyhJCLVHnA_G2jpX1E0V6EHI1qqhsNo10HT9oTuXZFAW9sKHpO0kW5pNw59LF65-cOzlB7U0CRWc0UgjgD_aa7oXbT4CP_ofBmjUf_qT677g5pIwgqx1oPyVTgkzlyjfn1m_KUBF2McrtIeg9_HZQpGOLlFh4GenWR8Mcs7OwFUb7HezPmkJgWD0FVXrAkPFUv-gdtnP233fHutwBc0E=w1920-h909-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Repeatability of Variational Bayes mean estimation as a function of data-set size.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Test VDB against collapsed Gibb’s sampling&lt;/li&gt;
  &lt;li&gt;Implement Linear Response Variational Bayes (LRVB).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Oct 2018 00:00:00 -0400</pubDate>
        <link>https://watsonryan.github.io/researchNotes/_projects/incrementalgraph/2018/10/16/Update.html</link>
        <guid isPermaLink="true">https://watsonryan.github.io/researchNotes/_projects/incrementalgraph/2018/10/16/Update.html</guid>
        
        <category>varitationlInf</category>
        
        <category>characterization</category>
        
        
        <category>_projects</category>
        
        <category>incrementalGraph</category>
        
      </item>
    
      <item>
        <title>Testing Mean and Covariance Equivalence</title>
        <description>&lt;p&gt;Ref:  &lt;a href=&quot;https://www.cs.nmsu.edu/~joemsong/publications/Song-SPIE2005-updated.pdf&quot;&gt;Song, Mingzhou, and Hongbin Wang. “Highly efficient incremental estimation of Gaussian mixture models for online data stream clustering.” &lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt;

&lt;p&gt;To  make our robust optimization scheme incremental, we need to implement an efficient clustering algorithm. This efficiency needs to be w.r.t. both computation and storage. This means that, ideally, we will only have to store a small batch of the most recent observations in memory.&lt;/p&gt;

&lt;p&gt;As an initial step to implement our incremental clustering approach, we need the ability to test the equivalence of mean vectors and covariance matrices of our current batch of observations against our prior mixture model. One method for testing this is presented below.&lt;/p&gt;

&lt;h2 id=&quot;covariance-test&quot;&gt;Covariance Test&lt;/h2&gt;

&lt;p&gt;To begin, let’s assume that we have a set of observations, $ { x_n } \in \mathbf{R}^d $. And we want to check if this set of observations has the same covariance as a hypothesis covariance matrix (i.e., we want to see if $\Sigma_x = \Sigma_0$, where $\Sigma_x = \text{cov}(x)$ and $\Sigma_0$ is our hypothesis).&lt;/p&gt;

&lt;p&gt;To do this, we must first transform our original data set with Cholesky decomposition of our hypothesis covariance ( the covariance test only works for unit covariance matrices ), as shown below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{ y_i \} = \{L_0^{-1} x_i \}, \ i=1,\cdots, n \quad \text{where} \quad \Sigma_0 = L_0 L_0^T&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Utilizing the transformed data set, we can construct the $W$-statistic, as shown below, which is known to have an asymptotic $\chi^2$ distribution with degrees of freedom $d(d+1)/2$.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{nWd}{2} \sim \chi^2_{d(d+1)/2} \qquad s.t. \qquad W = \frac{1}{d} Tr[(S_y - I)^2] - \frac{d}{n}[\frac{1}{d}Tr[S_y]]^2 + \frac{d}{n}&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mean-test&quot;&gt;Mean Test&lt;/h2&gt;

&lt;p&gt;To test the equivalence of mean vectors, we can construct the $T$-statistic, as shown below, which is known to have an asymptotic $F$ distribution.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{n-d}{d(n-1)}T^2 \sim F_{d,n-d} \qquad s.t. \qquad T^2 = n(\bar{x} - \mu_o)^T S^{-1} (\bar{x} - \mu_o)&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;test&quot;&gt;Test&lt;/h1&gt;

&lt;p&gt;As a simple test to validate our implementation, we utilized a simulated data set composed of several Gaussian components. The initial test is presented in the video below. From this simple test, we can see that we are able to distinguish when components in our steaming mixture model match our global mixture model.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;960&quot; height=&quot;720&quot; src=&quot;https://www.youtube.com/embed/b049i-O-iZI&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Testing mean and covariance equivalence on a simulated data set.
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 15 Oct 2018 00:00:00 -0400</pubDate>
        <link>https://watsonryan.github.io/researchNotes/_projects/incrementalgraph/2018/10/15/Update.html</link>
        <guid isPermaLink="true">https://watsonryan.github.io/researchNotes/_projects/incrementalgraph/2018/10/15/Update.html</guid>
        
        <category>varitationlInf</category>
        
        <category>incremental clustering</category>
        
        
        <category>_projects</category>
        
        <category>incrementalGraph</category>
        
      </item>
    
      <item>
        <title>Derivation of Cauchy M-Estimator</title>
        <description>&lt;p&gt;Ref:  &lt;a href=&quot;http://asrl.utias.utoronto.ca/~tdb/bib/barfoot_ser17.pdf&quot;&gt;Barfoot, Timothy D. State Estimation for Robotics. Cambridge University Press, 2017. Section 5.3&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;deriving-cauchy-m-estimator-for-map-estimation&quot;&gt;Deriving Cauchy M-Estimator For M.A.P Estimation&lt;/h1&gt;

&lt;p&gt;We would like to derive a common M-Estimator (Maximum Likelihood Type Estimator), the Cauchy cost function, from the point of view of covariance estimation. Specifically, we would like to derive it from the point-of-view of maximizing the a posteriori distribution.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We will begin with the traditional MAP estimation cost function.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(x) = \frac{1}{2} \sum_{n=1}^{N} r(x)^T \Sigma_n^{-1} r(x),&lt;/script&gt;

&lt;p&gt;where $r_n(x)$ is the residual of state $x$ at iteration $n$, and $\Sigma_n$ is the provided covariance at iteration $n$. This provides us with the traditional optimization problem, as provided below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{x^{*}\} = \text{arg min}_x \ J^{&#39;}(x) \ = \ \text{arg min}_x -\text{ln} \ p(x|z)&lt;/script&gt;

&lt;p&gt;In the optimization problem provided above, it is assumed that the covariance is provided a priori. However, this is not always a valid assumption, so, it is desirable to be able to estimation both the state vector and the covariance estimate concurrently. To do so, we can augment our optimization problem as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{x^*, \Sigma^*\} = \text{arg min}_{x,\Sigma} \ J^{&#39;}(x,\Sigma) = \ \text{arg min}_{x,\Sigma} -\text{ln} \ p(x|\Sigma, z)p(\Sigma),&lt;/script&gt;

&lt;p&gt;which can be factorized as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{x^*, \Sigma^*\} = \text{arg min}_{x,\Sigma} \prod_{n=1}^N \ p(x|\Sigma_n, z_n)p(\Sigma_n).&lt;/script&gt;

&lt;p&gt;Now, we need to provide a prior on our covariance matrix. A commonly utilized prior, due to the fact that it’s a conjugate proir to symmetric nonnegative-definite matrices, is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wishart_distribution&quot;&gt;Inverse Wishart distribution&lt;/a&gt;, which is defined as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_n \sim \mathcal{W}^{-1}(\Psi_n, \nu_n) \quad \text{if} \quad p(\Sigma_n) = \frac{|\Psi_n|^{\nu_n/2}}{2^{\frac{\nu_n\Sigma_n}{2}}\Gamma_{\Sigma_n(\nu_n/2)}} |\Sigma_n|^{-\frac{\nu_n + \Sigma_n +1}{2}} e^{-\frac{1}{2} Tr[\Psi_n\Sigma_n^{-1}]},&lt;/script&gt;

&lt;p&gt;where, $\nu_n$ is the degrees-of-freedom, and $\Psi_n$ is a scaling matrix.&lt;/p&gt;

&lt;p&gt;If we plug the inverse wishart prior into the factorized objective function, we are left with,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J^{&#39;}(x,\Sigma) = \frac{1}{2} \Sigma_{n=1}^N ( r_n(x)^T \Sigma_n^{-1} r_n(x) - [ \nu_n + \Sigma_n + 2 ] \text{ln}( |\Sigma_n^{-1}| ) ) + Tr[\Psi_n \Sigma_n].&lt;/script&gt;

&lt;p&gt;Now, to find the optimal covariance estimate, we can set the partial derivative of our objective function w.r.t. $\Sigma^{-1}$ equal to zero, as provided below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial{J^{&#39;}(x,\Sigma)}}{\partial{\Sigma_n^{-1}}}  = \frac{1}{2} r_n(x) r_n(x)^T - \frac{1}{2} [\nu_n + \Sigma_n +2] \Sigma_n + \frac{1}{2} \Psi_n&lt;/script&gt;

&lt;p&gt;Setting the expression provided above equal to zero, we are left with the optimal covariance estimate.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_n(x) = \frac{1}{\nu_n + \Sigma_n +2} \Psi_n + \frac{1}{\nu_n + \Sigma_n +2} r_n(x) r_n(x)^T&lt;/script&gt;

&lt;p&gt;Finally, if we plug our expression for the optimal $\Sigma_n$ back into the objective function, we are left with,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J^{&#39;}(x) = \frac{1}{2}\sum_{n=1}^N \alpha_n \ \text{ln} \ (1 + r_n(x)^T \Psi_n^{-1} r_n(x)),&lt;/script&gt;

&lt;p&gt;where $\alpha = \nu_n + \Sigma_n + 2$. Which is the same objective function as specified by the Cauchy M-estimator.&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Oct 2018 00:00:00 -0400</pubDate>
        <link>https://watsonryan.github.io/researchNotes/_projects/robustestdev/2018/10/05/Update.html</link>
        <guid isPermaLink="true">https://watsonryan.github.io/researchNotes/_projects/robustestdev/2018/10/05/Update.html</guid>
        
        <category>robustEstDev</category>
        
        <category>Cauchy</category>
        
        <category>M-estimator</category>
        
        
        <category>_projects</category>
        
        <category>robustEstDev</category>
        
      </item>
    
      <item>
        <title>Initial Test of Dense RGB-D SLAM in Greenhouse</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;testing-kintinuous&quot;&gt;Testing Kintinuous&lt;/h1&gt;

&lt;p&gt;During the last data collect, we attached a Kinect to the pollinatorBot. Using the Kinect data we can test the dense rgb-d slam. The implementation we’re currently testing is &lt;a href=&quot;https://github.com/mp3guy/Kintinuous&quot;&gt;Kintinuous&lt;/a&gt;. The real-time playback of the algorithm is shown below in the first video.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/Scls6cwd04I&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;b&gt; Inital test of Kintinuous on greenhouse dataset &lt;/b&gt; 
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;After optimizing, we can extract the final dense map estimate, as depicted in Fig. 1. As you would expect from the video above, the final map solution does not provide an accurate representation of the greenhouse environment.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/M2h7K56xK2Mbe2W5-Wzy_CG6AKE0tavMZxYnpWWJEs_tlyM-JEsHRII2r0PHsHj1aXDCRRYZEQtnkdFR9ldyfcb9iDDjBmm4exhqQhvCR8S2E3WyoiQiKzWOvxSi5B4QPsyALz24itHTANwSbZwn4NNfgOm7DCyhFV8lSoZuzxtYNeW-fKIgaLENvHH_Kl54FGiWH48JJoWHEZscisrRB5lyHJoD8wgvHBE3Ckg15EF74xv559M_jxzHEnzVDZ3PCNnm2p80qJKnbXfN6x303phmN8X3evwlFJchxsiXCwnNYx2_29iEJRNknlTQGJXYPYefo-3mIlqRzMiEzgXilbbKq5_MRll0HQKARlutHLDymvToqWXZ07SlM-HpLgUxxIkNVmM9qwU3MZcow5FLoZH4znOZ2Jnoz5hsRzVDey2QK2hw0qrsEEnRbYnFnRugLfnpmhwIaOxwEwmV7DT6VI4_wDPxZbom9Ic3EkaOB_llsTSdGT8oSVSrqvjWzs2jb9ax1GeNKrTH7M02VZ40rjWwMW8TcDQZiQQC7l_8VPdP8v0VL_xyiZTK3n8yGGPLyDOjkg13dFYkZzRWkyJ8hLTi9cIPFxUBefPO4s8zvI6DQhzGuVBG3wcsut3YE7f8DThJrGy-mShv2ez5DjIreJ0LhzYm9lCW83k=w1280-h751-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/M2h7K56xK2Mbe2W5-Wzy_CG6AKE0tavMZxYnpWWJEs_tlyM-JEsHRII2r0PHsHj1aXDCRRYZEQtnkdFR9ldyfcb9iDDjBmm4exhqQhvCR8S2E3WyoiQiKzWOvxSi5B4QPsyALz24itHTANwSbZwn4NNfgOm7DCyhFV8lSoZuzxtYNeW-fKIgaLENvHH_Kl54FGiWH48JJoWHEZscisrRB5lyHJoD8wgvHBE3Ckg15EF74xv559M_jxzHEnzVDZ3PCNnm2p80qJKnbXfN6x303phmN8X3evwlFJchxsiXCwnNYx2_29iEJRNknlTQGJXYPYefo-3mIlqRzMiEzgXilbbKq5_MRll0HQKARlutHLDymvToqWXZ07SlM-HpLgUxxIkNVmM9qwU3MZcow5FLoZH4znOZ2Jnoz5hsRzVDey2QK2hw0qrsEEnRbYnFnRugLfnpmhwIaOxwEwmV7DT6VI4_wDPxZbom9Ic3EkaOB_llsTSdGT8oSVSrqvjWzs2jb9ax1GeNKrTH7M02VZ40rjWwMW8TcDQZiQQC7l_8VPdP8v0VL_xyiZTK3n8yGGPLyDOjkg13dFYkZzRWkyJ8hLTi9cIPFxUBefPO4s8zvI6DQhzGuVBG3wcsut3YE7f8DThJrGy-mShv2ez5DjIreJ0LhzYm9lCW83k=w1280-h751-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Final map solution
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h4&gt;

&lt;p&gt;1) We used a Kinect 2.0, which is known to provided degraded data in an outdoor setting. We should test another rgb-d sensor.&lt;/p&gt;

&lt;p&gt;2) We can provide Kintinuous an initial pose-graph (i.e., provide it our lidar slam solution) so that it only has to reconstruct the dense map.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Oct 2017 00:00:00 -0400</pubDate>
        <link>https://watsonryan.github.io/researchNotes/_projects/pollinatorbot/2017/10/09/Update.html</link>
        <guid isPermaLink="true">https://watsonryan.github.io/researchNotes/_projects/pollinatorbot/2017/10/09/Update.html</guid>
        
        <category>pollinatorBot</category>
        
        <category>bumble bot</category>
        
        <category>slam</category>
        
        <category>rgb-d</category>
        
        <category>blam test</category>
        
        
        <category>_projects</category>
        
        <category>pollinatorBot</category>
        
      </item>
    
      <item>
        <title>Occupancy Grid Generation</title>
        <description>&lt;h2 id=&quot;occupancy-grid-generation-from-3d-slam-map&quot;&gt;Occupancy Grid Generation from 3D SLAM Map&lt;/h2&gt;

&lt;p&gt;To test our ability to generate an accurate occupancy grid, we will utilize the data-set collected in the green house on 10/3/17. For now, we are only concerned with a 2d occupancy grid to simplify the planning algorithms. To construct a 2d occupancy grid we can compress the 3d structure generated by our SLAM algorithm, as shown in the first video, into a 2d occupancy grid.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/xc8gBKzkVDM&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
SLAM solution with new greenhouse dataset 
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Initially we tried to compress the whole 3d structure to generate an occupancy grid as provided in the second video. As can be seen, the quality of the generated occupancy grid is of poor quality.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/V52x5girnSs&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
First attempt at occupancy grid generation 
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In an attempt to generate a higher fidelity occupancy grid, we can restrict the amount of the 3d structure that we compress (i.e., we know that robot’s height, so we don’t need to be concerned about occupied grid points above it). Using this technique the cost map was generated again, as provided in the third video. As can be seen, this map is much high fidelity.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/DRhahWNKARo&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Second attempt at occupancy grid generation 
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;next-steps-&quot;&gt;Next Steps :&lt;/h3&gt;

&lt;p&gt;Next, I will try to do some clustering over the occupancy grid and calculate the convex hull of each blob to make it an easier environment for path planning. &lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gnss-data&quot;&gt;GNSS Data&lt;/h2&gt;

&lt;p&gt;In addition to lidar, we also have a NovAtel SPAN system on board, which collectes GNSS/INS observables. To extract the NovAtel data from the ROS bag, the following three commands can be utilized, where the final file, gps.bin, can be converted to a RINEX file with NovAtels conversion software.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rostopic &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; -b ~/path/to/file -p /gps_data &amp;gt; gps.data
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$WVUPNG&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;./bin/rosBagToBin.py -i gps.data -o gps.bin
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;For some reason, on this data collect, we did not get a GNSS fix; however, we did collect INS data.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/jXTFq9aHQgN2i11DR0uuXoRFarQQCFwHsHfI1nbfetSLOdRQU5_hOXCgwGRbjilITjuKoyPABVJFD67tCZoRMdVGsi2cujaDEy72p4TqwP88yWnM_OMIwutMzapYtOLe-5rSb_Mr5TTmXK7xMYFzb8D2ndOBIJTJ16aMm9gvCrVAzvKTj3ZN_v8WKz8iKeQeKTbOv2ED3z4boUHlZAXr9G1jKglLOzHT9FRbnAtvuYrveeBxG0bEZ4XcEd5r2AZAot-t_MU-uNgTePMDaubf02Z9otv23zY0pUv6j7TWE79SQA-HsArgVxKOHDpLBVPQIg_9WHj-bV1NkhzVo0H19nr0RZWnMgB-1cajAajSi45p3FYBPt5ZlpRycYDdAOadQbjZzIFL3k87zgfXHwXFLbCn3w1rD_ILNfJIwAhWRJr9yHZxiawFYJUo1vEV1ZWBmIEzIk-Sv7WxoAAa1nJHO9317RltQgq78FPepkLM1Um-ecRFi7d0tUgsYOLtkLv2twLQK8PprwtaP3CUrlzbVUczw-bStPIizncyiesnWiS4AeKLH6tpdc56QU-0YMpiEIZq5wuhXAYVhLMtzuNvAF6TbosuGRe-vcH7sxD2Yx8889gu2nOYf841sseYFgo7t3vAQcdHckojW0zLX-6ADJGcKa_8KD0KO9c=w652-h168-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/jXTFq9aHQgN2i11DR0uuXoRFarQQCFwHsHfI1nbfetSLOdRQU5_hOXCgwGRbjilITjuKoyPABVJFD67tCZoRMdVGsi2cujaDEy72p4TqwP88yWnM_OMIwutMzapYtOLe-5rSb_Mr5TTmXK7xMYFzb8D2ndOBIJTJ16aMm9gvCrVAzvKTj3ZN_v8WKz8iKeQeKTbOv2ED3z4boUHlZAXr9G1jKglLOzHT9FRbnAtvuYrveeBxG0bEZ4XcEd5r2AZAot-t_MU-uNgTePMDaubf02Z9otv23zY0pUv6j7TWE79SQA-HsArgVxKOHDpLBVPQIg_9WHj-bV1NkhzVo0H19nr0RZWnMgB-1cajAajSi45p3FYBPt5ZlpRycYDdAOadQbjZzIFL3k87zgfXHwXFLbCn3w1rD_ILNfJIwAhWRJr9yHZxiawFYJUo1vEV1ZWBmIEzIk-Sv7WxoAAa1nJHO9317RltQgq78FPepkLM1Um-ecRFi7d0tUgsYOLtkLv2twLQK8PprwtaP3CUrlzbVUczw-bStPIizncyiesnWiS4AeKLH6tpdc56QU-0YMpiEIZq5wuhXAYVhLMtzuNvAF6TbosuGRe-vcH7sxD2Yx8889gu2nOYf841sseYFgo7t3vAQcdHckojW0zLX-6ADJGcKa_8KD0KO9c=w652-h168-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: GNSS Data Extraction   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Oct 2017 00:00:00 -0400</pubDate>
        <link>https://watsonryan.github.io/researchNotes/_projects/pollinatorbot/2017/10/04/Update.html</link>
        <guid isPermaLink="true">https://watsonryan.github.io/researchNotes/_projects/pollinatorbot/2017/10/04/Update.html</guid>
        
        <category>pollinatorBot</category>
        
        <category>bumble bot</category>
        
        <category>blam test</category>
        
        <category>occupancy grid</category>
        
        
        <category>_projects</category>
        
        <category>pollinatorBot</category>
        
      </item>
    
      <item>
        <title>ISAM2 Speed Test with BLAM</title>
        <description>&lt;h1 id=&quot;real-time-test-of-blam&quot;&gt;Real-time test of blam&lt;/h1&gt;

&lt;p&gt;Previously, we tested BLAM on a data set collected in the greenhouse; however, it was noted during that evaluation that the process was consuming excessive amounts of cpu and memory. To combat this issue, we testing the algorithm  when a relinearization threshold is set, which allows the optimizer to only relinearize variables whose linear delta magnitude is greater than the specified threshold. First, we can visually inspect the solution by evaluation the map generated, as provided in the video below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/A4VYfn8swbo&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Pose solution with incremental map updates when real-time data playback is provided 
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now, we can re-evaluate the cpu consumption of the algorithm. On the left-hand-side of Fig. 1, we can see the previous consumption of the algorithm, and on the right-hand-side of Fig. 1, we can see the consumption after varying the relinearization parameters. As can be see through Fig. 1 and the video provided above, the cpu consumption dropped considerable while to overall solution quality did not vary greatly.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/8Cz9HGbub8haW2JIW6ByfyrimAGjylkRckfiCAAeRJakadXHtK4o8m8v6_VbRRkIhphnO2Vjf-D32x6VBO1bghzP76gWaVWBpfG5PDr9hYcLUlzYMhQNleRym9C5cDnA7JAEzm6yNm5qK9fqtiy3ok-vSicTdOKy-4DMkxmp6O9wKZ_whRllRKxtQBkP6XQ3_cz-jearHyzIpfk7i88ieOxopZNy9KYv7vgrJ-0lBQVPlq80bMKVnZF49sscQMpcKh_BdmpqS-eDs1Oxp0Pv9U892c-7csDzoWHOZlfyWyvqi0s35hi6npS6r-C2E9h7FCJaJTE3sRrCrrt1ldGjPpeAxJTWO177SL_K8EeiXfocUc0Gc4I-NycclOotI4ryM0_uqGhq9biHW6FxHuqEfBVZsJ-9KBUO3i57T0AAZM6pleglOq6RS6cUjLRqcP_d0GGXnfiTc3wIMI8ndbK05otFwL9K_2LR5qlwHVzqyaI_2PIu01WjMrwIUZMRNYTL6ta8HTHLYG1D0asL17O8K1w_2_B0NmwfoYIRfLUdZaFy-wsoV8n3aJaSZ2mmAfYEmFzmcfQ2E221mNTOpLegeKS4f0JrMCt-6Yhj1o4In5i4GP0ujMCj1WPSWfyB3wLuplGm6534eFV2jeAsGNrvqGfJ_KsGV7ptxng=w1280-h429-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/8Cz9HGbub8haW2JIW6ByfyrimAGjylkRckfiCAAeRJakadXHtK4o8m8v6_VbRRkIhphnO2Vjf-D32x6VBO1bghzP76gWaVWBpfG5PDr9hYcLUlzYMhQNleRym9C5cDnA7JAEzm6yNm5qK9fqtiy3ok-vSicTdOKy-4DMkxmp6O9wKZ_whRllRKxtQBkP6XQ3_cz-jearHyzIpfk7i88ieOxopZNy9KYv7vgrJ-0lBQVPlq80bMKVnZF49sscQMpcKh_BdmpqS-eDs1Oxp0Pv9U892c-7csDzoWHOZlfyWyvqi0s35hi6npS6r-C2E9h7FCJaJTE3sRrCrrt1ldGjPpeAxJTWO177SL_K8EeiXfocUc0Gc4I-NycclOotI4ryM0_uqGhq9biHW6FxHuqEfBVZsJ-9KBUO3i57T0AAZM6pleglOq6RS6cUjLRqcP_d0GGXnfiTc3wIMI8ndbK05otFwL9K_2LR5qlwHVzqyaI_2PIu01WjMrwIUZMRNYTL6ta8HTHLYG1D0asL17O8K1w_2_B0NmwfoYIRfLUdZaFy-wsoV8n3aJaSZ2mmAfYEmFzmcfQ2E221mNTOpLegeKS4f0JrMCt-6Yhj1o4In5i4GP0ujMCj1WPSWfyB3wLuplGm6534eFV2jeAsGNrvqGfJ_KsGV7ptxng=w1280-h429-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: CPU consumption. (Left-hand-side) We have the cpu consumption for the inital test. (Right-hand-side) The cpu consuption when the relinearization threshold is set.
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we can look at the amount of memory required. In Fig. 2, we can see the previously reqiured amount of memory on the left and the required amount after varying the relinearization parameters. Again, it should be noted that there is a reduction in consumption.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/S5OL3s-X6lkkMhvpApIfA35rEVvwohApQMg2olGZ6P0ecAmDXvitOfd1vNE-gvCL-BZKgqQ7na2iUV2V7HOQEf2SzKTVXPIZVm_eHkhzSoKDm1-LzzcLJG_cICVK6NuKG3OZbGzJwgTxcb-emFPaji0_JmN4bBI0Rlvx9cXYRZCkvr_dAGtdQ9cFPbaIbdZ7Y5F2A4CIQPSuVIZJ4LfuONlPO1PXVf7CbQWE4xG7LVt0KYLrxdONAGuzPDsE1eHERafm5b9VGCFYweOsWNSeOH6UzrBUjpiA6IPuvvJ-3ItfndtXkDvrTICwImGBJbcTDErstLEDn9Y2Y4n4F8Vq4dmWAFthTvh33hFEXnhDxD6b511AEt1M60-CJ2nyGbhMJRK3tEzxuvKBucC2AH0jIoHVGQo4j93Wahem4FdbCv4LPf8rH9DawHwHDWve0vSW0MBIqlfxuND6j7OMQ_Ct4XH4orDIQPwcAeC1F5jgEFDsUcWNyXWdeFBzLa_qXy7HlMuKf8RYpZmqiYx4A2b4PdVjQ3D0_B2ugAQevpOy0uKYOgluRqSDESxwfXjg-TCKF4o17bQjPUZWnExDfcEHidaH8cIT76DGuxz5IzX6QRa0AtexEcPu1ftx8Iiz5iJhlDH35oZ5aqB877kMrScVmfIXkR4DdZHDZJE=w1280-h429-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/S5OL3s-X6lkkMhvpApIfA35rEVvwohApQMg2olGZ6P0ecAmDXvitOfd1vNE-gvCL-BZKgqQ7na2iUV2V7HOQEf2SzKTVXPIZVm_eHkhzSoKDm1-LzzcLJG_cICVK6NuKG3OZbGzJwgTxcb-emFPaji0_JmN4bBI0Rlvx9cXYRZCkvr_dAGtdQ9cFPbaIbdZ7Y5F2A4CIQPSuVIZJ4LfuONlPO1PXVf7CbQWE4xG7LVt0KYLrxdONAGuzPDsE1eHERafm5b9VGCFYweOsWNSeOH6UzrBUjpiA6IPuvvJ-3ItfndtXkDvrTICwImGBJbcTDErstLEDn9Y2Y4n4F8Vq4dmWAFthTvh33hFEXnhDxD6b511AEt1M60-CJ2nyGbhMJRK3tEzxuvKBucC2AH0jIoHVGQo4j93Wahem4FdbCv4LPf8rH9DawHwHDWve0vSW0MBIqlfxuND6j7OMQ_Ct4XH4orDIQPwcAeC1F5jgEFDsUcWNyXWdeFBzLa_qXy7HlMuKf8RYpZmqiYx4A2b4PdVjQ3D0_B2ugAQevpOy0uKYOgluRqSDESxwfXjg-TCKF4o17bQjPUZWnExDfcEHidaH8cIT76DGuxz5IzX6QRa0AtexEcPu1ftx8Iiz5iJhlDH35oZ5aqB877kMrScVmfIXkR4DdZHDZJE=w1280-h429-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Memory consumption. Left-hand-side) We have the cpu consumption for the inital test. (Right-hand-side) The cpu consuption when the relinearization threshold is set.
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Next, I’ll conduct a similar analysis on the data set collect outdoors. This will provide us with a GNSS ground truth to validate the localization provided by the lidar-based slam solution.&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Sep 2017 00:00:00 -0400</pubDate>
        <link>https://watsonryan.github.io/researchNotes/_projects/pollinatorbot/2017/09/18/Update.html</link>
        <guid isPermaLink="true">https://watsonryan.github.io/researchNotes/_projects/pollinatorbot/2017/09/18/Update.html</guid>
        
        <category>pollinatorBot</category>
        
        <category>bumble bot</category>
        
        <category>blam test</category>
        
        
        <category>_projects</category>
        
        <category>pollinatorBot</category>
        
      </item>
    
      <item>
        <title>Testing Robust Graph Optimization With Poor A-Priori Measurement Error Covariance </title>
        <description>&lt;h2 id=&quot;overly-confident-optimizer&quot;&gt;Overly Confident Optimizer&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As an initial test, we will utilize the unmodified ( i.e, no false constraints added, and no additional noise added to the graph ) Manhattan 3500 dataset.  To test the sensitivity of the optimization routine to the initial measurement error covariance, we provide the optimizer with a measurement error covariance that is much smaller ( $R_t * 1e^{-6}$ )  than the true distribution from which the errors are sampled. This will provide insight into the robustness of the algorithms to the $a-prior$ measurement error covariance.&lt;/p&gt;

&lt;p&gt;First, we will test the switchable constraint methodology. This is depicted in Fig. 1, where it can be seen that the optimization did not provide an accurate pose-graph estimate.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/aqsD-49RpHXtbC7jI-CvOBnmlWVtPD8D-pg8rVIEC4ovBQYPFeKHeRWDDMnEr7rN1XUx7qtldb3Xto-ZAufXzol7XpKEvkcLwUxzxPFBT2-_GmoHr2wPiITG4zQ0v4us9oSwUVZ90qpg80BY8uC4kEGVnEPsX6ppTbIqxhtwsFbKRRT8JMxJwNGiFk5AAkAqOGSap2IokCSXbkU9lzyY4LwEZsAVDLvzWgVY_9g8JIzyk8sEo7ZJO9eySBPKLApTXXlabHxM0pjtjERX4pCye1ZDIKU5HB_judWDYP46NpllvgK7FLeS7YOPnSsERrQM9qtsiKisVdONxLlP138jcPK8xXy-gUKuKbZUr3EgmJAY1dgH3hFEQojhlrbxJgdmrBCRWFH8-CIpT34LgRW02nyGAbiSRwDoXOR3cLc2mkVk48ynOL3zbw0r6N_b5r3G9l7UBDqQrWiNhy6aYsE5iPFEIGE0BOAtM4N_LOqB_0USrAWvgoFDAAncaMeHF-E3XjYyMESof_v4StTRkYi3t3bd4GFUupsD5a6FCRN8FXeNifvB8h4H7iOPWKV9aZbprHXLq6J6vd6bPbCwi7D2R06Pvo3Ov6BoDWsZVCEtCCYJfXH6TRp8Uv7vvuvSp5Q78CM3b-l798VVsFwsfkq5w7kPVZWfOfB5dtg=w1280-h633-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/aqsD-49RpHXtbC7jI-CvOBnmlWVtPD8D-pg8rVIEC4ovBQYPFeKHeRWDDMnEr7rN1XUx7qtldb3Xto-ZAufXzol7XpKEvkcLwUxzxPFBT2-_GmoHr2wPiITG4zQ0v4us9oSwUVZ90qpg80BY8uC4kEGVnEPsX6ppTbIqxhtwsFbKRRT8JMxJwNGiFk5AAkAqOGSap2IokCSXbkU9lzyY4LwEZsAVDLvzWgVY_9g8JIzyk8sEo7ZJO9eySBPKLApTXXlabHxM0pjtjERX4pCye1ZDIKU5HB_judWDYP46NpllvgK7FLeS7YOPnSsERrQM9qtsiKisVdONxLlP138jcPK8xXy-gUKuKbZUr3EgmJAY1dgH3hFEQojhlrbxJgdmrBCRWFH8-CIpT34LgRW02nyGAbiSRwDoXOR3cLc2mkVk48ynOL3zbw0r6N_b5r3G9l7UBDqQrWiNhy6aYsE5iPFEIGE0BOAtM4N_LOqB_0USrAWvgoFDAAncaMeHF-E3XjYyMESof_v4StTRkYi3t3bd4GFUupsD5a6FCRN8FXeNifvB8h4H7iOPWKV9aZbprHXLq6J6vd6bPbCwi7D2R06Pvo3Ov6BoDWsZVCEtCCYJfXH6TRp8Uv7vvuvSp5Q78CM3b-l798VVsFwsfkq5w7kPVZWfOfB5dtg=w1280-h633-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: Test Switchable Constraints When Poor Measurement Error Covariance is Provided 
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Next, we can look at the ability of the max-mixtures approach to handle the specified scenario. Again, it can seen that this optimization routine does not handle this scenario well, as depicted in the left-hand-side of Fig. 2. However, this method does perform slightly better than the switchable constraints method ( i.e., the structure of the graph is still present; however, the erroneous pose-estimates are corrupting the initial plot ), which can be seen by in the right-hand-side of Fig. 2.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/HZXyXnUFVeTpEVc3ByQTj8CpMHH-u7blvfVrGbnWfjEZHI28IlxHccBeezVZZHUUBYMrqTYfaYTSeAacsGjdBZDoDtLUz0nPdPbsV6SSS_y_gQqAPtT5Ozd9Ruyrmz-_yCMTtR1iSW-yMyH5Pjb1nPsoL_MQtqjJXrLo8BXODP77wPnjDSW4qw74YEGkzZqvLvXiLMPZ8fWCYvCWrqKlhCk_pUH5KDjvB3l5lvXD_yE9j1q74ZF_rFPv_t8ELkOCHrm0dFtQqWRHVaL02yzuhJyO-uF5z92WmfH0oRLprraJiatRBncSRvaYKo_b9FgHObhE7gikbtmIOtFgKscBgyzmeheJzqsnorN2-zZ6V9Q68BSOHQ5EoNKqfQycirPtDaTnXXixb7zxQybocEzpbIAgV_HTgKSEfQWG2meW1dTNDKwS--Xf-_gxHwhfWwWQ4Dw7BWIgqzXN0b0CPG2sUB6H33FvO-8KrpUyLKdPe74uRTPAvI9mF9NR0v98iKoGSOlLAjKAECTA8_9XOCWzMtTQygeyGOeHFcHW8ndl6TwWRxmlpgw8UptSmPTKujdQPRGS0hLVoLlQC1IWq8PMeCdl8D_bbTbRpbKHn35bkLnpMoPIjxBhcbdRGk7ky_YWPTxFIXWn37WyFY7rVTEeNddo1J_aakzGYvw=w1280-h633-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/HZXyXnUFVeTpEVc3ByQTj8CpMHH-u7blvfVrGbnWfjEZHI28IlxHccBeezVZZHUUBYMrqTYfaYTSeAacsGjdBZDoDtLUz0nPdPbsV6SSS_y_gQqAPtT5Ozd9Ruyrmz-_yCMTtR1iSW-yMyH5Pjb1nPsoL_MQtqjJXrLo8BXODP77wPnjDSW4qw74YEGkzZqvLvXiLMPZ8fWCYvCWrqKlhCk_pUH5KDjvB3l5lvXD_yE9j1q74ZF_rFPv_t8ELkOCHrm0dFtQqWRHVaL02yzuhJyO-uF5z92WmfH0oRLprraJiatRBncSRvaYKo_b9FgHObhE7gikbtmIOtFgKscBgyzmeheJzqsnorN2-zZ6V9Q68BSOHQ5EoNKqfQycirPtDaTnXXixb7zxQybocEzpbIAgV_HTgKSEfQWG2meW1dTNDKwS--Xf-_gxHwhfWwWQ4Dw7BWIgqzXN0b0CPG2sUB6H33FvO-8KrpUyLKdPe74uRTPAvI9mF9NR0v98iKoGSOlLAjKAECTA8_9XOCWzMtTQygeyGOeHFcHW8ndl6TwWRxmlpgw8UptSmPTKujdQPRGS0hLVoLlQC1IWq8PMeCdl8D_bbTbRpbKHn35bkLnpMoPIjxBhcbdRGk7ky_YWPTxFIXWn37WyFY7rVTEeNddo1J_aakzGYvw=w1280-h633-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 2 :: Test (unmodified) Max-Mixtures When Poor Measurement Error Covariance is Provided 
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we can evaluate how the non-parametric clustering extension to max-mixtures performs. From Fig. 3, we can see that the specified optimization routine is robust to the poor initial measurement error covariance. This allowed the methodology to estimate the graph accurately.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/JT5e_9lEHe6tp4e_nn76SmG-rZWFED7NfJJLnCR5NOalcdjibZuUoiUPjYllIJeBEwTuLU1R9qjybhoC2nuWTghybXr1Ln6u5T4ezuN6or4wr7M30b9_-6tPxdOwdAkYpAKjF90CrrmkhA8od1H2EUU3yXD-xuVD7DEd9EAZajdOjtTr9y-tV1YAFfv-tpVX1HikTzBAVVbFydDfmIbT8mb-hdfA6j9gE3gaGCRUWmEbGxVOURumM6Bp4hw-hApdcWZR-QIYSE_SBjlww8QhD9TrqAXfgJaoufij3g3qAq9UhJarTNWMVlXBQSKQ2qhYwfem-Yyr4EQqDxfhcy5KQS1mD22aaoLAnhPLkuCUT935nZCOWI1jPaum-H9wV3oJswZfGuPdusO9EusE8XN9wH_Tm1vjxkPeZmpjQIJX2wmkuhA4OQpzSKG0g36LOyyW7LtmtKgODmbfk1_zc-49FFO_PJaKg7I0QwVagRt63CQLoKBxrZRtQTulorxL8d-P3i0WE2HfqgTr9hLmTvkN2NK-dR_6EK7VhJkc-oh7a7mulgBzDVF9K3d5nKzbpEFmLF80JU_d1iKQGNSstssrW6mfB7BZnPekuBc9I8mYewcqNcTprQWxdDqSp-57mi16PmGujd4OwSdKOwOBQmQSP52ZnsuWBFq8OBI=w614-h303-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/JT5e_9lEHe6tp4e_nn76SmG-rZWFED7NfJJLnCR5NOalcdjibZuUoiUPjYllIJeBEwTuLU1R9qjybhoC2nuWTghybXr1Ln6u5T4ezuN6or4wr7M30b9_-6tPxdOwdAkYpAKjF90CrrmkhA8od1H2EUU3yXD-xuVD7DEd9EAZajdOjtTr9y-tV1YAFfv-tpVX1HikTzBAVVbFydDfmIbT8mb-hdfA6j9gE3gaGCRUWmEbGxVOURumM6Bp4hw-hApdcWZR-QIYSE_SBjlww8QhD9TrqAXfgJaoufij3g3qAq9UhJarTNWMVlXBQSKQ2qhYwfem-Yyr4EQqDxfhcy5KQS1mD22aaoLAnhPLkuCUT935nZCOWI1jPaum-H9wV3oJswZfGuPdusO9EusE8XN9wH_Tm1vjxkPeZmpjQIJX2wmkuhA4OQpzSKG0g36LOyyW7LtmtKgODmbfk1_zc-49FFO_PJaKg7I0QwVagRt63CQLoKBxrZRtQTulorxL8d-P3i0WE2HfqgTr9hLmTvkN2NK-dR_6EK7VhJkc-oh7a7mulgBzDVF9K3d5nKzbpEFmLF80JU_d1iKQGNSstssrW6mfB7BZnPekuBc9I8mYewcqNcTprQWxdDqSp-57mi16PmGujd4OwSdKOwOBQmQSP52ZnsuWBFq8OBI=w614-h303-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 3 :: Test D.P. Max-Mixtures When Poor Measurement Error Covariance is Provided 
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Sep 2017 00:00:00 -0400</pubDate>
        <link>https://watsonryan.github.io/researchNotes/_projects/dirichletrobustposegraph/2017/09/14/Update.html</link>
        <guid isPermaLink="true">https://watsonryan.github.io/researchNotes/_projects/dirichletrobustposegraph/2017/09/14/Update.html</guid>
        
        <category>dirichletRobustPoseGraph</category>
        
        
        <category>_projects</category>
        
        <category>dirichletRobustPoseGraph</category>
        
      </item>
    
      <item>
        <title>Inital SLAM Test on Gigabyte Board</title>
        <description>&lt;h1 id=&quot;real-time-test-of-blam&quot;&gt;Real-time test of blam&lt;/h1&gt;

&lt;p&gt;Today, we started looking into the run-time requirements of our SLAM implementation when processing on the Gigabyte motherboard. A video of the algorithms performance when real-time playback is provided is provided below. From the video, it can be seen that the algorithm is able to process the lidar data in real-time while providing a consistent pose and map estimate.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/6kwIqYxTCPw&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; align=&quot;center&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Pose solution with incremental map updates when real-time data playback is provided 
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;With other processing requirements on the platform, we have to be cognizant of the amount of processing and memory capabilities of the board being consumed by the algorithm. First, we look at the CPU consumption, as provided in Fig. 1. From the provided figure, it can be seen that a substantial percentage the the processing capability is being utilized. In the future, we will need to look into methods to reduce the cpu consumption.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/5TRhuPgtYiHUA4-32k3Z-NGCDGtGJAbkrNFx82ZXXgGwEynMmoyg4USlIGJRgo4vPGawPNB9jRv1Ap8TuxfvOtXZLOEoEFOS3C-X_B7gycDHRRfDG4UUaDbW9BIAFUY1NdczuALmKso6eCqPP60h09OHFNi8mys6xy9ZOyjn_eoVC_clIOEytFyflD35NrzWbyoQkJGfG0OghrtbhpPhB-Ji4KmMoee3hybmnCF2tbGf8sIb-S0uBVcnCv1QRMWqPURY29yP6k4mYCv4so4UaB9TIW4JyhBLe3wKCz-saunOkFyJwLAd6djSDiksomx7GkbQLyLSXUTa-DOFMiNgWN-6MZdCVC_6xlNG3xg3YJZ0zvsza8qBG_zZn1zfvtYGNR7o06KUNB7gO5XqFhGWLrGLkqq1pH7eg5Y-H7T_7i5_uTbhiebQeoy1KYMKoYwAXJVowFQrntoh2u0LIGMUXu1KYhy1UmZ4F13tuOKCidRd-VYFpr4vgCj995kDMJcfj_GB_-VP7NBtpLPGIxYJPZHNrCcRDk7n21kCUnKeqrebZwYu_KtP9Td7jZQnnRLga0TR22UlJZ--qzN_Fqdh1cw4NZvHJgt3MeIS-SyC0ASHZW4huJIr39FQmBJfXqmJY5Y-nBdy0Be3T09jZrivuA1zJ9ODl5v1Q6g=w1280-h857-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/5TRhuPgtYiHUA4-32k3Z-NGCDGtGJAbkrNFx82ZXXgGwEynMmoyg4USlIGJRgo4vPGawPNB9jRv1Ap8TuxfvOtXZLOEoEFOS3C-X_B7gycDHRRfDG4UUaDbW9BIAFUY1NdczuALmKso6eCqPP60h09OHFNi8mys6xy9ZOyjn_eoVC_clIOEytFyflD35NrzWbyoQkJGfG0OghrtbhpPhB-Ji4KmMoee3hybmnCF2tbGf8sIb-S0uBVcnCv1QRMWqPURY29yP6k4mYCv4so4UaB9TIW4JyhBLe3wKCz-saunOkFyJwLAd6djSDiksomx7GkbQLyLSXUTa-DOFMiNgWN-6MZdCVC_6xlNG3xg3YJZ0zvsza8qBG_zZn1zfvtYGNR7o06KUNB7gO5XqFhGWLrGLkqq1pH7eg5Y-H7T_7i5_uTbhiebQeoy1KYMKoYwAXJVowFQrntoh2u0LIGMUXu1KYhy1UmZ4F13tuOKCidRd-VYFpr4vgCj995kDMJcfj_GB_-VP7NBtpLPGIxYJPZHNrCcRDk7n21kCUnKeqrebZwYu_KtP9Td7jZQnnRLga0TR22UlJZ--qzN_Fqdh1cw4NZvHJgt3MeIS-SyC0ASHZW4huJIr39FQmBJfXqmJY5Y-nBdy0Be3T09jZrivuA1zJ9ODl5v1Q6g=w1280-h857-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: CPU consumption   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Next, we can look at the amount of memory consumed by the SLAM implementation, as shown in Fig. 2. This is not an unrealisitic amount of memory to be consumed by the mapping and localization software; however, we will look into methods of reducing the memory footprint in the future.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/uz6rRU6pHpJ6otQiSNJWvf9oDazdbfqCTNdnQc6eJpgcIT_T2aJspjtvVNonnIiG0e2bGf8XWt4_TJlZoTnjOTu9-8NhpRIDbUFxpCgisFbDhoR1aO9jp_bT8ojvBQVLUB0PYsolKi5E96tvqkJD3grJMCuTA0PpPGToRUMDsusX7twmziK51NH-BA7da77-RZXsvbwUSoMu_eLnllC8xMlb914Yhcqw7FAIerFl4ILPaI16oxeebz_mhGZQVDgEtfOpe9uXhWR3rmk4I8F8-Nc2xVUTcjwwhRwavDwV0vgNa1HL03ZPfV8yMQQ8pV4pFZndiEPI2O6MsJL6aBF8s2F0cWyRvFGebEYRbIyMi6kVnpz05T-XK3ug8NTAA3MyWtOhwJa8Rn8gefv9ma7GMBS6ab7aS2kEeNr8KK89wI4-NUf80H2XhKo_eWMMrlJYKFQ--mYV9rx6PRgdZZg_K3IYWmwbrkpwDSZ9LqXVOOgwMDTq7ocfQsTIPBAXiCNXcG5aYrF4OGr1Qv7V9dlaONgoq4fbaev4AmZBK0nfuIco-WhQysb0iTBji7VgfpdiiJg2yLVvJGgWyja4DaL3vi3ZUEZ4A-WouoCYE8D3EOEF0KUN9JrxWu2OTXepbC3KCdmnREAAKchyu0nJXcIMOXRzaaNSlU6C_1I=w1280-h857-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/uz6rRU6pHpJ6otQiSNJWvf9oDazdbfqCTNdnQc6eJpgcIT_T2aJspjtvVNonnIiG0e2bGf8XWt4_TJlZoTnjOTu9-8NhpRIDbUFxpCgisFbDhoR1aO9jp_bT8ojvBQVLUB0PYsolKi5E96tvqkJD3grJMCuTA0PpPGToRUMDsusX7twmziK51NH-BA7da77-RZXsvbwUSoMu_eLnllC8xMlb914Yhcqw7FAIerFl4ILPaI16oxeebz_mhGZQVDgEtfOpe9uXhWR3rmk4I8F8-Nc2xVUTcjwwhRwavDwV0vgNa1HL03ZPfV8yMQQ8pV4pFZndiEPI2O6MsJL6aBF8s2F0cWyRvFGebEYRbIyMi6kVnpz05T-XK3ug8NTAA3MyWtOhwJa8Rn8gefv9ma7GMBS6ab7aS2kEeNr8KK89wI4-NUf80H2XhKo_eWMMrlJYKFQ--mYV9rx6PRgdZZg_K3IYWmwbrkpwDSZ9LqXVOOgwMDTq7ocfQsTIPBAXiCNXcG5aYrF4OGr1Qv7V9dlaONgoq4fbaev4AmZBK0nfuIco-WhQysb0iTBji7VgfpdiiJg2yLVvJGgWyja4DaL3vi3ZUEZ4A-WouoCYE8D3EOEF0KUN9JrxWu2OTXepbC3KCdmnREAAKchyu0nJXcIMOXRzaaNSlU6C_1I=w1280-h857-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Memory consumption   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Next, I’ll conduct a similar analysis on the data set collect outdoors. This will provide us with a GNSS ground truth to validate the localization provided by the lidar-based slam solution.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Sep 2017 00:00:00 -0400</pubDate>
        <link>https://watsonryan.github.io/researchNotes/_projects/pollinatorbot/2017/09/13/Update.html</link>
        <guid isPermaLink="true">https://watsonryan.github.io/researchNotes/_projects/pollinatorbot/2017/09/13/Update.html</guid>
        
        <category>pollinatorBot</category>
        
        <category>bumble bot</category>
        
        <category>blam test</category>
        
        
        <category>_projects</category>
        
        <category>pollinatorBot</category>
        
      </item>
    
      <item>
        <title>Testing D.P. M.M On Graph With Randomly Added False Constraints</title>
        <description>&lt;h2 id=&quot;randomly-added-false-constraints&quot;&gt;Randomly Added False Constraints&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;To validate our robust pose graph optimization technique, the Manhattan 3500 data set will be utilized. As an initial test, the original data set is corrupted by randomly adding erroneous range constraints. An example of a corrupted pose graph is provide in the right-hand-side of Fig. 1, where the red lines represent erroneous constraints. For this evaluation, up to 1500 false constraints (i.e., over 20\% of the total constraints) are added to the original pose graph.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/KJ4NfCSF6tnVbxiQWFpjfAhqOErcOjw6OfgE9nl3iUMBDAiOGf_o1vlNws9Li3EHuumIb8chwUsHbEdw7qhRyldNohoDjtyoU8p8c-qK6xb0zlDXdPEB4yNliDxJooBjz-qqVQ7GIR7qKG3NzN1-15isZpbRuJjZ84IcgJLGEKk24TFWD-6UVlGmxBnq_65AyXjjzhw0YH-C5Y6fpeDfoBVVA7Tp3ebXHvY0gKp76uyQPboLKEEMWSlgE0fz3qPpYNkSSerYfhp6WbibAu7wYZoa-laO4rjU8qIbWGNg6cpfEHvjJuHegt1UnixNj9VXz2mfDFbIv07xzeU7xZW9IYmDnk6h-c_rP0OUghL5C29l-n6FwzQYqNRPXpZEoyyqMjqQG4D8uWxz8YKw2hireD2gb3wCrAA-r8KVqHpDvsA_AqfQECH90MqzpC5NK6VAdN_qVfru8JPU06h_gqa9e7-kj2oEBcHWhV-afiB4la6lY_NmpmD25fcHJmp2eYx9e8-rFM8xcte6iFepi8jsiaFLLa74W6O4NHdyr_T-sTqmGMhO9xPv4qla9evaceZXNVY72a5oFiYc5wkeVIiYShiOhR0knTplTiIRcGWVnlVHZVMv6vKkT0gVFIjPHh3jrDS1Cm2KaQbSGT_6At-i-J9xWpBsqRg7cjc=w1280-h338-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/KJ4NfCSF6tnVbxiQWFpjfAhqOErcOjw6OfgE9nl3iUMBDAiOGf_o1vlNws9Li3EHuumIb8chwUsHbEdw7qhRyldNohoDjtyoU8p8c-qK6xb0zlDXdPEB4yNliDxJooBjz-qqVQ7GIR7qKG3NzN1-15isZpbRuJjZ84IcgJLGEKk24TFWD-6UVlGmxBnq_65AyXjjzhw0YH-C5Y6fpeDfoBVVA7Tp3ebXHvY0gKp76uyQPboLKEEMWSlgE0fz3qPpYNkSSerYfhp6WbibAu7wYZoa-laO4rjU8qIbWGNg6cpfEHvjJuHegt1UnixNj9VXz2mfDFbIv07xzeU7xZW9IYmDnk6h-c_rP0OUghL5C29l-n6FwzQYqNRPXpZEoyyqMjqQG4D8uWxz8YKw2hireD2gb3wCrAA-r8KVqHpDvsA_AqfQECH90MqzpC5NK6VAdN_qVfru8JPU06h_gqa9e7-kj2oEBcHWhV-afiB4la6lY_NmpmD25fcHJmp2eYx9e8-rFM8xcte6iFepi8jsiaFLLa74W6O4NHdyr_T-sTqmGMhO9xPv4qla9evaceZXNVY72a5oFiYc5wkeVIiYShiOhR0knTplTiIRcGWVnlVHZVMv6vKkT0gVFIjPHh3jrDS1Cm2KaQbSGT_6At-i-J9xWpBsqRg7cjc=w1280-h338-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: Manhattan 3500 dataset. (Left-hand-side) Un-corrupted graph. (Right-hand-side) Pose-graph corrupted by erroneous constraints    
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Utilizing the corrupted Manhattan dataset, the proposed methodology is evaluated along side two commonly used approaches. To quantify the accuracy of the optimzier, the median of the residual sum of squares (RSOS) of the $\it{X-Y}$ positioning error is reported, as in Fig. 2. From Fig. 2, it can be seen that  the max-mixtures approach, with a pre-defined mixture model, performs considerable worse than the switchable constraint and clustering technique as the number of erroneous constraints increases. Additionally, it should be noted that both switchable constraints and the clustering technique stay relatively constant, with respect to the median RSOS error, as the number of false constraints is increased; however; the clustering optimization technique provides a smaller bias.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/9zvF3rx7xKMUmWiull2oG3ZIZwyDsA372BefTkHPZyqOy_Ds5MSyecOrkQ5KV9BIrrvkhDMwUzM45ApHMP3OsJZs2MnIpETlFJjhpkssigjfM8x32UnIH_53r8HwzeTfsu33U9P-BlNSNaM8Ty3rv5wIZGcQ8tI6RkLm0j2zHLY_ouEcmLOkJdcmodaNf9DH-7qNaYH0m5CGLu5755WTSQOrOMdZGNk9AxlPUFwsfRVfDMhIJSpeEAkY82ZYLsKwEckxdD7nMXiNt9NOUQOTV9uUWTxuQYs81EfK7nnTGmJDQuOvrpUxF263NgVaq5ejWQBEnz2voXg4yGno0CzFR4CMvtVCYZ3oAbFD8PVpPxHzPBzfAvyS0YWABzYCP_5rbAQW2yiftmUF6EO5dMze8a6qZDUn5MQeb9EoDD2_N2L1okwc8HYXBGIOJS5XAZwUhL3vw1hf-7oBmTc4e8zxmXh8MousFHofSLn2Zw6i9EnG6oWn5jDHrXUMTqOvw_-X-AIY4_4tE0_FuduiDqIHB6Pd6yFfcGEAJGol3pOo2Vn4In2BXD5X3cU-qjSzDZ36aIdY7HiQO9e0lWWn9f8SO65iqdedke_yYwcWJ7Kq-WjGXgFVhisgDMz7H3M_03OwdAgt5eMzsz_yr3dcu82kGlH-cRhkGE5PD6c=w1280-h633-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/9zvF3rx7xKMUmWiull2oG3ZIZwyDsA372BefTkHPZyqOy_Ds5MSyecOrkQ5KV9BIrrvkhDMwUzM45ApHMP3OsJZs2MnIpETlFJjhpkssigjfM8x32UnIH_53r8HwzeTfsu33U9P-BlNSNaM8Ty3rv5wIZGcQ8tI6RkLm0j2zHLY_ouEcmLOkJdcmodaNf9DH-7qNaYH0m5CGLu5755WTSQOrOMdZGNk9AxlPUFwsfRVfDMhIJSpeEAkY82ZYLsKwEckxdD7nMXiNt9NOUQOTV9uUWTxuQYs81EfK7nnTGmJDQuOvrpUxF263NgVaq5ejWQBEnz2voXg4yGno0CzFR4CMvtVCYZ3oAbFD8PVpPxHzPBzfAvyS0YWABzYCP_5rbAQW2yiftmUF6EO5dMze8a6qZDUn5MQeb9EoDD2_N2L1okwc8HYXBGIOJS5XAZwUhL3vw1hf-7oBmTc4e8zxmXh8MousFHofSLn2Zw6i9EnG6oWn5jDHrXUMTqOvw_-X-AIY4_4tE0_FuduiDqIHB6Pd6yFfcGEAJGol3pOo2Vn4In2BXD5X3cU-qjSzDZ36aIdY7HiQO9e0lWWn9f8SO65iqdedke_yYwcWJ7Kq-WjGXgFVhisgDMz7H3M_03OwdAgt5eMzsz_yr3dcu82kGlH-cRhkGE5PD6c=w1280-h633-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 2 :: Accuracy of the estimated measurement covariance model, with respect to the Frobenius norm, as a function of the number of erroneous constraints     
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;With it shown that the proposed method is as robust other state-of-the-art optimization techniques, the discussion can proceed to the principle benefit of the proposed approach, which is that accurate knowledge of the $a-priori$ measurement covariance is not required. To expand upon this idea, the pose graph thats corrupted by 1500 erroneous constraints in greater detail.&lt;/p&gt;

&lt;p&gt;First, we can extract the residuals from the optimized graph and visually evaluate the performance of the estimated covariance. A scatter plot of the residuals is provided in the left-hand side of Fig. 3, where the black cluster represent the inlier distribution and the red scatter represents the residuals of the erroneous constraints. On the right-hand side of Fig. 3, we can see our estimated inlier covariance encapsulating the inlier residuals.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/2YqPGd8DIUdS4zR1Wq7asW7ZbabxUP1r8R1ndJYSSp5vE9Em63ItEb-KdZSUsKuR9gyokAQOxQwo_hpn6yFpAwB1Kqa7Dz2oBfbwK02Q-lVpH69tawd27y4vJK81jIm1sACpPl4IuLu9Swt0sMJrmhVfd7mkiQLmazrrGIjqaTkp9lYiOH6u2ywqLH_YynYsoi3EN87REpVEfWLduS40h0FKMgxysTQsrP7PLc_OVyAzg08kRTFb9vCkNB2AAhY8Tm1uuSC0t1lXrbTIWO20aejktfXUmj5Uax6EzaXinUHgy9gZrOlFZ8rnqhB0kE-dtwTzEVU9XdRvxB92W0SrNfzGBasjLuzk1M_0aZTy-Apq8S-kZiRU-iN59Ybik_tN6k3BmdzZBwYKeor3l5q52fwLEbAsvljLV62Qa_U8lOc-XKV_JIrTRoaVRWZ8GJFahubqHvLIv0cgMGx6RYV-98kcE8c4SpBwLllpA3fTrXdB5cgeU2OLFo9HaXc6RO0emkb0Lclochmwf4uKBBiLN31FerkVsWMZ3bK_GyrU60YRabU3fRLleV1u01L74DlgQs64rD0Z_KLsy8eLWOSZIppbRI9N9S0azND3Bp9p8meF1ONL4GNLCzMA0lfka7LDUUbJo9TklCc46XXE07OwD7Auwxx0nTf9Sjg=w1260-h665-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/2YqPGd8DIUdS4zR1Wq7asW7ZbabxUP1r8R1ndJYSSp5vE9Em63ItEb-KdZSUsKuR9gyokAQOxQwo_hpn6yFpAwB1Kqa7Dz2oBfbwK02Q-lVpH69tawd27y4vJK81jIm1sACpPl4IuLu9Swt0sMJrmhVfd7mkiQLmazrrGIjqaTkp9lYiOH6u2ywqLH_YynYsoi3EN87REpVEfWLduS40h0FKMgxysTQsrP7PLc_OVyAzg08kRTFb9vCkNB2AAhY8Tm1uuSC0t1lXrbTIWO20aejktfXUmj5Uax6EzaXinUHgy9gZrOlFZ8rnqhB0kE-dtwTzEVU9XdRvxB92W0SrNfzGBasjLuzk1M_0aZTy-Apq8S-kZiRU-iN59Ybik_tN6k3BmdzZBwYKeor3l5q52fwLEbAsvljLV62Qa_U8lOc-XKV_JIrTRoaVRWZ8GJFahubqHvLIv0cgMGx6RYV-98kcE8c4SpBwLllpA3fTrXdB5cgeU2OLFo9HaXc6RO0emkb0Lclochmwf4uKBBiLN31FerkVsWMZ3bK_GyrU60YRabU3fRLleV1u01L74DlgQs64rD0Z_KLsy8eLWOSZIppbRI9N9S0azND3Bp9p8meF1ONL4GNLCzMA0lfka7LDUUbJo9TklCc46XXE07OwD7Auwxx0nTf9Sjg=w1260-h665-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 3 :: Estimated measurement inlier distribution for the Manhattan 3500 data set corrupted by 1500 erroneous constraints. An identity $a-priori$ measurement covariance was provided.      
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We can extend the analysis of the optimizer’s ability to accurately estimate the measurement covariance model by evaluating the performance as the number of erroneous constraints varies. To quantify the accuracy of the estimated covariance model, the Frobenius norm of the difference between the true covariance matrix and the estimated matrix is utilized, as depicted in Fig. 4. From Fig. 4, it should be noted that the error in the estimated covariance distribution is relatively flat with respect to the number of erroneous constraints.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/lqoXWAwoGKRjj3yeCrFtXDvkaFUyUX_bTjIkJdq_N1aBeJ-4-Cuy5K4cGOw1AFqvsvxpeLR6YXCRijhHrarB_I3RFEbihS7ZwX7_xGWsrc7OFud_5JWDw1MTm-o4CF-F0FALdD-DqToPMZQCQZ6NMewJRKhQI_Mi2FubWK9iFXFADZR_mcC6NQPuacJv4id0-kIXVJZliWMO8oqE8e01axUWpPCbN-24rT7OwnwM8G7yy0Yp67yXEM5WPANBD6OnjPR96X4BEqFbtKvjbNWepcMcOH7U4y0-UkC8GqydFc8UNlMtaO5np4IzMHbfQxNhpesYcdW_oPZcpG9Hv24B6MaGdj2acj-7RUFc2V0KO_GIRpIgpLi5z0wePb-bqqqbxOX5D7V6DNRHfQkJOxfHPQ_ZyBCdOvlMJZ2WP5BN2JSjOos__87CSyHcdxFxOu0svNFLkXdcf-lKdtI3KOKt5Aylh4nz07K8eEm_8Z76R_B2Dj6keac1geB3Zwsh7PiBTjel5gGz8wh0AAVFNr9PF-rpMh51_wLtuvYQcpNLUqrUhvpO3R4IWCbZFpBrJ8IZDqxJF_4-iOOZEMC4Ejv96sqyLK4vyn3hhjGk7bsBuRka-nM2O0Qn0mHo6E-y5ZNvC3sIlAd1DNTt6dlYvz7ZOkkUfqNrg2ZnD_A=w1280-h633-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/lqoXWAwoGKRjj3yeCrFtXDvkaFUyUX_bTjIkJdq_N1aBeJ-4-Cuy5K4cGOw1AFqvsvxpeLR6YXCRijhHrarB_I3RFEbihS7ZwX7_xGWsrc7OFud_5JWDw1MTm-o4CF-F0FALdD-DqToPMZQCQZ6NMewJRKhQI_Mi2FubWK9iFXFADZR_mcC6NQPuacJv4id0-kIXVJZliWMO8oqE8e01axUWpPCbN-24rT7OwnwM8G7yy0Yp67yXEM5WPANBD6OnjPR96X4BEqFbtKvjbNWepcMcOH7U4y0-UkC8GqydFc8UNlMtaO5np4IzMHbfQxNhpesYcdW_oPZcpG9Hv24B6MaGdj2acj-7RUFc2V0KO_GIRpIgpLi5z0wePb-bqqqbxOX5D7V6DNRHfQkJOxfHPQ_ZyBCdOvlMJZ2WP5BN2JSjOos__87CSyHcdxFxOu0svNFLkXdcf-lKdtI3KOKt5Aylh4nz07K8eEm_8Z76R_B2Dj6keac1geB3Zwsh7PiBTjel5gGz8wh0AAVFNr9PF-rpMh51_wLtuvYQcpNLUqrUhvpO3R4IWCbZFpBrJ8IZDqxJF_4-iOOZEMC4Ejv96sqyLK4vyn3hhjGk7bsBuRka-nM2O0Qn0mHo6E-y5ZNvC3sIlAd1DNTt6dlYvz7ZOkkUfqNrg2ZnD_A=w1280-h633-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 4 :: Accuracy of the estimated measurement covariance model, with respect to the Frobenius norm, as a function of the number of erroneous constraints
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 12 Sep 2017 00:00:00 -0400</pubDate>
        <link>https://watsonryan.github.io/researchNotes/_projects/dirichletrobustposegraph/2017/09/12/Update.html</link>
        <guid isPermaLink="true">https://watsonryan.github.io/researchNotes/_projects/dirichletrobustposegraph/2017/09/12/Update.html</guid>
        
        <category>dirichletRobustPoseGraph</category>
        
        
        <category>_projects</category>
        
        <category>dirichletRobustPoseGraph</category>
        
      </item>
    
  </channel>
</rss>
