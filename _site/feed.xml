<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research Notes</title>
    <description></description>
    <link>http://localhost:4000/researchNotes/</link>
    <atom:link href="http://localhost:4000/researchNotes/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 19 Jul 2017 21:53:45 -0400</pubDate>
    <lastBuildDate>Wed, 19 Jul 2017 21:53:45 -0400</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>Validation of G.M.M Covariance Estimation Using Simulated Distributions</title>
        <description>&lt;p&gt;Yesterday, we tested the ability of our GMM to estimate the covariance of a residual distribution using the Manhattan 3500 pose-graph. Today, we will continue to validate our GMM by simulating a distribution to see if our GMM approaches the correct covariance as the number of iterations increases.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;first-test&quot;&gt;First Test&lt;/h3&gt;

&lt;p&gt;The first distribution that the GMM was tested on is depicted in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/eMy5-Cdp3htUZ1baVwDdHHFFWelaexhl4n8KlUsYtHtwKRdCMlv1I9RSGZmD8TpD8BZdOJyYCp3fqxntQKRsIqlSoJ2rUEkFh6c6nUtQh8nIr_RDdAHT_zaBvUnWd83oVkGADpjEdg=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/eMy5-Cdp3htUZ1baVwDdHHFFWelaexhl4n8KlUsYtHtwKRdCMlv1I9RSGZmD8TpD8BZdOJyYCp3fqxntQKRsIqlSoJ2rUEkFh6c6nUtQh8nIr_RDdAHT_zaBvUnWd83oVkGADpjEdg=w1024-h566-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: Residual Scatter ( Ratio of Inler/Outler = 0.01 ).   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using the distribution depicted in Figure 1, the GMM was utilized to provide a covariance estimate at different iteration values. The metric used to quantify the “closeness” of two covariance matricies was $ || P_{true} - P_{est} ||_{F}$.&lt;/p&gt;

&lt;p&gt;The convergence of the GMM as the number of iterations increases is shown in Figure 2.  Additionally, the final covariance estimate for the inlier distribution ( i.e. the covariance estimate when the number of iterations equaled fifty ) is shown in Figure 3.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/64yf_NF4CamayMwmgVkByCxEg1KTZeJA37x0mXBtpimDYgvQv2k7HseYfPtPxgls69trteEnhDKksUqN7hLgri7T3esWNxWF7pfHIh9bXHVGOx5bpP5_dW0I2_emE7jLn3IkgQPm1g=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/64yf_NF4CamayMwmgVkByCxEg1KTZeJA37x0mXBtpimDYgvQv2k7HseYfPtPxgls69trteEnhDKksUqN7hLgri7T3esWNxWF7pfHIh9bXHVGOx5bpP5_dW0I2_emE7jLn3IkgQPm1g=w1024-h566-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 2 :: Convergence of Covariance Estimate From GMM.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/-HghJkDadBemQSMrwTNZTeBG998S2eE1H0em1eogSbEfhSa4FopholQD_cRjWc78FxqJjTGZUmi6a-lacLNlygB9zViTpF9cFHCLj33UVt-7RucJhXI0--H9Jh9w1qZvr2-IKwXbeQ=w416-h67-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/-HghJkDadBemQSMrwTNZTeBG998S2eE1H0em1eogSbEfhSa4FopholQD_cRjWc78FxqJjTGZUmi6a-lacLNlygB9zViTpF9cFHCLj33UVt-7RucJhXI0--H9Jh9w1qZvr2-IKwXbeQ=w416-h67-no&quot; border=&quot;0&quot; alt=&quot; photo CodeCogsEqn_zpsaxae44q4.gif&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 3 :: Covariance Comparison.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;second-test&quot;&gt;Second Test&lt;/h3&gt;

&lt;p&gt;From the previous test, we have seen that the covariance estimate approaches the true covariance as the number of iterations of the GMM is allowed to increase. However, that test did not contain a high ratio of outliers. For this test, we increase the fraction of outliers in the distribution. The residual scatter for this test is depicted in Figure 4.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/YWuM0E7Ot6G2q7uMcufzgOpEt3was2X49sALk8y-cLXAUVlrPq1zzfR6GnhworD-dZ5VSeemYIrYBnpPa4K669Tv0_tip9KgykHAtHOZhV5Xms8s19oeDUrCClvI_yn1KV6YCHuO_Q=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/YWuM0E7Ot6G2q7uMcufzgOpEt3was2X49sALk8y-cLXAUVlrPq1zzfR6GnhworD-dZ5VSeemYIrYBnpPa4K669Tv0_tip9KgykHAtHOZhV5Xms8s19oeDUrCClvI_yn1KV6YCHuO_Q=w1024-h566-no&quot; border=&quot;0&quot; alt=&quot; photo simScatter2_zpsbdnst01e.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 4 :: Residual Scatter ( Ratio of Inler/Outler = 0.1 ).   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Again, the Frobenius norm was utilized to quantify the accuracy of our estimate. The accuracy of the estimate as the number of iterations increases is shown in Figure 5. This result again shows that the GMM approaches the true covariance estimate as the number of iterations increases.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/ke0DiuuYzz1vzPJGqfPaP6mXWcHNdoj4LLyUR_Xj37c4mBN-WYc817g8FqFUhv_BwR8zrYeM0lA8IlVapSTMr-jnbVvaa49HZOPvEOazWDz85ShOEV1yxkS0pgytK1ZJbQEsUJDDbw=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/ke0DiuuYzz1vzPJGqfPaP6mXWcHNdoj4LLyUR_Xj37c4mBN-WYc817g8FqFUhv_BwR8zrYeM0lA8IlVapSTMr-jnbVvaa49HZOPvEOazWDz85ShOEV1yxkS0pgytK1ZJbQEsUJDDbw=w1024-h566-no&quot; border=&quot;0&quot; alt=&quot; photo covEst2_zpsx2xkxi7d.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 5 :: Convergence of Covariance Estimate From GMM.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The final covariance estimate for the inlier distribution ( i.e., the covariance estimate when the number of iterations equaled fifty ) is shown in Figure 6.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/JzC7kdmP03YnJNhkUBlHyWWmqUOgc9rqY8qQdFskhXn2KPOOfHl8McaSF5Q214HAf-0diteosAn3_WA6SNjUQX_JD4Is4TEo-MLRJi6DjWuGLfiHYYJZ417279iDg6-puN7W-ZY9-A=w402-h67-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/JzC7kdmP03YnJNhkUBlHyWWmqUOgc9rqY8qQdFskhXn2KPOOfHl8McaSF5Q214HAf-0diteosAn3_WA6SNjUQX_JD4Is4TEo-MLRJi6DjWuGLfiHYYJZ417279iDg6-puN7W-ZY9-A=w402-h67-no&quot; border=&quot;0&quot; alt=&quot; photo CodeCogsEqn 1_zpszaabqtv0.gif&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 6 :: Covariance Comparison.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;final-test&quot;&gt;Final Test&lt;/h3&gt;

&lt;p&gt;For the final test, we increase the fraction of outliers to 30 percent. The scatter of the residuals is shown in Figure 7.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/pAlOV_lAfnluMKP0Dz2KIS5cqYO8l-6lR0pIErRkqrk9_ona65dYfCLisFoJYYDPox9_l-gK3mYl8aHkfjR11Dv4DWtA8PSFHEUk49KXjOMWXSQ3XxcFC4bIghGdgQ2j7GG8wB1sFg=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/pAlOV_lAfnluMKP0Dz2KIS5cqYO8l-6lR0pIErRkqrk9_ona65dYfCLisFoJYYDPox9_l-gK3mYl8aHkfjR11Dv4DWtA8PSFHEUk49KXjOMWXSQ3XxcFC4bIghGdgQ2j7GG8wB1sFg=w1024-h566-no&quot; border=&quot;0&quot; alt=&quot; photo simScatter3_zpsfinbvgo6.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 7 :: Residual Scatter ( Ratio of Inler/Outler = 0.3 ).   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The accuracy of the covariance estimate as the number of iterations provided to the GMM increases is shown in Figure 9. This shows a trend similar to the two previously conducted tests.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/s994_248IunbysgqHn-4Lu3wzEeaoIPzRQikwTdapfp6OjFJ1U1iUWb4KIq555yDy50hpQXm24fH6i0ZCJHKrXMYuhgkG0DDXLCOoMWnAKmVk1rZWdjzw6fNrU9JFhDILUSRsC_Ikg=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/s994_248IunbysgqHn-4Lu3wzEeaoIPzRQikwTdapfp6OjFJ1U1iUWb4KIq555yDy50hpQXm24fH6i0ZCJHKrXMYuhgkG0DDXLCOoMWnAKmVk1rZWdjzw6fNrU9JFhDILUSRsC_Ikg=w1024-h566-no&quot; border=&quot;0&quot; alt=&quot; photo covEst3_zpslbhx8i9b.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 8 :: Convergence of Covariance Estimate From GMM.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The final covariance estimate for the inlier distribution is shown in Figure 9.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/-HghJkDadBemQSMrwTNZTeBG998S2eE1H0em1eogSbEfhSa4FopholQD_cRjWc78FxqJjTGZUmi6a-lacLNlygB9zViTpF9cFHCLj33UVt-7RucJhXI0--H9Jh9w1qZvr2-IKwXbeQ=w416-h67-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/-HghJkDadBemQSMrwTNZTeBG998S2eE1H0em1eogSbEfhSa4FopholQD_cRjWc78FxqJjTGZUmi6a-lacLNlygB9zViTpF9cFHCLj33UVt-7RucJhXI0--H9Jh9w1qZvr2-IKwXbeQ=w416-h67-no&quot; border=&quot;0&quot; alt=&quot; photo CodeCogsEqn 2_zpskgxg3vxq.gif&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 9 :: Covariance Comparison.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;Next, these mixture models will be provided to the max-mixtures robust noise model to see how well it can optimize when faced with faulty pose graphs&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 19 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/07/19/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/07/19/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Gausian Mixture Model</category>
        
        <category>max-mixtures</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Validation of G.M.M Covariance Estimation Using Manhattan Pose Graph</title>
        <description>&lt;p&gt;The previous two weeks have been spent, primarily, on porting code and tracking down bugs. Now, it looks like the G.M.M is working properly. To validate this, the Manhattan 3500 pose graph — as depicted in Figure 1 — was utilized.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/tVZLogjFEOfMGJk8tvuNYpixcBsL472LZYbhm8IRVGqCO74kD2XEV-hqqr3k4TdRYQ5WQhafKvsfCBcz4wLnAieV-S7ligGmJsYLDCxXwhghq8UtUEZe0ZB-uttdFlvte-8tdCENZA=w608-h542-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/tVZLogjFEOfMGJk8tvuNYpixcBsL472LZYbhm8IRVGqCO74kD2XEV-hqqr3k4TdRYQ5WQhafKvsfCBcz4wLnAieV-S7ligGmJsYLDCxXwhghq8UtUEZe0ZB-uttdFlvte-8tdCENZA=w608-h542-no&quot; border=&quot;0&quot; alt=&quot; photo M3500_eg2o_zpstu9zq4qo.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: Manhattan 3500 Pose-graph.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using the Manhattan 3500 pose-graph, 100 false constraints — as shown in Figure 2 — were added to see how well we can estimate the inlier/outlier distribution.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/_NHc35kLjMa3ZWSb8LHbLvol8KAd7TUYqv81-bWhhGOFspObggZrRLwZtqHSSZ9SUzTWaKkP2gnvuylOA-ZtKNAZ7O3faxofIset9bm9fp-caesdO_WilgO3npZ7tLAwP0Iw1ZkoJg=w977-h665-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/_NHc35kLjMa3ZWSb8LHbLvol8KAd7TUYqv81-bWhhGOFspObggZrRLwZtqHSSZ9SUzTWaKkP2gnvuylOA-ZtKNAZ7O3faxofIset9bm9fp-caesdO_WilgO3npZ7tLAwP0Iw1ZkoJg=w977-h665-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 2 :: Manhattan 3500 Pose-graph with 100 False Constraints.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using the graph represented in Figure 2, an initial optimization utilizing $L_2$ was conducted to extract the residuals. The residual scatter is shown in Figure 3. From this figure, a clear inlier cluster is shown in the center with the scattered residuals representing the false constraints.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/mRWRAjYrBjb-L2Mz8U81VESEnVJrUF1aXlOPyhB5PHlbqb3OjZJFbYBUydWkdjvx-VIEZPC7c-EVZeXvOCUIK_elVk83ZfKnsApC3Nv3JzCzP1tze7VjyJ1o25HWe_9ntOeQ9bjjHQ=w630-h332-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/mRWRAjYrBjb-L2Mz8U81VESEnVJrUF1aXlOPyhB5PHlbqb3OjZJFbYBUydWkdjvx-VIEZPC7c-EVZeXvOCUIK_elVk83ZfKnsApC3Nv3JzCzP1tze7VjyJ1o25HWe_9ntOeQ9bjjHQ=w630-h332-no&quot; border=&quot;0&quot; alt=&quot; photo residualScatter_zpsszsigx4x.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 3 :: Residual Scatter.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;With this set of residuals, we can utilize the G.M.M to estimate the inlier/outlier clusters. When Depicted in Figure 4 is the estimated inlier cluster. Through visual inspection, this looks like a fair estimate.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/o7Lu8J8021rsWXCfyBRub825QQ8quQNbh1I6cy14FtLUCp8qQtAu9wdsIPQXgv-UwBEcO8OPfiB-mJggBR-SqgbIHQTiaf0qmH-9boKPbPe5-KFbycIdjfJtfci7ahErgI7k1p0UTw=w630-h332-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/o7Lu8J8021rsWXCfyBRub825QQ8quQNbh1I6cy14FtLUCp8qQtAu9wdsIPQXgv-UwBEcO8OPfiB-mJggBR-SqgbIHQTiaf0qmH-9boKPbPe5-KFbycIdjfJtfci7ahErgI7k1p0UTw=w630-h332-no&quot; border=&quot;0&quot; alt=&quot; photo inlierCov_zpsc8e16igm.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 4 :: Inlier Covariance Estimate.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The next two figures shown the outlier covariance estimates provided by the G.M.M. Again, through visual inspection, these look like decent estimates of the residual covariance.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/bQNlojrVx85kI7WJ3xEhZtSmmhHI8j2ci_FAGygLsuHEGva4FNGs0eVFxZpwYZSjy1NERqkIAzB_bgY9v8a9sLd5X7h6PXnKZByPbiknK6oZVLvYuvVMBHO-FUPHx2T6JrkI2hURpg=w630-h332-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/bQNlojrVx85kI7WJ3xEhZtSmmhHI8j2ci_FAGygLsuHEGva4FNGs0eVFxZpwYZSjy1NERqkIAzB_bgY9v8a9sLd5X7h6PXnKZByPbiknK6oZVLvYuvVMBHO-FUPHx2T6JrkI2hURpg=w630-h332-no&quot; border=&quot;0&quot; alt=&quot; photo oulierCov_zpsjgzlpuc8.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 5 :: First Outlier Covariance.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/MGb3Oqq1pMNmRM0aHyGKv7py3DcAhIcMh6keG7ykZDZl6pRn9-ikXlXcq2NgLeJ3fqiL0hItqcZ2ctNbBJeeVXXZwlee8aLmxJx1Bc8OdsXsFp3OQS-He3h4cYWrRORlFCavekQcHw=w630-h349-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/MGb3Oqq1pMNmRM0aHyGKv7py3DcAhIcMh6keG7ykZDZl6pRn9-ikXlXcq2NgLeJ3fqiL0hItqcZ2ctNbBJeeVXXZwlee8aLmxJx1Bc8OdsXsFp3OQS-He3h4cYWrRORlFCavekQcHw=w630-h349-no&quot; border=&quot;0&quot; alt=&quot; photo outlier2_zpsexh9wgvg.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 6 :: Second Outlier Covariance.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;Next, I’m going to setup a simulation that will generate residual distributions. I’ll use the simulated distributions to validate that the estimate inlier covariance approaches the true distribution as the number of G.M.M. iterations increases.&lt;/p&gt;

</description>
        <pubDate>Tue, 18 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/07/18/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/07/18/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Gausian Mixture Model</category>
        
        <category>max-mixtures</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Innovation Adaptive Estimation Based on Inverse Wishart Distribution</title>
        <description>&lt;p&gt;For most INS integration applications the extended Kalman filter is the algorithm of choice. For the general application, a schematic overview of the algorithm is provided in Figure 1. From this figure it can be seen that the Kalman filter has two main steps: prediction and measurement update. In addition to the two main steps, the initialization of the Q, R matrices is incredible important for system convergence.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For most applications, the Kalman filter’s system noise covariance matrix, Q , and measurement noise covariance matrix, R , are determined during the development phase by laboratory measurements of the system, simulation and trials. However, there are some cases where this cannot be done. For example, if an INS/GNSS integration algorithm or INS calibration algorithm is designed for use with a range of different inertial sensors, the system noise covariance will not be known in advance of operation. Similarly, if a transfer alignment algorithm (Section 15.1) is designed for use on different aircraft and weapon stores without prior knowledge of the flexure and vibration environment, the measurement noise covariance will not be known in advance.   ** REF ::  Paul D. Groves Principles of GNSS, Inertial, and Multisensor Integrated Navigation Systems&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/z6YE6NmFB6MXf211uKJxf9fHApWcMURJh9XWLbu50viiXKc-0I6E0sFevPNxk4wYOhMoMsughAhn5171JbDf7dcSrZNGYhF1BEYS4F2iFCvmuZCqXHT8M3zRBPIp8gV93WL5Ml7aYg=w276-h489-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/z6YE6NmFB6MXf211uKJxf9fHApWcMURJh9XWLbu50viiXKc-0I6E0sFevPNxk4wYOhMoMsughAhn5171JbDf7dcSrZNGYhF1BEYS4F2iFCvmuZCqXHT8M3zRBPIp8gV93WL5Ml7aYg=w276-h489-no&quot; border=&quot;0&quot; alt=&quot; photo kfSchematic_zpsnbdnjnhw.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: General Schematic of Extended Kalman Filter.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The traditional method to adaptively tune the Q and R matrices is known as Innovation Adaptive Estimation. The first step in this method is to calculate the measurement innovation covariance for the last n measurements, as show below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C_K = \frac{1}{n} \sum_{j=k-n}^k \delta z_j \delta z_j^T&lt;/script&gt;

&lt;p&gt;With this covariance estimate, the Q and R matricies can be updated.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_k = K_k C_k K_K^T&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_k = C_k - H_k P_K^- H_K^T&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;extension-to-traditional-iae&quot;&gt;Extension to Traditional IAE&lt;/h2&gt;

&lt;p&gt;Today, Clark proposed the idea of utilizing a similar approach to the one were implementing for robust pose-graph optimization to adaptively update the Q, R matrices? This approach will utilize the Gaussian Inverse Wishart distribution to estimate the innovation covariance matrix. The Gaussian Inverse Wishart distribution is defined below,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;GIW(\mu,\Sigma | m_o, \kappa_o, \nu_o, S_o) := \mathcal{N}(\mu | m_o, \frac{1}{\kappa_o}\Sigma) IW(\Sigma| S_o,\nu_o) ,&lt;/script&gt;

&lt;p&gt;which can be represented as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;GIW(\mu,\Sigma | m_o, \kappa_o, \nu_o, S_o) =  e^{-\frac{\kappa_o}{2}|| \mu - m_o||_{\Sigma}} e^{-\frac{1}{2} tr(\Sigma^{-1}S_o)} \frac{1}{Z_{GIW}(D,\kappa_o,\nu_o,S_o} |\Sigma|^{-\frac{\nu_o+D+2}{2}},&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_{GIW}(D,\kappa_o,\nu_o,S_o) = 2^{\frac{(\nu_o+1)D}{2}}\pi^{\frac{D(D+1)}{4}}\kappa_o^{-D/2} \|S_o\|^{-\nu_o/2} \prod_{d=1}^{D} \Gamma(\frac{\nu_o+1-d}{2})&lt;/script&gt;

&lt;h4 id=&quot;parameter-definition&quot;&gt;Parameter Definition&lt;/h4&gt;

&lt;p&gt;$m_o$ –&amp;gt; Prior mean for $\mu$&lt;/p&gt;

&lt;p&gt;$\kappa_o$ –&amp;gt; belief in $m_o$&lt;/p&gt;

&lt;p&gt;$S_o$ –&amp;gt; prior $\Sigma$&lt;/p&gt;

&lt;p&gt;$\nu_o$ –&amp;gt; belief in $S_o$&lt;/p&gt;

</description>
        <pubDate>Fri, 30 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/06/30/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/06/30/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>IAE</category>
        
        <category>Inverse Wishart</category>
        
        <category>Idea</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Comparison of Max-Mix Bayesian Mixture Model to M-Estimators</title>
        <description>&lt;h1 id=&quot;robust-noise-model-comparison&quot;&gt;Robust Noise Model Comparison&lt;/h1&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now that we have the BMM-DP working, we can begin testing it against other robust noise models on commonly used datasets. To start, we will test the BMM-DP  against six commonly used M-estimators. All comparisons shown below will have 100 false constraints added to each pose-graph.
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;manhattan-3500&quot;&gt;Manhattan 3500&lt;/h2&gt;

&lt;p&gt;To start the comparison we will utilize the commonly used Manhattan 3500 pose-graph. First, we optimized the pose-graph with $L_2$ optimization and the BMM-DP. The results are provided in Table 1. From this table it can be seen that the BMM-BP outperformed $L_2$ optimization with respect to RSS positioning error; however, that is not a surprise, so next we will compare the BMM-DP results to the results obtained by several M-estimators.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/WNKQyF9D3uFquId0Zedh8bsUYNT7mNJLh2vvRLAThbApSOBD9oukNBRwoZ6T3RImmleQN759xOVIRdjYQmWeSdjegK4ZCzjoZVH-0dxk-VVXijvluu6lxJsgQ5axApCoaRBR1KWHrw=w395-h78-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/WNKQyF9D3uFquId0Zedh8bsUYNT7mNJLh2vvRLAThbApSOBD9oukNBRwoZ6T3RImmleQN759xOVIRdjYQmWeSdjegK4ZCzjoZVH-0dxk-VVXijvluu6lxJsgQ5axApCoaRBR1KWHrw=w395-h78-no&quot; border=&quot;0&quot; alt=&quot; photo orgStats_zpsmtirkwkk.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Table 1 :: Median of RSS positioning error for Manhattan 3500 pose-graph Using $L_2$ and BMM-DP.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;From previous testing, it is known that all M-estimators are sensitive to the user specified kernel width. Due to this sensitivity, for this comparison, we ran each M-estimator with several kernel widths to find the optimal kernel width for each M-estimator with respect to this dataset. This is depicted in Figure 1, where is can be seen that there is great variability in the RSS positioning error with respect to the specified kernel width.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/hxCNOzoKyhdr46NM-Pk7N2bq-XLQGgfrQACsfOlJrZmcsxxyBFFAwsFvET4xZQ5ASN2ALaAEbmr6mLSGJiGm8IqGTOQWowVfHfQrZLrN4hiJDeEzQng4q6eCpn97_O8_65lXx8e6Nw=w630-h332-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/hxCNOzoKyhdr46NM-Pk7N2bq-XLQGgfrQACsfOlJrZmcsxxyBFFAwsFvET4xZQ5ASN2ALaAEbmr6mLSGJiGm8IqGTOQWowVfHfQrZLrN4hiJDeEzQng4q6eCpn97_O8_65lXx8e6Nw=w630-h332-no&quot; border=&quot;0&quot; alt=&quot; photo man3500_zpscx4shcce.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Median of RSS positioning error for Manhattan 3500 pose-graph.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The optimal M-Estimator results are depicted in Table 2. From this table is can be seen that the Tukey kernel performed the best out of the M-estimators; however, the positioning error provided by the Tukey kernel is still substantially larger than the positioning error provided by the BMM-BP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/WqGZyw_A7fmTsgovfC6W4tDpcVpYHzqmfAZsX1WAWRFpBvrygxzkLAR7EmX-ylABJY6ODuypnMHLoyhLR_KCuS-jDtNDQMRQa_5eq_hZfvaeCVRokbxRCCAFUFm0Y09Kgsinrxc18Q=w467-h154-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/WqGZyw_A7fmTsgovfC6W4tDpcVpYHzqmfAZsX1WAWRFpBvrygxzkLAR7EmX-ylABJY6ODuypnMHLoyhLR_KCuS-jDtNDQMRQa_5eq_hZfvaeCVRokbxRCCAFUFm0Y09Kgsinrxc18Q=w467-h154-no&quot; border=&quot;0&quot; alt=&quot; photo stats_zpsfdirssj7.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Table 2 :: Median of RSS positioning error for Manhattan 3500 pose-graph using M-Estimators.   
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Now that we have this code setup we can do a similar comparison for other commonly used pose-graphs. Additionally, we need to test the BMM-DP against switchable constraints.&lt;/p&gt;

</description>
        <pubDate>Wed, 28 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/06/28/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/06/28/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>M-Estimator</category>
        
        <category>Switchable Constraints</category>
        
        <category>Mixture Model</category>
        
        <category>Dirichlet Process</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title> Continued Testing On Manhattan 3500</title>
        <description>&lt;h1 id=&quot;initial-testing&quot;&gt;Initial Testing&lt;/h1&gt;

&lt;p&gt;As mentioned in the previous update, the manhattan3500 data-set is being utilized to test the BMM DP– this pose-graph is depicted in Figure 1. Initially, we process the fault-free pose-graph and the pose-graph that has 100 false constraints added with traditional $L_2$ optimization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.lucacarlone.com/images/M3500_eg2o.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Manhattan 3500 pose-graph
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The residual scatter plots for the fault-free graph and the graph with 100 false constraints using $L_2$ optimization are shown in  Figures 2 and 3, respectively. This plots depict what would be expected; both distributions have approximately zero mean with the  faulty graph residuals have a larger variance.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/BNZrx-XybVTee_hnOFM8fuZOKYtJrNFDffgYlUks3WQ9k5flqMijAxzjTMJTNAn-jFlZTo4AQ299j9DkmGzZDjeYe7yR7UwGU51_0o080rQIuE5845abbjAfuv3lRuzfW00JQFJaKw=w630-h156-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/BNZrx-XybVTee_hnOFM8fuZOKYtJrNFDffgYlUks3WQ9k5flqMijAxzjTMJTNAn-jFlZTo4AQ299j9DkmGzZDjeYe7yR7UwGU51_0o080rQIuE5845abbjAfuv3lRuzfW00JQFJaKw=w630-h156-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-kq4xebQrYv_zpsoavucrk8.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Fault free residual scatter   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/aH8ONLnws-m4PTRFJA734FyUS8XbDXObWCB7c_FXlmi91AwdDSQMbyMS8SV-C2V5m3f5XOv5LKPsqFSVXKEQV8pNKVLC4xy-nzLcv_hhsBSFpwUi6hMC-9Gh2PsPhwcxLL-B6dajXA=w630-h156-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/aH8ONLnws-m4PTRFJA734FyUS8XbDXObWCB7c_FXlmi91AwdDSQMbyMS8SV-C2V5m3f5XOv5LKPsqFSVXKEQV8pNKVLC4xy-nzLcv_hhsBSFpwUi6hMC-9Gh2PsPhwcxLL-B6dajXA=w630-h156-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-QXmCmNdSzLvkhWK_zpsavcmhecx.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 3 :: Pose graph residual scatter when 100 faulty constraints are added   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using the faulty residuals, the BMM DP was tested. The BMM DP provided an inlier/outlier distribution mixture as shown below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_I = [ 0.04946242, -0.05057746, -0.0594329 ]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P_I =
  \begin{bmatrix}
    16.89882198 &amp;&amp; -0.90303439 &amp;&amp;  0.08602105 \\
   -0.90303439 &amp;&amp; 17.08403007  &amp;&amp; 0.06889149 \\
   0.08602105 &amp;&amp;  0.06889149 &amp;&amp; 10.95290421
  \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_O = [ 0.056539, -0.35273277, -0.83455116 ]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P_O =
  \begin{bmatrix}
     2087.43888892 &amp;&amp; -651.08477092 &amp;&amp;  198.82917436 \\
   -651.08477092 &amp;&amp; 1977.03231206  &amp;&amp; -92.72713996 \\
   198.82917436 &amp;&amp;  -92.72713996 &amp;&amp; 1264.0179354
  \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now, we can test the estimated mixture by feeding it into the Max-Mixture model to see how well it optimizes the pose-graph in comparison to $L_2$.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table class=&quot;mbtablestyle&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Optimization Stragety&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Total Graph Error – $\mathcal{X}^2$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$L_2$ with no faults&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;73.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$L_2$ with faults&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.964e+07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Max-Mix Using BMM-BP with Faults&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;65.33&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
From this table, we can see that the BMM-BP estimate mixture preforms very well in comparison to $L_2$.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Testing, testing, and more testing .. .&lt;/p&gt;
</description>
        <pubDate>Mon, 26 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/06/26/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/06/26/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Density Estimation</category>
        
        <category>Mixture Model</category>
        
        <category>Dirichlet Process</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Testing Bayes Mixture Model with Dirichlet Process On Manhattan 3500 Pose-Graph</title>
        <description>&lt;h1 id=&quot;initial-testing&quot;&gt;Initial Testing&lt;/h1&gt;

&lt;p&gt;To move from simulated data to a collected pose-graph, the manhattan3500 data-set 
was utilized – this pose-graph is depicted in Figure 1. Initially, we process the 
fault-free pose-graph and the pose-graph that has 100 false constraints added.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.lucacarlone.com/images/M3500_eg2o.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Manhattan 3500 pose-graph
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;After processing both pose-graphs, the residuals from each graph can be analyzed. 
The residual scatter plots for the fault-free graph and the faulty graph are shown in 
Figures 2 and 3, respectively. This plots depict what would be expected; both distributions
have approximately zero mean with the faulty graph residuals have a larger variance.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/BNZrx-XybVTee_hnOFM8fuZOKYtJrNFDffgYlUks3WQ9k5flqMijAxzjTMJTNAn-jFlZTo4AQ299j9DkmGzZDjeYe7yR7UwGU51_0o080rQIuE5845abbjAfuv3lRuzfW00JQFJaKw=w630-h156-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/BNZrx-XybVTee_hnOFM8fuZOKYtJrNFDffgYlUks3WQ9k5flqMijAxzjTMJTNAn-jFlZTo4AQ299j9DkmGzZDjeYe7yR7UwGU51_0o080rQIuE5845abbjAfuv3lRuzfW00JQFJaKw=w630-h156-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-kq4xebQrYv_zpsoavucrk8.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Fault free residual scatter   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/aH8ONLnws-m4PTRFJA734FyUS8XbDXObWCB7c_FXlmi91AwdDSQMbyMS8SV-C2V5m3f5XOv5LKPsqFSVXKEQV8pNKVLC4xy-nzLcv_hhsBSFpwUi6hMC-9Gh2PsPhwcxLL-B6dajXA=w630-h156-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/aH8ONLnws-m4PTRFJA734FyUS8XbDXObWCB7c_FXlmi91AwdDSQMbyMS8SV-C2V5m3f5XOv5LKPsqFSVXKEQV8pNKVLC4xy-nzLcv_hhsBSFpwUi6hMC-9Gh2PsPhwcxLL-B6dajXA=w630-h156-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-kq4xebQrYv_zpsoavucrk8.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 3 :: Pose graph residual scatter when 100 faulty constraints are added   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using the faulty residuals, the BMM DP was tested. To allow for easy 
visualization, the classification is shown in 2D slices of the 3D residual 
scatter (i.e., Figure 4 only shows the classification of the X-Y subset of the
data).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/s9G-ETU_5pVOlnqhG19669jd7EnHTruKDbRgPd5C2N3NUF4FyYhe0sudaSOr9AdQGZP2M0KAe1CzmV4mfTmtaTj3JnbGOV0LLh0zHEGk9h-HzMS3JMl_rgukuKCx_BOnow6XDBJaqA=w630-h344-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/s9G-ETU_5pVOlnqhG19669jd7EnHTruKDbRgPd5C2N3NUF4FyYhe0sudaSOr9AdQGZP2M0KAe1CzmV4mfTmtaTj3JnbGOV0LLh0zHEGk9h-HzMS3JMl_rgukuKCx_BOnow6XDBJaqA=w630-h344-no&quot; border=&quot;0&quot; alt=&quot; photo xyComp_zpsb50ukvct.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 4 :: Classification on X-Y residuals   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/8OKMbRHYIsRpnO_xjs14NHiuV0Oaxucy0FOBScUlRhmHj30aX8_nYtEvoL1cduKQv9lXvXtOmoXmGEcr_fJ6Bo9Pc9MP94ghr8992CQ1cEzYQ1ArU1lzlF0_D6ffZmR0xpODdg_zdg=w630-h344-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/8OKMbRHYIsRpnO_xjs14NHiuV0Oaxucy0FOBScUlRhmHj30aX8_nYtEvoL1cduKQv9lXvXtOmoXmGEcr_fJ6Bo9Pc9MP94ghr8992CQ1cEzYQ1ArU1lzlF0_D6ffZmR0xpODdg_zdg=w630-h344-no&quot; border=&quot;0&quot; alt=&quot; photo xThetaComp_zpsbfgfyxgc.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 5 :: Classification on X-Heading residuals   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/0XMqB5ZgIgZO_BtLamjJcynxbYVNFn94whs3uXc_LiDDXai5jKd1BFo2CBxt5c6M54tgRKF9jpiNP3t1lA8NKQDtE42EszqMgRYttHJcNIWivSmdpa7U4FScBH5ksqf4Ml8p9T_d9A=w630-h344-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/0XMqB5ZgIgZO_BtLamjJcynxbYVNFn94whs3uXc_LiDDXai5jKd1BFo2CBxt5c6M54tgRKF9jpiNP3t1lA8NKQDtE42EszqMgRYttHJcNIWivSmdpa7U4FScBH5ksqf4Ml8p9T_d9A=w630-h344-no&quot; border=&quot;0&quot; alt=&quot; photo yThetaComp_zpsx4y9vdrm.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 6 :: Classification on Y-Heading residuals   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;From the three figures above, it can be seen that the BMM DP is categorizing the 
data into two bins ( i.e., inlier or outlier ) without being instructed that 
there are two categories present in the data-set.&lt;/p&gt;

&lt;h3 id=&quot;next-steps-&quot;&gt;Next Steps ::&lt;/h3&gt;

&lt;p&gt;Next, the mixture components provided by the BMM DP algorithm, which are  shown 
below, will be incorporated into the max-mixtures framework to see how well this
Gaussian mixture preforms.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_I = [ 0.04946242, -0.05057746, -0.0594329 ]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P_I =
  \begin{bmatrix}
    16.89882198 &amp;&amp; -0.90303439 &amp;&amp;  0.08602105 \\
   -0.90303439 &amp;&amp; 17.08403007  &amp;&amp; 0.06889149 \\
   0.08602105 &amp;&amp;  0.06889149 &amp;&amp; 10.95290421
  \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_O = [ 0.056539, -0.35273277, -0.83455116 ]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P_O =
  \begin{bmatrix}
     2087.43888892 &amp;&amp; -651.08477092 &amp;&amp;  198.82917436 \\
   -651.08477092 &amp;&amp; 1977.03231206  &amp;&amp; -92.72713996 \\
   198.82917436 &amp;&amp;  -92.72713996 &amp;&amp; 1264.0179354
  \end{bmatrix} %]]&gt;&lt;/script&gt;

</description>
        <pubDate>Wed, 21 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/06/21/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/06/21/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Density Estimation</category>
        
        <category>Mixture Model</category>
        
        <category>Dirichlet Process</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Initial Testing of Bayes Mixture Model with Dirichlet Process</title>
        <description>&lt;h1 id=&quot;initial-testing&quot;&gt;Initial Testing&lt;/h1&gt;

&lt;p&gt;As one method to robustly optimize when confronted with erroneous data, we are 
testing clustering algorithms to learn the true residual distribution, which 
will — hopefully — allow us to properly de-weight faulty observables. All of 
the code used for this initial testing is housed 
&lt;a href=&quot;https://github.com/watsonryan/summerAFIT/tree/master/bayes_gmm&quot;&gt;HERE&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;data-generation&quot;&gt;Data Generation&lt;/h3&gt;

&lt;p&gt;To begin testing the Gaussian Mixture Model with Dirichlet Process for outlier 
cluster, a simple 2D data-set was generated. This data-set can be see in Figure 1.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/n0IFvZrMP-A_nJlNaxaCBcmGvHWyKGqp1zrMpup7Nzs9kaFKouVzfhxSOGNcp9XUwZOeuJgApgCsalw6h2Skfm9UkbiUaCgamzxAvLirMA10fEJVaLz8QMbQkudxu-o9m0DY8DFehw=w630-h355-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/n0IFvZrMP-A_nJlNaxaCBcmGvHWyKGqp1zrMpup7Nzs9kaFKouVzfhxSOGNcp9XUwZOeuJgApgCsalw6h2Skfm9UkbiUaCgamzxAvLirMA10fEJVaLz8QMbQkudxu-o9m0DY8DFehw=w630-h355-no&quot; border=&quot;0&quot; alt=&quot;true photo true_zpslveuybzz.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Generated data-set  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;testing&quot;&gt;Testing&lt;/h3&gt;

&lt;p&gt;For an initial test, both the inlier and outlier distributions are sampled 
evenly (i.e., 100 data points were selected from each distribution). The clustering 
results are shown below in figure 2. What is interesting to note is that the number 
of clusters was not specified; however, the iterative algorithm correctly classified 
both distributions without adding addition partitions to the data-set.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/3vTA_STPi7iPA66wi1rjK7eBZzWYbs7Gto2drc_pVm92rJi1y_580LGqlWxWwkdga5e8MKCxuSeW6zFCfU46cLy-XyhS1hD1kkIokCHhKREk4NvNvnbJ_t9c4ltIHwZusB2fmyKAwA=w630-h344-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/3vTA_STPi7iPA66wi1rjK7eBZzWYbs7Gto2drc_pVm92rJi1y_580LGqlWxWwkdga5e8MKCxuSeW6zFCfU46cLy-XyhS1hD1kkIokCHhKREk4NvNvnbJ_t9c4ltIHwZusB2fmyKAwA=w630-h344-no&quot; border=&quot;0&quot; alt=&quot; photo igmmT1_zps3cxyalcv.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: DP GMM Initial Test  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We can also look at the run-time of the collapsed Gibbs sampler. For the case 
were our data-set is composed of 200 data points, the result is shown below in 
figure 3. This shows that the collapsed Gibbs sampling is fairly consistent, with
respect to time, over all iterations.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/mlQh9s_bSM6I_y6cEv75Yx_-xut8-FP3rOHDP-b_6Xsl_FqkQcNcKsNZiOnDRvVwv744wveL62qPCewznqmi8eWzZddF7yovcTEHCTrfjpzYXDOUbJLYabN9WkpDgprl5oQLdbOspw=w630-h312-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/mlQh9s_bSM6I_y6cEv75Yx_-xut8-FP3rOHDP-b_6Xsl_FqkQcNcKsNZiOnDRvVwv744wveL62qPCewznqmi8eWzZddF7yovcTEHCTrfjpzYXDOUbJLYabN9WkpDgprl5oQLdbOspw=w630-h312-no&quot; border=&quot;0&quot; alt=&quot; photo itialTime_zpshhv3qg1x.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 3 :: Collapsed Gibbs Sampling Initial Time Test  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;However, were not necessarily interesting in the run-time of this implementation 
because it will need to be re-written later. So, something more beneficial to look 
at may be the mean iteration time as the size of the data-set grows. This is depicted 
in Figure 4, where a clear linear trend is shown between the data-set size 
and the iteration time.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/bNjcwx9Ffl32TyqeOuRKnfgRVn0b9mPIIXiO9LFrWJfvR_hblJrOJFciEZpyNn_QtAX0PraibhKz2Tu41CQFM3cr59ufSif35uDpu4Y0DtTiOjelGXDB0Z-gmLGMGy92D-MozRdyjw=w630-h312-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/bNjcwx9Ffl32TyqeOuRKnfgRVn0b9mPIIXiO9LFrWJfvR_hblJrOJFciEZpyNn_QtAX0PraibhKz2Tu41CQFM3cr59ufSif35uDpu4Y0DtTiOjelGXDB0Z-gmLGMGy92D-MozRdyjw=w630-h312-no&quot; border=&quot;0&quot; alt=&quot; photo timeComp_zpskemvd6qx.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 4 :: Mean Iteration Time of Gibbs Sampling  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 15 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/06/15/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/06/15/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Density Estimation</category>
        
        <category>Mixture Model</category>
        
        <category>Dirichlet Process</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Dirichlet Process Mixture Model</title>
        <description>&lt;h1 id=&quot;dirichlet-process-gaussian-mixture-model&quot;&gt;Dirichlet Process Gaussian Mixture Model&lt;/h1&gt;

&lt;p&gt;This tutorial is provided to give a brief overivew of the Dirichlet Process Gaussian Mixture Model ( DP GMM ) and prerequisite information.
&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;prerequisite-information&quot;&gt;Prerequisite Information&lt;/h1&gt;

&lt;p&gt;Before diving into the DP-GMM, we will first discuss some prerequisite information. We will start by defining the multivariate Gaussian and its likelihood function. Next, a brief overview of the conjugate priors will be provided. Finally, we will discuss two commonly used distributions: the inverse-Wishart and the Dirichlet distribution.&lt;/p&gt;

&lt;h2 id=&quot;multivariate-gaussian&quot;&gt;Multivariate Gaussian&lt;/h2&gt;

&lt;p&gt;The multivariate Gaussian is simply a generalization of the univariate Guassian to higher dimensions. To define this slightly more formally, a vector a real-valued random variables, $\mathcal{X} = [ X_1, X_2 \ldots, X_n]$, has a Gaussian distribution with mean $\mu \in \mathcal{R}^n$ and covariance $\Sigma \in P^n_{++}$ — $P^n_{++}$ is a manifold composed of all symmetric positive definite nxn matricies (i.e., $P^n_{++} \in SO(n))$ — if it’s distribution can be charaterized by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x | \mu,\Sigma) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp^{(-\frac{1}{2}|| x-\mu||_{\Sigma}^{2} )}.&lt;/script&gt;

&lt;h3 id=&quot;multivariate-gaussian-likelihood&quot;&gt;Multivariate Gaussian Likelihood&lt;/h3&gt;

&lt;p&gt;Give a mean $\mu$ and covariance $\Sigma$ we can calcuate the likelihod of a set of random vectors $\mathcal{X} = [X_1, X_2, \ldots, X_n]$ by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathcal{X} | \mu,\Sigma) = \prod^N_{n=1} \mathcal{N}(X_n|\mu,\Sigma),&lt;/script&gt;

&lt;p&gt;which can be represents as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathcal{X} | \mu,\Sigma) = \frac{1}{(2\pi)^{ND/2} (|\Sigma|^{N/2})}e^{-\frac{N}{2}|| \mu - \bar{x}||_{\Sigma}} e^{-\frac{1}{2} tr(\Sigma^{-1}S_{\bar{x}})},&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{x} = \frac{1}{N} \sum^N_{n=1} x_n&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_{\bar{x}} = (x_n - \bar{x})(x_n - \bar{x})^{T}.&lt;/script&gt;

&lt;h2 id=&quot;conjugate-prior&quot;&gt;Conjugate Prior&lt;/h2&gt;

&lt;p&gt;Conjugate priors are widely used because they simplify computation (i.e., they allow us to analytically integrate out latent variables and only sample parameters of interest through collapsed Gibbs sampling, which will be discussed in greater detail later).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Def:&lt;/strong&gt;
A family $\mathcal{F}$ of prior distributions, $p(\theta)$, is conjugate to a likelilhood, $p(\theta | \mathcal{D})$, if the posterior, $p(\theta | \mathcal{D})$,  is in $\mathcal{F}$.&lt;/p&gt;

&lt;p&gt;With that in mind, we will next define the Gaussian inverse Wishart and the Dirichlet distribution, which are two commonly used conjugate priors for the multivariate Gaussian and the catagorical distributions, respectively.&lt;/p&gt;

&lt;h2 id=&quot;gaussian-inverse-wishart&quot;&gt;Gaussian Inverse Wishart&lt;/h2&gt;

&lt;p&gt;For the mean $\mu$ and covariance matrix $\Sigma$ of a multivariate Gaussian, the Gaussian-inverse-Wishart
(GIW) prior is fully conjugate,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;GIW(\mu,\Sigma | m_o, \kappa_o, \nu_o, S_o) := \mathcal{N}(\mu | m_o, \frac{1}{\kappa_o}\Sigma) IW(\Sigma| S_o,\nu_o) ,&lt;/script&gt;

&lt;p&gt;which can be represented as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;GIW(\mu,\Sigma | m_o, \kappa_o, \nu_o, S_o) =  e^{-\frac{\kappa_o}{2}|| \mu - m_o||_{\Sigma}} e^{-\frac{1}{2} tr(\Sigma^{-1}S_o)} \frac{1}{Z_{GIW}(D,\kappa_o,\nu_o,S_o} |\Sigma|^{-\frac{\nu_o+D+2}{2}},&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_{GIW}(D,\kappa_o,\nu_o,S_o) = 2^{\frac{(\nu_o+1)D}{2}}\pi^{\frac{D(D+1)}{4}}\kappa_o^{-D/2} \|S_o\|^{-\nu_o/2} \prod_{d=1}^{D} \Gamma(\frac{\nu_o+1-d}{2})&lt;/script&gt;

&lt;h4 id=&quot;parameter-definition&quot;&gt;Parameter Definition&lt;/h4&gt;

&lt;p&gt;$m_o$ –&amp;gt; Prior mean for $\mu$&lt;/p&gt;

&lt;p&gt;$\kappa_o$ –&amp;gt; belief in $m_o$&lt;/p&gt;

&lt;p&gt;$S_o$ –&amp;gt; prior $\Sigma$&lt;/p&gt;

&lt;p&gt;$\nu_o$ –&amp;gt; belief in $S_o$&lt;/p&gt;

&lt;h2 id=&quot;dirichlet-distribution&quot;&gt;Dirichlet Distribution&lt;/h2&gt;

&lt;p&gt;To define a conjugate prior for the multinomial distribution, the Dirichlet distribution is commonly utilized. The Dirichlet distribution is a distribution over possible parameter vectors for a multinomial distribution (e.g., the Beta distribution is a special case of the Dirichlet distribution when n = 2). The Dirichlet distribution defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta \sim \text{Dir}(\alpha) \quad \text{if} \quad p(\theta | \alpha) = \frac{\Gamma(\sum_i^n(\alpha_i))}{\prod_i^n(\Gamma(\alpha_i))}\prod_{i=1}^{n}\theta_i^{\alpha_i-1} \mathcal{I}(\theta \in S),&lt;/script&gt;

&lt;p&gt;where $\theta = (\theta_1, \ldots, \theta_n)$, $\alpha = (\alpha_1, \ldots, \alpha_n)$ s.t. $\alpha_i &amp;gt; 0$, and $S$ is the probability simplex. A simplex is simply a generalization of the triangle to n-dimensional space (i.e., $S = \lbrace \alpha \in \mathcal{R}^n : \alpha_i \geq 0 : \sum_i^n \alpha_i = 1 \rbrace$ ).&lt;/p&gt;

&lt;p&gt;With that brief description of the Dirichlet distribution, it is useful to visualize an example case. To do this, a simple python script that plots the Dirichlet density when n = 3 (i.e., a situation when three clusters are present in dataset) has been provided. The figure below shows the density when $\alpha = $ , [1,1,1], [5,5,5], and [2,2,8]. As can be seen, when $\alpha = $ [1,1,1] there is a uniform likelihood for each cluster, (i.e., there is no prior knowledge); however, as the $\alpha$’s very, the density can be seen to shift around the simplex.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/mTtXL49uTh_epEHCi1fKZa8eT3KBO_RAELbBkNSg-Os6RROyLqfMUZUGnXpBO54Cqo4KAADkLF2KLc8WvvrWHWwOPnNU8Vq05_kyyg6nTllcMa9p41Nw5TVeX0N8mnrYicYbPUL2xg=w630-h181-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/mTtXL49uTh_epEHCi1fKZa8eT3KBO_RAELbBkNSg-Os6RROyLqfMUZUGnXpBO54Cqo4KAADkLF2KLc8WvvrWHWwOPnNU8Vq05_kyyg6nTllcMa9p41Nw5TVeX0N8mnrYicYbPUL2xg=w630-h181-no&quot; border=&quot;0&quot; alt=&quot; photo dirDensity_zpsozji7b0u.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Dirichlet density  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mixture-models&quot;&gt;Mixture Models&lt;/h1&gt;

&lt;p&gt;With the overview provided above, we can now move to mixture models for data clustering. This section will start with a finite mixture model and move to an infinite mixture model to overcome the inherent difficultly of selecting the number of data partitions.&lt;/p&gt;

&lt;h2 id=&quot;finite-mixture-model&quot;&gt;Finite Mixture Model&lt;/h2&gt;

&lt;p&gt;The model that will be utilized for the finite Bayesian mixture model is shown in the figure below. For each observed data vector $x_i$ , we have a latent variable $z_i \in [1, 2, . . . , K]$ indicating which of the K components $x_i$ belongs to. $ \pi_i = P(z = k)$ is the prior probability that $x_i$ belongs to component $k_i$. Given $z_i = k$, $x_i$ is generated by the $k^{th}$ Gaussian mixture component with mean vector $\mu_k$ and covariance matrix $\Sigma_k$.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/9TFTxhxtKe87M01N6Oa1qNc_Q0jNrBwwZt5J_a5bBwHJibZsKWWTe6-Ad_2BpqOXzc_LwgHWy6xA-inQYBsa-Zjr7Wa0yXxevclhi8xk0SRWgevhCn2pDEJ14Eq6G0D5U3jXPmJSsA=w136-h235-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/9TFTxhxtKe87M01N6Oa1qNc_Q0jNrBwwZt5J_a5bBwHJibZsKWWTe6-Ad_2BpqOXzc_LwgHWy6xA-inQYBsa-Zjr7Wa0yXxevclhi8xk0SRWgevhCn2pDEJ14Eq6G0D5U3jXPmJSsA=w136-h235-no&quot; border=&quot;0&quot; alt=&quot; photo finiteMixture_zps8bj7ff6i.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Finite Mixture Model  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We can provide a prior for our mixing weights, $\pi$ , using the Dirichlet distribution (i.e., $p(\pi | \alpha) = \text{Dir}(\pi | \alpha)$). We can also provide a prior for our mixture components using using the inverse Gaussian Wishart distribution  (i.e., $p(\mu_k, \Sigma_k | m_o, \kappa_o, \nu_o, S_o) = GIW( \mu_k, \Sigma_k | m_o, \kappa_o, \nu_o, S_o) )$.&lt;/p&gt;

&lt;h4 id=&quot;collapsed-gibbs-sampling&quot;&gt;Collapsed Gibbs Sampling&lt;/h4&gt;

&lt;p&gt;Because we selected $\pi | \alpha$ and $p(\mu_k,\Sigma_k | \beta)$ — where we define the hyper-parameter $\beta = (m_o,\kappa_o,\nu_o,S_o)$ — to be conjugate, we can analytically integrate out the parameters $\pi$, $\mu_k$, and $\Sigma_k$. This allows us to dramatically reduce our sampling space to component assignmetns $z$. This is represented by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(z_i = k | z_{\ i}, \mathcal{X}, \alpha, \beta) \propto p(z_i = k | z_{\ i} \alpha) p (x_i | \mathcal{X}_{\ i}, z_i = k, z_{\ i}, \beta) ,&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z_i = k| z_{\ i}, \alpha) = \frac{ (N_k - 1) + \frac{\alpha}{K} }{N + \alpha -1}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p (x_i | \mathcal{X}_{\ i}, z_i = k, z_{\ i}, \beta) = \int_{\mu_k} \int_{\Sigma_k} p(\mathcal{X}_k | \mu_k, \Sigma_K) p(\mu_k, \Sigma_k | \beta) d\mu_k d\Sigma_k .&lt;/script&gt;

&lt;p&gt;Psudo code for collapsed Gibbs sampling for a finite GMM is given below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/dJcyfLTXmQZn0RxJuTQEHv2IYacyGHC-HLWVb8n4UdZ8pMjvr8w8pxUhaIUlomjqWdJ8pHRNmUu_ey-73ZbTQ4EbBGSEGPbeD-bpnlcd7HWsUAY_yeLiaZC7mBZQxVt0Ly093axaZw=w630-h278-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/dJcyfLTXmQZn0RxJuTQEHv2IYacyGHC-HLWVb8n4UdZ8pMjvr8w8pxUhaIUlomjqWdJ8pHRNmUu_ey-73ZbTQ4EbBGSEGPbeD-bpnlcd7HWsUAY_yeLiaZC7mBZQxVt0Ly093axaZw=w630-h278-no&quot; border=&quot;0&quot; alt=&quot; photo algo1_zps40azobjo.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 3 :: Finite Mixture Algorithm  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;infinite-mixture-model&quot;&gt;Infinite Mixture Model&lt;/h2&gt;

&lt;h4 id=&quot;chinese-restaurant-process&quot;&gt;Chinese Restaurant Process&lt;/h4&gt;

&lt;p&gt;Before we dive into the details of the infinite mixture model, we will first describe the Dirichlet process through the Chinese restaurant problem. This construct is a commonly used one is statistics. It is described as customers seating themselves in a resturant with an infinite number of tables.  This process has a &lt;em&gt;rich-get-richer&lt;/em&gt; property, that is, tables with more people have a higher probabily of getting more people. The &lt;em&gt;rich-get-richer&lt;/em&gt; property can be more formally defined as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z_i = k | z_{\ i} , \alpha ) =\left\{
                \begin{array}{ll}
                  \frac{N_k}{N+\alpha-1} \quad \text{if k is occupied} \\
                  \frac{\alpha}{N+\alpha-1} \quad \text{if k is a new table}
                \end{array}
              \right.&lt;/script&gt;

&lt;p&gt;Now, we will move to the infinite mixture model. This model is closely related to the finite mixture model, with the exception being that the Dirichlet process is now used to define the mixing weight priors. This allows us to circimvent the issue of defining the number of partitions by allowing the model to select from an infinite number of partitions.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/eswxi4_Pb2f76WtmCb63z8NWjvAoaonYQkx9OzKmiyNzDtmMIa72mSCkQczSR71RsHNh6r4YrPJAmjGS8V3DKP0p7fhWcEc0MCxUTwkL0THapO4oh7FcEE7R-u1MgfCw2aVbpQCHfw=w132-h232-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/eswxi4_Pb2f76WtmCb63z8NWjvAoaonYQkx9OzKmiyNzDtmMIa72mSCkQczSR71RsHNh6r4YrPJAmjGS8V3DKP0p7fhWcEc0MCxUTwkL0THapO4oh7FcEE7R-u1MgfCw2aVbpQCHfw=w132-h232-no&quot; border=&quot;0&quot; alt=&quot; photo inifiteMixture_zpsybjiovvy.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 4 :: Infinite Mixture Model  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;collapsed-gibbs-sampling-1&quot;&gt;Collapsed Gibbs Sampling&lt;/h4&gt;

&lt;p&gt;Again, because we selected $p(\pi | \alpha)$ and $p(\mu_k,\Sigma_k | \beta)$ — where we define the hyper-parameter $\beta = (m_o,\kappa_o,\nu_o,S_o)$ — to be conjugate, we can analytically integrate out the parameters $\pi$, $\mu_k$, and $\Sigma_k$. This is represented by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(z_i = k | z_{\ i}, \mathcal{X}, \alpha, \beta) \propto p(z_i = k | z_{\ i} \alpha) p (x_i | \mathcal{X}_{\ i}, z_i = k, z_{\ i}, \beta) ,&lt;/script&gt;

&lt;p&gt;where,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z | \alpha) = \frac{\alpha^K \prod_{k=1}^K (N_k-1)!}{\prod_{n=1}{N} ( i-1+\alpha}),&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_i | \mathcal{X_{\ i}}, z_i = k, z_{\ i}, \beta) = p(x_i,\beta) = \int_{\mu}\int_{\Sigma} p(x_i | \mu, \Sigma)p(\mu,\Sigma | \beta)d\mu d\Sigma.&lt;/script&gt;

&lt;p&gt;Psudo code for collapsed Gibbs sampling for a infinite GMM is given below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/b3N9-r2dcZzGx0XCCzVhnpoKoB7MGFl-a94aVTP3Gybdu_g_ziBQC2DrNN8ucNAY6WkcGreDMLS18gOr5BFH8nTpYfkYXwzHBWna_3YUUNZitH9A7Mg9zhX7a851LpSKeYgOa56J2Q=w630-h350-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/b3N9-r2dcZzGx0XCCzVhnpoKoB7MGFl-a94aVTP3Gybdu_g_ziBQC2DrNN8ucNAY6WkcGreDMLS18gOr5BFH8nTpYfkYXwzHBWna_3YUUNZitH9A7Mg9zhX7a851LpSKeYgOa56J2Q=w630-h350-no&quot; border=&quot;0&quot; alt=&quot; photo inifModel_zpszslpm6g2.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 5 :: Infinite Mixture Algorithm  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 14 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/06/14/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/06/14/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Dirichlet</category>
        
        <category>Density Estimation</category>
        
        <category>Mixture Model</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Initial Testing With Collected Data</title>
        <description>&lt;h1 id=&quot;initial-testing&quot;&gt;Initial Testing&lt;/h1&gt;

&lt;p&gt;With two collected datasets, we can begin characterizing our SLAM algorithm.&lt;/p&gt;

&lt;h2 id=&quot;indoor&quot;&gt;Indoor&lt;/h2&gt;

&lt;p&gt;First, we will look at the SLAM solution using only the lidar data. This solution 
is shown below in the first figure. This figure depicts the generated map on the left and 
the pose solution on the right.&lt;/p&gt;

&lt;h3 id=&quot;lidar-solution&quot;&gt;Lidar Solution&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/wSclDDBh4lCFsDNwF0-1jdMeWTzfo-oEp-fJFSgQA1jeOKcnZp52ruFa2UKQ8HWcintG8cBWz0fWXTX8fwXqUpQrldisR4MK67_nSoN68dhKJVyFhITXaoYvU_MWAIlnMafHDyd2NQ=w630-h329-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/wSclDDBh4lCFsDNwF0-1jdMeWTzfo-oEp-fJFSgQA1jeOKcnZp52ruFa2UKQ8HWcintG8cBWz0fWXTX8fwXqUpQrldisR4MK67_nSoN68dhKJVyFhITXaoYvU_MWAIlnMafHDyd2NQ=w630-h329-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Initial indoor testing   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;gps-solution&quot;&gt;GPS Solution&lt;/h3&gt;

&lt;h2 id=&quot;outdoor&quot;&gt;Outdoor&lt;/h2&gt;

&lt;p&gt;Now, we can look at the outdoor dataset.&lt;/p&gt;

&lt;h3 id=&quot;lidar-solution-1&quot;&gt;Lidar Solution&lt;/h3&gt;

&lt;p&gt;Again, we will begin by looking at SLAM solution when only utilizing the lidar 
data. This solution is depicted in Figure 2. This figure depicts the generated 
map on the left and the pose solution on the right.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/xLla8OeL4tI1tpzrNbxj-N9pIaYTRc80P2d2-XvjMkPizVihH1LiyREoVbj2vcTY_2_7HD7yyti_-Iy4zopycewSHlLNzUN1IRHw9SC6rv7zYEwkiR0RvVWptENEx-8jzvNwyjMdnw=w630-h148-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/xLla8OeL4tI1tpzrNbxj-N9pIaYTRc80P2d2-XvjMkPizVihH1LiyREoVbj2vcTY_2_7HD7yyti_-Iy4zopycewSHlLNzUN1IRHw9SC6rv7zYEwkiR0RvVWptENEx-8jzvNwyjMdnw=w630-h148-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-Fadm3d0kM4o_zpshkgymxno.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Initial outdoor testing   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;gps-solution-1&quot;&gt;GPS Solution&lt;/h3&gt;

&lt;h3 id=&quot;comparison&quot;&gt;Comparison&lt;/h3&gt;
</description>
        <pubDate>Wed, 14 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/labupdates/2017/06/14/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/labupdates/2017/06/14/Update.html</guid>
        
        <category>labUpdates</category>
        
        <category>bumble bot</category>
        
        <category>blam test</category>
        
        
        <category>_projects</category>
        
        <category>labUpdates</category>
        
      </item>
    
      <item>
        <title>Density Estimation</title>
        <description>&lt;p&gt;As has been shown over previous days, the m-estimator is fairly sensitive to 
both the user specified kernel width and the dataset. So, today we move our focus 
to methods that can estimate the correct data distribution as a first step to 
move toward an adaptive estimator.&lt;/p&gt;

&lt;h3 id=&quot;density-estimination&quot;&gt;Density Estimination&lt;/h3&gt;

&lt;p&gt;Assume that we have n observations which are realizations of univariate 
random variables,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_1, \ldots, X_n \quad \text{i.i.d} \quad \sim  \quad F,&lt;/script&gt;

&lt;p&gt;where F is the cumulative distribution function. The goal of this field is to 
estimate the density,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{f}(x) = \frac{d}{dx}\hat{F}(x) \quad \quad \text{where} \quad \quad  \hat{F}(x) = \frac{1}{n}\sum_i^n I(X_i \leq x).&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;gaussian-mixture-model&quot;&gt;Gaussian Mixture Model&lt;/h4&gt;

&lt;p&gt;One method of density estimation is the Gaussian Mixture Model (GMM). This model 
assumes, as the name suggests, that the data points are generated by a mixture 
of finitely many Gaussian distributions with unknown parameters, as shown below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \sum_i^n \omega_i \mathcal{N}(X|\mu_i,\Sigma_i)&lt;/script&gt;

&lt;p&gt;One method to solve GMM is to utilized the Expectation-Maximization (EM) algorithm. 
This method iterates between expectation and maximization steps to find the 
correct parameters. The expectation steps holds the current estimate of the 
Gaussian parameters constant to posterior distribution of the latent variables 
$\omega$. Next, the maximization steps utilizes the $\omega$ values to find the
new parameters of the Gaussian by maximizing the Eq. shown below,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta^{K+1} = \text{argmax} \int_{\Omega} p(\Omega|\Theta,|Z) \text{ln} p(\Theta|\Omega,Z)d\Omega&lt;/script&gt;

&lt;p&gt;where $\Omega$ is the set of weights, $\Theta$ is the set of Gaussian parameters,
and Z is an indicator function which takes a value of zero or one.&lt;/p&gt;

&lt;p&gt;One notable disadvantange of GMM is,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When one has insufficiently many points per mixture, estimating the covariance 
matrices becomes difficult, and the algorithm is known to diverge and find 
solutions with infinite likelihood unless one regularizes the covariances 
artificially,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;which is significant when utilizing GMM for data rejection. This is due to the 
hope that outlier distribution should have substantially fewer data points then 
the inlier dataset.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;bayesian-gaussian-mixture-with-dirichlet-distribution&quot;&gt;Bayesian Gaussian Mixture With Dirichlet Distribution&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;testing&quot;&gt;Testing&lt;/h3&gt;

&lt;p&gt;To begin testing, a simple dataset was generated from two independent Gaussian distributions.
This sample dataset is depicted in Fig. 1.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/n0IFvZrMP-A_nJlNaxaCBcmGvHWyKGqp1zrMpup7Nzs9kaFKouVzfhxSOGNcp9XUwZOeuJgApgCsalw6h2Skfm9UkbiUaCgamzxAvLirMA10fEJVaLz8QMbQkudxu-o9m0DY8DFehw=w630-h355-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/n0IFvZrMP-A_nJlNaxaCBcmGvHWyKGqp1zrMpup7Nzs9kaFKouVzfhxSOGNcp9XUwZOeuJgApgCsalw6h2Skfm9UkbiUaCgamzxAvLirMA10fEJVaLz8QMbQkudxu-o9m0DY8DFehw=w630-h355-no&quot; border=&quot;0&quot; alt=&quot;true photo true_zpslveuybzz.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Generated dataset  
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using this dataset, the Gaussian Mixture Model (GMM) and the Bayesian Gaussian 
Mixture Model with a Dirichlet Process (BGMM) were tested. The result when 
both the inlier and outlier distributions are sampled evenly is shown in the 
figure below. When each cluster has the same number of samples, both models 
perform well.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/zwZVCS0FviNf2K5BqcjRa2lBP9DCEaFS3zqOY5xmeIL1Wfk54JwlOg577maFIQlXFHo6vS0iDwJ6DV4MyqFzNrdJIu1ZhDgy4UXlCwysW0vxio6pEg4Hr1yuLSoJ7oVygL-Ofc4Ljg=w630-h344-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/zwZVCS0FviNf2K5BqcjRa2lBP9DCEaFS3zqOY5xmeIL1Wfk54JwlOg577maFIQlXFHo6vS0iDwJ6DV4MyqFzNrdJIu1ZhDgy4UXlCwysW0vxio6pEg4Hr1yuLSoJ7oVygL-Ofc4Ljg=w630-h344-no&quot; border=&quot;0&quot; alt=&quot;5000Faults photo 5000_zpssoowqaxi.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Testing GMM and BGMM when ratio of faulty to clean observables is 1
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As noted above, GMM can perform poorly when insufficiently many points are 
included in each mixture. To test this, the outlier distribution was down sampled
to see when GMM begins to fail. To test this statement, the next three plots
were included.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/zwZVCS0FviNf2K5BqcjRa2lBP9DCEaFS3zqOY5xmeIL1Wfk54JwlOg577maFIQlXFHo6vS0iDwJ6DV4MyqFzNrdJIu1ZhDgy4UXlCwysW0vxio6pEg4Hr1yuLSoJ7oVygL-Ofc4Ljg=w630-h344-no=w1024-h558-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/zwZVCS0FviNf2K5BqcjRa2lBP9DCEaFS3zqOY5xmeIL1Wfk54JwlOg577maFIQlXFHo6vS0iDwJ6DV4MyqFzNrdJIu1ZhDgy4UXlCwysW0vxio6pEg4Hr1yuLSoJ7oVygL-Ofc4Ljg=w630-h344-no&quot; border=&quot;0&quot; alt=&quot;5000Faults photo 5000_zpssoowqaxi.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 3 :: Testing GMM and BGMM when ratio of faulty to clean observables is 0.1 
&lt;/p&gt;
&lt;br /&gt;


When the ratio of faulty to clean observables is down sampled to 0.01 
(i.e., 5000 inlier samples and 50 outlier samples), you can begin to see that 
GMM is have difficulty discerning between the distributions while the BGMM model 
is still accurately representing both distributions.


&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/0NgTdlJNCbEYq_LSgjHRGKUZYUtjYdoDjhTUgm_NbYl8zeYf0UIWxbCLTTpPOFNtOUTbfB0skl_l9YBYkTuK0gbKnxyOoOG9ljR29x2Nlfaw7gmdt04lbgAI8kH-Ntvidbqt_L190w=w630-h344-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/0NgTdlJNCbEYq_LSgjHRGKUZYUtjYdoDjhTUgm_NbYl8zeYf0UIWxbCLTTpPOFNtOUTbfB0skl_l9YBYkTuK0gbKnxyOoOG9ljR29x2Nlfaw7gmdt04lbgAI8kH-Ntvidbqt_L190w=w630-h344-no&quot; border=&quot;0&quot; alt=&quot;50Faults photo 50_zps6mbljs7o.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 4 :: Testing GMM and BGMM when ratio of faulty to clean observables is 0.01 
&lt;/p&gt;
&lt;br /&gt;


When the ratio of faulty to clean observables is down sampled to 0.005, it can be 
seen that GMM provides an extremely poor estimate of the densities while BGMM still
accurately represents the data.

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/IZdN1H6aG6-tliUkOfxNN6YLfhqaq_xkA21uGvIAc9jJj4TbTXFkrgt6SLwsX8MHe-xZ4W-YaQACLZIyIM9nShfCfpAvIIY-U2QJ_IRhsl8cOa2PjYQOqRPAMSIo9lHz1LlacyhVbw=w630-h344-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/IZdN1H6aG6-tliUkOfxNN6YLfhqaq_xkA21uGvIAc9jJj4TbTXFkrgt6SLwsX8MHe-xZ4W-YaQACLZIyIM9nShfCfpAvIIY-U2QJ_IRhsl8cOa2PjYQOqRPAMSIo9lHz1LlacyhVbw=w630-h344-no&quot; border=&quot;0&quot; alt=&quot;25Faults photo 25_zpsgzggfkaw.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 5 :: Testing GMM and BGMM when ratio of faulty to clean observables is 0.005
&lt;/p&gt;
&lt;br /&gt;

&lt;/p&gt;
</description>
        <pubDate>Wed, 07 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/06/07/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/06/07/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Density Estimation</category>
        
        <category>Gausian Mixture Model</category>
        
        <category>Dirichlet Process</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
  </channel>
</rss>
