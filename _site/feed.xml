<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research Notes</title>
    <description></description>
    <link>http://localhost:4000/researchNotes/</link>
    <atom:link href="http://localhost:4000/researchNotes/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 27 Jul 2017 08:19:13 -0400</pubDate>
    <lastBuildDate>Thu, 27 Jul 2017 08:19:13 -0400</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>Expectation Maximization</title>
        <description>&lt;h1 id=&quot;note-overview&quot;&gt;Note Overview&lt;/h1&gt;

&lt;p&gt;For this tutorial I was asked to provide a brief overview of Expectation Maximization (E.M.) and model selection. However, to begin, I will need to start with Maximum Likelihood Estimation (M.L.E.). With that in mind, this set of notes will be organized as follows. First, I will discuss M.L.E and provide a short example that links it back to equations we’ve seen before. Then, I will discuss the E.M. algorithm and give a few applications. Finally, I will briefly talk about model selection and give an applicable example.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;maximum-likelihood-estimation&quot;&gt;Maximum Likelihood Estimation&lt;/h1&gt;

&lt;p&gt;Assume that we are given a dataset $Y= \lbrace y_i, \ldots, y_n \rbrace $ where $y_i \in \mathbf{R}^d,$ where Y’s probability distribution depends on some unknown parameter $\theta \in \Theta$. Then, the likelihood of obserserving this data given a value for $\theta$ is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;lik(\theta, Y) = \prod_i^n p( y_i, \theta),&lt;/script&gt;

&lt;p&gt;and the value of $\theta$ that makes our observed data the most probable is known as the M.L.E.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta} = \text{argmax} \ \text{log} \ p( Y | \theta) = \sum_i^n \text{log} \ p(y_i | \theta)&lt;/script&gt;

&lt;h3 id=&quot;example--batch-estimation&quot;&gt;Example – Batch Estimation&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As we have assume before, let’s suppose that we have a measurement model,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\widetilde{y} = Hx + \nu \quad \text{where} \quad \nu = \mathcal{N}(0,R).&lt;/script&gt;

&lt;p&gt;Because we have defined $\nu$ is Gaussian, $\widetilde{y}$ is also Gaussian. With that knowledge, we may describe the PDF of the measurements as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\widetilde{y}) = \frac{1}{(2\pi)^{n/2} ||R||^{1/2}} e^{-\frac{1}{2}(\widetilde{y} - E[\widetilde{y}])^T R^{-1}(\widetilde{y}-E[\widetilde{y}])}&lt;/script&gt;

&lt;p&gt;where we will define the normalization constant as $c$ for the rest of this example.&lt;/p&gt;

&lt;p&gt;Now, we can find the expectation of $\widetilde{y}$ as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[\widetilde{y}] = E[Hx+\nu] = E[Hx] + E[\nu] = E[Hx] = Hx.&lt;/script&gt;

&lt;p&gt;With this information we can represent the conditional probability of our measurements given our states,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\widetilde{y} | x) = c e^{-\frac{1}{2}(\widetilde{y} - Hx)^T R^{-1}(\widetilde{y}-Hx)} .&lt;/script&gt;

&lt;p&gt;As stated above, the goal of M.L.E is to find the state estimate, $\hat{x}$ that maximizes $p(\widetilde{y} | x)$, which is the likelihood that x resulted in the measurement of $\widetilde{y}$. This means that our objective function is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{max}\ J(x) = c e^{-\frac{1}{2}(\widetilde{y} - Hx)^T R^{-1}(\widetilde{y}-Hx)} .&lt;/script&gt;

&lt;p&gt;Now, we note that maximizing $ p( \widetilde{y} | x ) $ will result in the same solution as maximizing $ ln( p( \widetilde{y} | x ) ) $. Therefore, we modify the objective function by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{max}\ J(x) = ln ( c e^{-\frac{1}{2}(\widetilde{y} - Hx)^T R^{-1}(\widetilde{y}-Hx)} ) \quad = \quad ln(c) -\frac{1}{2}(\widetilde{y} - Hx)^T R^{-1}(\widetilde{y}-Hx).&lt;/script&gt;

&lt;p&gt;We can convert this to a minimization problem by negating the equation above,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{min}\ J(x) = \frac{1}{2}(\widetilde{y} - Hx)^T R^{-1}(\widetilde{y}-Hx) - ln(c),&lt;/script&gt;

&lt;p&gt;which expands to yield,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{min}\ J(x) = \frac{1}{2} [\widetilde{y}^T R^{-1} \widetilde{y} - 2x^T H^T R^{-1} \widetilde{y} + x^T H^T R^{-1} H x ] - ln(c).&lt;/script&gt;

&lt;p&gt;Now, we can apply the first differential condition,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ \partial J(x) } { \partial x } = \frac{1}{2} [ -2 \widetilde{y}^{T} R^{-1} H + 2 x^T H^T R^{-1} H] = x^{T} H^T R^{-1} H - \widetilde{y} R^{-1} H = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{T} H^{T} R^{-1} H = \widetilde{y}^T R^{-1} H&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boxed{ \hat{x} = ( H^{T} R^{-1} H )^{-1} H^{T} R^{-1} \widetilde{y} }&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If we are also interested in the covariance of our estimate, we can find that easily too. We first note that the covariance of a state estimate is given by $P = E[(\hat{x}-\bar{x})(\hat{x}-\bar{x})^{T}].$&lt;/p&gt;

&lt;p&gt;We can start by finding the expression for $\bar{x}$, which is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{x} = E[\hat{x}] = E[( H^{T} R^{-1} H )^{-1} H^{T} R^{-1} (y+\nu)]  = ( H^{T} R^{-1} H )^{-1} H^{T} R^{-1} y.&lt;/script&gt;

&lt;p&gt;Now, we can find an expression for $(\hat{x} - \bar{x}), $ which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{x} - \bar{x} = ( H^{T} R^{-1} H )^{-1} H^{T} R^{-1}(y+\nu) - ( H^{T} R^{-1} H )^{-1} H^{T} R^{-1} y = (H^T R^{-1} H)^{-1} H^{T} R^{-1} \nu&lt;/script&gt;

&lt;p&gt;Finally, plugging the above equation into the covariance equation yields,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P = E[ (H^T R^{-1} H)^{-1} H^{T} R^{-1} \nu \nu^T R^{-1} H(H^{T} R^{-1} H)^{-1} ]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boxed{  = (H^{T}R^{-1}H)^{-1} }&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;additional-links&quot;&gt;Additional Links&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kamperh.com/notes/kamper_matrixcalculus13.pdf&quot;&gt;Vector and Matrix Calculus Refresh&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf&quot;&gt;MLE Properties&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;expectation-maximization&quot;&gt;Expectation Maximization&lt;/h1&gt;

&lt;p&gt;Now, we can move onto Expectation Maximization. Where it should be noted that, all this algorithm is doing is calculating and M.L.E for data when some variables are not observed.&lt;/p&gt;

&lt;h3 id=&quot;motivation-examples&quot;&gt;Motivation Examples&lt;/h3&gt;

&lt;p&gt;As shown above, if we are given a full dataset, we can pretty simple calculate a MLE. However, there are several applications where not provided all of the information need to fully characterize our problem. This issue segue us into the primary reason for utilizing E.M., which is to optimize is the presence of missing information. To provide a little motivation, I have provided two example applications.&lt;/p&gt;

&lt;h4 id=&quot;precise-gnss-data-processing&quot;&gt;Precise GNSS Data Processing&lt;/h4&gt;

&lt;p&gt;The first application is GNSS carrier-phase processing. When precise positioning of a platform is required and GNSS data is being utilized, the carrier-phase observable must be incorporated into your filtering algorithm. This carrier-phase observable can be thought of as a sinusoidal wave being propagated from the satellite to the user; however, you do not know the exact number of wave lengths that you are from the satellite. If the ambiguity can be resolved then cm level positioning is obtained.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/NO-bGGEP-AN3cVUoBDypQ1XRCgOdhutnLHiTA6LB4IlyNaTmThwGRQs-SvAk8rl_azQaGCH5aXE9gm0WbRNBOPwxFaSFpH6ndcCmz3PcT_ooPOdjcGcAQhUcWkDVEAvdG3lKg6pBH4jRZnX8F2_6J-W5Mxp0YuSWftwN3tBCQDCe8eLGIHccJduBIYAUK_3Osa7RMn9qwCgx8Ut-7ApEs4fR7647wtInZfExMrsl53Dhc_uRrLmNZMy1Stj5eYWZ-9vGVtRt3ogwVoYtgxrUt9DzPktrpGfkNwb_2kn4sMxDISqsUZdXJ0GBhyrQ4pcCsNEyDG46XONJdIZFlebPESPt77Q2or7VTZqmHJYdiQLdLE-gKRvTNt-W7XVT3K-mqPGpn6_fXoSe5Ek3C6ZwlcTI242oRdrL0quFxpFjIWrdO3ZK_jTs-XWKB2uuUZnaDhvEXbsUS8LsC3hbbsFBuOCKe4UKRnig5xdL1gDGEIyW7fCMNdsTPZCn8LhzIqs16iHv8mTjK_5kqW-RnT-PycN2BBVJ8GkM-TTLsJcsEymq22idG_5fDqez23zcjMf_fyEXoyGhrVL-I2BqonyL4UlPmokS1_aP-HkEbVjWQpl-l5H7sL1OKaObbFwAaBPC0O29uPu9-UxF-VIY10DCDH8SX-YFIcz73lWIAXAngI0dIw=w1033-h363-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/NO-bGGEP-AN3cVUoBDypQ1XRCgOdhutnLHiTA6LB4IlyNaTmThwGRQs-SvAk8rl_azQaGCH5aXE9gm0WbRNBOPwxFaSFpH6ndcCmz3PcT_ooPOdjcGcAQhUcWkDVEAvdG3lKg6pBH4jRZnX8F2_6J-W5Mxp0YuSWftwN3tBCQDCe8eLGIHccJduBIYAUK_3Osa7RMn9qwCgx8Ut-7ApEs4fR7647wtInZfExMrsl53Dhc_uRrLmNZMy1Stj5eYWZ-9vGVtRt3ogwVoYtgxrUt9DzPktrpGfkNwb_2kn4sMxDISqsUZdXJ0GBhyrQ4pcCsNEyDG46XONJdIZFlebPESPt77Q2or7VTZqmHJYdiQLdLE-gKRvTNt-W7XVT3K-mqPGpn6_fXoSe5Ek3C6ZwlcTI242oRdrL0quFxpFjIWrdO3ZK_jTs-XWKB2uuUZnaDhvEXbsUS8LsC3hbbsFBuOCKe4UKRnig5xdL1gDGEIyW7fCMNdsTPZCn8LhzIqs16iHv8mTjK_5kqW-RnT-PycN2BBVJ8GkM-TTLsJcsEymq22idG_5fDqez23zcjMf_fyEXoyGhrVL-I2BqonyL4UlPmokS1_aP-HkEbVjWQpl-l5H7sL1OKaObbFwAaBPC0O29uPu9-UxF-VIY10DCDH8SX-YFIcz73lWIAXAngI0dIw=w1033-h363-no&quot; border=&quot;0&quot; alt=&quot; photo inifModel_zpszslpm6g2.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Factor Graph With Only Pseudorange Observables   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/gTnmMmc_sAWel1lZQ5LJXxeSPE999WuMpyFt-7qWdw1yhWanjMGYy7QH4cR5On29K5-kj5LXjy4voOGRZQCqLtJFMZZVYmQCpygft5XcFDWCyNH-UXvMdW-u7MIMsEAVkI3HFJINmWbiP6-JG-ws0glkf0e8icBiWQxqOcqbpwJxiMx8AnQBKxHEbKHCy2FuqBb7qjJ66PHvnpt-k9gq02d024sIzKF3Bqrk_2MQD7l4k7J_mQXZb0AmJhufnGLuA3PZNbfg_kX56-gGZnvz5FAspJZgqxGUMa7UBtgNxLw9-3Aj727oGqt9FAg7eubLY9QRjy-2AS2rwPI8vsBMY7G_nhuXeRfpAdKkoKUNwmDZhc7JiMbJxTQid223Xz6MpTWD01CePfM8asMm5tfke-mTzH8EzdSCT3A3liIJnRAsQi1yyt7dXVjV-bXqoLM4wRBsukM9VxjTnRRjpcneZx4ieTM0MzJLta_oEuKDZeT4IkC10s05o6LaClECsuLo7gGIYlc0IGWWvutLVxLnyaif6TE-VT0Q1o1T_4j-bPA_p7jIGG2FJBKa5RdaFmVszNJVB2-bTV4wDH2ZmhPt1djAhXtPQns8QEjw5ZgjQu0D_bXaccUlsUI1VEdF9m8PASYqWSkRq224EwS-xFbkmQLYze1sWVhab6AdLFG2zV00ZA=w1033-h432-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/gTnmMmc_sAWel1lZQ5LJXxeSPE999WuMpyFt-7qWdw1yhWanjMGYy7QH4cR5On29K5-kj5LXjy4voOGRZQCqLtJFMZZVYmQCpygft5XcFDWCyNH-UXvMdW-u7MIMsEAVkI3HFJINmWbiP6-JG-ws0glkf0e8icBiWQxqOcqbpwJxiMx8AnQBKxHEbKHCy2FuqBb7qjJ66PHvnpt-k9gq02d024sIzKF3Bqrk_2MQD7l4k7J_mQXZb0AmJhufnGLuA3PZNbfg_kX56-gGZnvz5FAspJZgqxGUMa7UBtgNxLw9-3Aj727oGqt9FAg7eubLY9QRjy-2AS2rwPI8vsBMY7G_nhuXeRfpAdKkoKUNwmDZhc7JiMbJxTQid223Xz6MpTWD01CePfM8asMm5tfke-mTzH8EzdSCT3A3liIJnRAsQi1yyt7dXVjV-bXqoLM4wRBsukM9VxjTnRRjpcneZx4ieTM0MzJLta_oEuKDZeT4IkC10s05o6LaClECsuLo7gGIYlc0IGWWvutLVxLnyaif6TE-VT0Q1o1T_4j-bPA_p7jIGG2FJBKa5RdaFmVszNJVB2-bTV4wDH2ZmhPt1djAhXtPQns8QEjw5ZgjQu0D_bXaccUlsUI1VEdF9m8PASYqWSkRq224EwS-xFbkmQLYze1sWVhab6AdLFG2zV00ZA=w1033-h432-no&quot; border=&quot;0&quot; alt=&quot; photo inifModel_zpszslpm6g2.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Factor Graph With Pseudorange and Carrier-Phase Observables   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;data-clustering&quot;&gt;Data Clustering&lt;/h4&gt;

&lt;p&gt;The second example is a form of soft data clustering. In this example, we have a dataset with an unknown number of clusters. We can utilize E.M. to perform soft-clustering of each data point.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/kjMOEB0ZAhcftehuFgN6f5vCBdniVXqHbKK4dmscEV28MG91e6UIo8O1NwrxCZ-NSvAr_YCkGfv09FohZLp0nR5GWF6qf3x5qeVB1aUSTsvEI0c_8y3RRetHlQizBrY8_CSexaXj5dyWheItEqp9O_0coqtG2BN0RMRdrCCygIuAAyA-jhFVP4tomFf9TI_LeKVjDckXcs27figec28IJfDAJMmlvJyvRDeeW8zSEfvW92j_ED-YL_ZqOE59gNhrhIpIkJusBUGGdxhfGIB8DOXYYsJaNSbFUwEVjomDWbc5Griat2BClJVLCRU6UCWWUjTmUqyuEfxNgMlBD6bezE2E7QoRpX2NLqxuP95YcYAu-CCReGIAJ_DQj4UAqK3C14v85AUvTcYbh9tzcGsLzSc0rlKzrtROEY0nQ9T5dka3RFMFgEaBFz4YN5_-8M-vNH1EQF_0mvs8IuZmyG6rcBvKvrMSzUFk_DXTyae6JnzVBCcl0HLAag4jEqtMhkUNR18NWanROfsTMBrnolELnViF4oRE1a5WR13ENURHPl3fE6WsJrxuakAPHo4Ix5beEMNQUj4lH37Esit0G1YAbGI-IEwlex4IwVBux93AArU1gKJlW2k3KpKGGF5ERb-AiwyebpwJORTT_rqeDuN-e6F3fVmX-FSgtfhkfyMJ7YMoUA=w1024-h539-no
&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/kjMOEB0ZAhcftehuFgN6f5vCBdniVXqHbKK4dmscEV28MG91e6UIo8O1NwrxCZ-NSvAr_YCkGfv09FohZLp0nR5GWF6qf3x5qeVB1aUSTsvEI0c_8y3RRetHlQizBrY8_CSexaXj5dyWheItEqp9O_0coqtG2BN0RMRdrCCygIuAAyA-jhFVP4tomFf9TI_LeKVjDckXcs27figec28IJfDAJMmlvJyvRDeeW8zSEfvW92j_ED-YL_ZqOE59gNhrhIpIkJusBUGGdxhfGIB8DOXYYsJaNSbFUwEVjomDWbc5Griat2BClJVLCRU6UCWWUjTmUqyuEfxNgMlBD6bezE2E7QoRpX2NLqxuP95YcYAu-CCReGIAJ_DQj4UAqK3C14v85AUvTcYbh9tzcGsLzSc0rlKzrtROEY0nQ9T5dka3RFMFgEaBFz4YN5_-8M-vNH1EQF_0mvs8IuZmyG6rcBvKvrMSzUFk_DXTyae6JnzVBCcl0HLAag4jEqtMhkUNR18NWanROfsTMBrnolELnViF4oRE1a5WR13ENURHPl3fE6WsJrxuakAPHo4Ix5beEMNQUj4lH37Esit0G1YAbGI-IEwlex4IwVBux93AArU1gKJlW2k3KpKGGF5ERb-AiwyebpwJORTT_rqeDuN-e6F3fVmX-FSgtfhkfyMJ7YMoUA=w1024-h539-no
&quot; border=&quot;0&quot; alt=&quot; photo inifModel_zpszslpm6g2.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 3 :: Soft Clustering Using E.M.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;brief-intuitive-explanation&quot;&gt;Brief Intuitive Explanation&lt;/h3&gt;

&lt;p&gt;Expectation Maximization is an iterative optimization method. This method is incredible similar to that of M.L.E., in that its aim is to estimate the unknown parameters $\theta$, given the set of measurements, Y; however, now we have missing information N. So, with E.M., we want to maximize the posterior probability of the parameters $\theta$ given the data Y, marginalizing over N:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta^* = \text{argmax} \sum_N P( \Theta, N | Y)&lt;/script&gt;

&lt;p&gt;In the next section, a slightly more informative discussion on E.M. is provided. Specifically, we will discuss expectation maximization is in the context of a &lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/emk.pdf&quot;&gt;lower bound maximization&lt;/a&gt;. That is, we can think of the expectation step as calculating a lower bound to the posterior distribution. Then, the maximization step is optimizing our bound, which improves the estimate for the unknowns.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/q2SzOD_7uDGsctzfXjdpbFRREfEH2oi3nYGmqX0vjwmssQw-1ExI7c8qSo4XR5FhnW5JhwzyNW6EGLFbLSzU4_yUHcYkwg9aR8BzCi0EG9dFD_USO4bVW0qbfvDIP8o9quuAxEPtYgVDozei523DMImle2hVdj5S9Gabkj5uIGiuyeDZgxPBZsTAjDJDZxzQK87G6Uh9eZKsa5hSHFVYrXrabdSaaPvNHy5EHoE6L2oLjdA0_QwVMm_w0VUrdLTzVuQveT_rJhRFwG4f1S_klChbDmOu7k8JVVNVHLPf1igVeihnC5w7OcfKSJSlbf3jOwLvb0HV73WvcB_ngP_YRPURdx-rBgVpUsyso5rIaO7wzpTGND9ypToffzoOup9fW9RKIIPX6TV88OiERQq8QnkpXsAjpCdWiC-myF19EGMB1ESVsMTsRtai0klN48FdaMOSrYs-ti20XN44p5Z7QF1GEUbCE-ECLcYqzpjMRKNy5YouuhJdFctINdSUi4cZvmzsjW0uEXtiuqc0JZus92rSaRlKXMYph0GkAKEt5UGuZNjO8mf9tfF1RcJCD1sF-XBd2rJq2G__FdrXQ5Z8L2IFETnoM4eMoi3vdkwTlIMvgBQA0aV9WEu592b4QqI6V1lvl9ehMoLpRul9M9sNn1X31clwKOEHBxzrtdGQpn72sA=w1096-h616-no
&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/q2SzOD_7uDGsctzfXjdpbFRREfEH2oi3nYGmqX0vjwmssQw-1ExI7c8qSo4XR5FhnW5JhwzyNW6EGLFbLSzU4_yUHcYkwg9aR8BzCi0EG9dFD_USO4bVW0qbfvDIP8o9quuAxEPtYgVDozei523DMImle2hVdj5S9Gabkj5uIGiuyeDZgxPBZsTAjDJDZxzQK87G6Uh9eZKsa5hSHFVYrXrabdSaaPvNHy5EHoE6L2oLjdA0_QwVMm_w0VUrdLTzVuQveT_rJhRFwG4f1S_klChbDmOu7k8JVVNVHLPf1igVeihnC5w7OcfKSJSlbf3jOwLvb0HV73WvcB_ngP_YRPURdx-rBgVpUsyso5rIaO7wzpTGND9ypToffzoOup9fW9RKIIPX6TV88OiERQq8QnkpXsAjpCdWiC-myF19EGMB1ESVsMTsRtai0klN48FdaMOSrYs-ti20XN44p5Z7QF1GEUbCE-ECLcYqzpjMRKNy5YouuhJdFctINdSUi4cZvmzsjW0uEXtiuqc0JZus92rSaRlKXMYph0GkAKEt5UGuZNjO8mf9tfF1RcJCD1sF-XBd2rJq2G__FdrXQ5Z8L2IFETnoM4eMoi3vdkwTlIMvgBQA0aV9WEu592b4QqI6V1lvl9ehMoLpRul9M9sNn1X31clwKOEHBxzrtdGQpn72sA=w1096-h616-no
&quot; border=&quot;0&quot; alt=&quot; photo inifModel_zpszslpm6g2.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 4 :: EM as Lower Bound
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;em-as-lower-bound-optimization&quot;&gt;E.M. as Lower Bound Optimization&lt;/h3&gt;

&lt;p&gt;Again, the premise of EM is to start with a guess $\Theta^i$ for the parameters $\Theta$, compute an easily computed lower bound $B(\Theta | \Theta^t)$ to the function $ \text{log} \ P( \theta | Y) $, and maximize that bound instead. If iterated, this procedure will converge to a MLE $\Theta^*$ of the objective function.&lt;/p&gt;

&lt;p&gt;So, our goal is to maximize the likelihood of our parameter vector $\theta$ given the data, Y, and the unknown parameters, N.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta^* = \text{argmax} \ log \ P(Y | \Theta) = \text{argmax} \ log \ \sum_N P(Y,N,\Theta)&lt;/script&gt;

&lt;p&gt;Currently, this equation not easily solvable because it includes the logarithm of a summation. So, let’s manipulate it slightly to see if we can find a tractable bound.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log \ P(Y | \Theta) = \ log \ \sum_N P(Y,N,\Theta) = \text{log} \sum_N f^t(N) \frac{P(Y,N,\Theta)}{f^t(N)}&lt;/script&gt;

&lt;p&gt;Where, for now, we can just say that f^t(N) is an arbitrary probability distribution of the space of N.&lt;/p&gt;

&lt;p&gt;Now, by applying Jensen’s inequality, we have&lt;/p&gt;

&lt;blockquote&gt;&lt;div style=&quot;background-color:#FFFF00; color:#000000; font-style: normal; font-family: Georgia;&quot;&gt;
Jensen's Inequality for Log :: &lt;br /&gt;&lt;br /&gt;

If 
$$ \sum_i \lambda_i = 1 $$

 Then, 

$$ ln \sum_i \lambda_i Q_i \geq \sum_i \lambda_i ln Q_i $$

&lt;/div&gt;&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B(\Theta, \Theta^t) := \sum_N f^t(N) \ \text{log} \ \frac{P(Y,N,\Theta)}{f^t(N)} \leq \ \text{log} \sum_N f^t(N) \frac{P(Y,N,\Theta)}{f^t(N)}&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;how-do-we-find-an-optimal-bound&quot;&gt;How do we find an optimal bound?&lt;/h4&gt;

&lt;p&gt;The E.M. algorithm not only finds a lower bound, it finds the optimal one ( i.e., it finds the lower bound that touches the likelihood curve at the current estimate for $\Theta^t$. )&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B(\Theta^t, \Theta^t) = \sum_N f^t(N) \ \text{log} \ \frac{P(Y,N,\Theta)}{f^t(N)}&lt;/script&gt;

&lt;p&gt;Now, this becomes a constrained optimization problem because $\sum_N f^t(N) = 1.$ So, to solve for the optimal bound, Lagrange Multipliers must be introduced.&lt;/p&gt;

&lt;p&gt;That is, we must find a new cost function that takes the form of&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min J'(x_1,x_2,\lambda) = \phi(x_1,x_2) + \lambda \psi(x_1,x_2).&lt;/script&gt;

&lt;p&gt;So, let’s define $\phi,$ and $\psi$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi = \sum_N f^t(N) \ \text{log} \ \frac{P(Y,N,\Theta)}{f^t(N)},&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi = 1 - \sum_N f^t(N).&lt;/script&gt;

&lt;p&gt;Now, we can define our new cost function as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J'(f^t) = \sum_N f^t(N) \ \text{log} \ \frac{P(Y,N,\Theta)}{f^t(N)} + \lambda[1-\sum_N f^t(J)] .&lt;/script&gt;

&lt;p&gt;Take the derivative,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{ \partial J'}{ \partial f^t(N)} =  \text{log} \ P(Y,N,\Theta^t) - \text{log} f^t(N) - 1 - \lambda,&lt;/script&gt;

&lt;p&gt;and solve for $f^t(N)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^t(N) = \frac{P(Y,N,\Theta^t)}{\sum_N P(Y,N,\Theta^t)} = P(N | Y,\Theta^t).&lt;/script&gt;

&lt;p&gt;Now, plugging this expression back into the first equation in this section, we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B(\Theta^T , \Theta^t) = \sum_N P(N | Y, \Theta^t) \ \text{log} \ \frac{P(Y,N,\Theta^t)}{P(N | Y,\Theta^t)} = \text{log} P(Y,\Theta^t)&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;how-do-we-optimize-this-bound&quot;&gt;How do we optimize this bound&lt;/h4&gt;

&lt;p&gt;First, let’s start with the bound,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B(\Theta, \Theta^t) = \sum_N P(N | Y,\Theta^t) \ \text{log} \frac{P(Y,N,\Theta)}{P(N|Y,\Theta^T)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \sum_N P(N|Y,\Theta^t) \ \text{log} P(Y,N,\Theta) -\sum_N P(N|Y,\Theta^t) \ \text{log} \ P(N|Y,\Theta^t)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \sum_N P(N|Y,\Theta^t) \ \text{log} P(Y,N,\Theta) - \mathcal{H}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \sum_N P(N|Y,\Theta^t) \ \text{log} P(Y,N | \Theta) + \text{log} P(\Theta) - \mathcal{H}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= Q^t(\Theta) + \text{log} P(\Theta) + \mathcal{H}&lt;/script&gt;

&lt;p&gt;So, after molding the equation, we have the&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta^{t+1} = \text{argmax} [ Q^t(\Theta) + \text{log} P(\Theta) ].&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;final-algorithm&quot;&gt;Final Algorithm&lt;/h4&gt;

&lt;p&gt;Every iteration of the EM algorithm starts by finding a lower bound $B(\Theta,\Theta^t)$ at the current guess $\Theta^t$. Then, the lower bound is maximized to improved the estimate $\Theta^{t+1}$. Thus, the EM algorithm can be written compactly as,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Expectation –&amp;gt; $f^t (N) = P( N | Y, \Theta^t) $&lt;/li&gt;
  &lt;li&gt;Maximization –&amp;gt; $ \Theta^{t+1} = \text{argmax} [Q^{t}(\Theta) + \text{log} P(\Theta)] $&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, it is important to know that $Q^t(\Theta)$ is calculated in the expectation by using the current estimate of $\Theta$. However, the M-Step is optimizing $Q^{t}$ w.r.t. the free paramater $\Theta$ to generate a new estimate, $\Theta^{t+1}$.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;additional-links-1&quot;&gt;Additional Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jstor.org/stable/2527783?seq=1#page_scan_tab_contents&quot;&gt;The First Paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.mit.edu/6.435/www/Dempster77.pdf&quot;&gt;Seminal Paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/emk.pdf&quot;&gt;Lower Bound Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;model-selection--complexity-selection&quot;&gt;Model Selection –&amp;gt; Complexity Selection&lt;/h1&gt;

&lt;p&gt;The main idea of this section was summed up by George Box when he said “essentially, all models are wrong, but some are useful.” I’m not going to go into much detail on this topic because we have already discussed most of the important topics in the Bias-Variance discussion. However, I will provide an example of model selection that I conducted a few months ago.&lt;/p&gt;

&lt;h3 id=&quot;model-selection-example&quot;&gt;Model Selection Example&lt;/h3&gt;

&lt;p&gt;Recently, there has been quite a bit of a discussion around GNSS spoofing. The primary concern is that the signal structure is public knowledge so anyone with the technical background could replicate the signal and broadcast is to a user. An example of a spoofed receiver is shown in the figure below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/asxg5rvUFmB7i9RGtNVNePN1HEaWIGhG6jLIwQaKykC6_q2JFqt5RgKNIkl3WSE1dUiGqxbCF3p7lsYyEl1xo9SmvgTwAdRIF6FZlctkv3WWZ4i8zijv3JxmmgZkq_2Ynqbxs7NeP-oB1KyplfcInU-ZufwUvEW_BK19twcRFqVWyjeWHD28y5gobfKBnkYBPRSOriPK0EQ62ZNFsa-gHas6DxseZY94pcvbIIdtnfWaTUdyZpW35St_cpeb2PtadcFNlrAvYkVEkEWkVR2WgLfKHoSbj3Yr84kTi-Gdm73UhfzoOSYk42SXY38FvG2KuYzUbR2D0qst1DOmWyjBX9EuyLOSKQTrs8o7wKRPAqom1zM9XKppJDfU-MUf22SX9WVXSfWO3Poz0j5H9eSNY2BCbW1BVd_ZdLPofBekRatkI5gMGFstJQVW6vYDyIX4cSYr8lR8phAXtFzpi2Luxi-594Y1WFhU1_jr9ag1P9byVlYHd1W2h6JooR6wGfkpYHvOuxxKUgnmwuXcGAGy3j7yQM2ZkH5Qkc-mqD7b9mio1VWKWQ_XBTzYWoTIRZQhO2mxU6QB-eZP1pCjly2aB0u6dqZDa6sBvr5G7F2lN394wZyTulsvgAO0c3uGv_xdwMD9VO7jVZ8mop9XLlCR2PR-gMll_t8dQn9lbKlYhQDEXw=w1205-h665-no
&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/asxg5rvUFmB7i9RGtNVNePN1HEaWIGhG6jLIwQaKykC6_q2JFqt5RgKNIkl3WSE1dUiGqxbCF3p7lsYyEl1xo9SmvgTwAdRIF6FZlctkv3WWZ4i8zijv3JxmmgZkq_2Ynqbxs7NeP-oB1KyplfcInU-ZufwUvEW_BK19twcRFqVWyjeWHD28y5gobfKBnkYBPRSOriPK0EQ62ZNFsa-gHas6DxseZY94pcvbIIdtnfWaTUdyZpW35St_cpeb2PtadcFNlrAvYkVEkEWkVR2WgLfKHoSbj3Yr84kTi-Gdm73UhfzoOSYk42SXY38FvG2KuYzUbR2D0qst1DOmWyjBX9EuyLOSKQTrs8o7wKRPAqom1zM9XKppJDfU-MUf22SX9WVXSfWO3Poz0j5H9eSNY2BCbW1BVd_ZdLPofBekRatkI5gMGFstJQVW6vYDyIX4cSYr8lR8phAXtFzpi2Luxi-594Y1WFhU1_jr9ag1P9byVlYHd1W2h6JooR6wGfkpYHvOuxxKUgnmwuXcGAGy3j7yQM2ZkH5Qkc-mqD7b9mio1VWKWQ_XBTzYWoTIRZQhO2mxU6QB-eZP1pCjly2aB0u6dqZDa6sBvr5G7F2lN394wZyTulsvgAO0c3uGv_xdwMD9VO7jVZ8mop9XLlCR2PR-gMll_t8dQn9lbKlYhQDEXw=w1205-h665-no
&quot; border=&quot;0&quot; alt=&quot; photo inifModel_zpszslpm6g2.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 5 :: Spoofing Example
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I wanted to see how hard it is to detect spoofing. To conduct this study, I was provided a very large dataset from UT Austin ( $\sim$ 2,000,000 labeled data points ). And I trained a few learning algorithms; however, it seemed like no matter how complex the algorithm I was stuck at a threshold of 99.6% classification accuracy. For example, below is a fairly complex binary classification tree.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/7J4Fkd2GtCwEDDf6AhPph0pVuic-gl72BCrBwpHCFHbd9WS7TPXnaLKLa7a9a8C3zxSDGCBxJntfgFx2LKDlQhGmgTq2r0fZorRSggEU_2IhpdLXOm8LLQYOlzFxeghQkSa46L6QpD_o8lSe_xAYj61DyHy0BHTFNHE5b8ViB5DaHdj_J6qMjGwWn_TgIusU2AKsw8or9jYRN-S-ckl7fwcCpMgjrjnADB88HwXzobWWEzGn7niZBjVlaxP0m4NYJNEk8oEXTS78PlU55UwYcz8FaYql7gfJ4bzGNby9U4Pw1DwVObr4Bmva7uurQqoTifnC1CavZjDfnDDpnSr-QlD-10LmYrEg1jg0wdrtIkY16-1Li2fFUj9K0QUIx_qUD66hvy_M3OW8hqEqeAp7xigAqanNRLlmCkcquCCelGOWUcsOvAIrI9gt_wdO6wcNNttb_MgeNs1JKoi7XGyjA3O1TNkEUjK55exrpCnA12q5qlySdUtaXYm7Y1qAuFDeH7tuCLofM3Tu0gFTV3QBkDzXBpIUk85zbEtlr3iESG7SnhEHPDy5vLAFqIG51TRSlgPE_mVyiF5pk_REZLX-QaoVhUEYccUVoZ0LsR0HqFuxIAV87gQZwrhsy2I2nk_43gi8XO2opJ25wB1podxBrpd2gmjKogkTi1ThX9d1DJSCNw=w1280-h626-no
&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/7J4Fkd2GtCwEDDf6AhPph0pVuic-gl72BCrBwpHCFHbd9WS7TPXnaLKLa7a9a8C3zxSDGCBxJntfgFx2LKDlQhGmgTq2r0fZorRSggEU_2IhpdLXOm8LLQYOlzFxeghQkSa46L6QpD_o8lSe_xAYj61DyHy0BHTFNHE5b8ViB5DaHdj_J6qMjGwWn_TgIusU2AKsw8or9jYRN-S-ckl7fwcCpMgjrjnADB88HwXzobWWEzGn7niZBjVlaxP0m4NYJNEk8oEXTS78PlU55UwYcz8FaYql7gfJ4bzGNby9U4Pw1DwVObr4Bmva7uurQqoTifnC1CavZjDfnDDpnSr-QlD-10LmYrEg1jg0wdrtIkY16-1Li2fFUj9K0QUIx_qUD66hvy_M3OW8hqEqeAp7xigAqanNRLlmCkcquCCelGOWUcsOvAIrI9gt_wdO6wcNNttb_MgeNs1JKoi7XGyjA3O1TNkEUjK55exrpCnA12q5qlySdUtaXYm7Y1qAuFDeH7tuCLofM3Tu0gFTV3QBkDzXBpIUk85zbEtlr3iESG7SnhEHPDy5vLAFqIG51TRSlgPE_mVyiF5pk_REZLX-QaoVhUEYccUVoZ0LsR0HqFuxIAV87gQZwrhsy2I2nk_43gi8XO2opJ25wB1podxBrpd2gmjKogkTi1ThX9d1DJSCNw=w1280-h626-no
&quot; border=&quot;0&quot; alt=&quot; photo inifModel_zpszslpm6g2.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 6 :: Decision Tree
&lt;/p&gt;

&lt;p&gt;So, I turned my attention to trying to minimize the complexity of the system while mainting the integrity of the classifier. To do this, I used a simple ensemble of trees, where the key concept is that many weak learners can be aggregated into one
high-fidelity estimator.&lt;/p&gt;

&lt;p&gt;To reduce the complexity, we first looked at the number of trees required in our ensemble. To measure the accuracy of our tree ensemble, we looked at the out of bag (oob) error.  That is, each tree is trained on a subset of data ( i.e., a bag ) and tested on the remaining data. The oob error as a function of the number of trees in an ensemble is shown below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/C67uCmkeKOSBIN48GHpnrKn3bIn5GoC9AvohV1Nw_4hImY3R3veqks2vU3t_JRsHn1dY9I2DT5h9v6-VsJJORh9Ti7Z1s5mnTNAhZSegbiK2fnroaHUY-wOdjL0PLizQAnnlfuD2D8AL01o8qvWxr3I1DT58dycQUbgMv99saWfMvkvSlw-iBl7J22z_wHORxUfs8AKN0M5417kW-vjzM9xjs70XQG51IfrH65VjK__VWhC4aUsf1r9t7DeYhe52BN1goeBTkx-DV2js0H48LNdP0MYk2VXsFggaodqF99rlViYrXjMbBm3QnmelINao0Vo1Ib21viUJaHjbBro9zDWJSIP1mz3XY3pAiNxBJNBTznZoOSIXdvtNbHICjng7oueFriOzn_nv2R_XLDTDCsKS4HDmNt0tMgbyVs86lZH_xcS5GsiFm-i_NXDAVoMDaGigakOFEnedUqtSsfWhlBMT_NSoFG2Y-iCWO13RS8mnR9k4KTG-1ppuiFUZ7Wcnf-lL73YYD-rfa0JsTQSi8fbnEJCDfA4BGAokqXIVXyeTkFDZcx5Piid47xSZtq-TrccmgelChgAyOzDUkIpkOsRWwoKeWcA0p_MLHsEHF9qaQQWlkVqWsMXyKVd-IVSXbGECaF3xpFTGmA5OcSzg5cqP8YfaZecFq-LmyRRiq88B1Q=w1280-h597-no
&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/C67uCmkeKOSBIN48GHpnrKn3bIn5GoC9AvohV1Nw_4hImY3R3veqks2vU3t_JRsHn1dY9I2DT5h9v6-VsJJORh9Ti7Z1s5mnTNAhZSegbiK2fnroaHUY-wOdjL0PLizQAnnlfuD2D8AL01o8qvWxr3I1DT58dycQUbgMv99saWfMvkvSlw-iBl7J22z_wHORxUfs8AKN0M5417kW-vjzM9xjs70XQG51IfrH65VjK__VWhC4aUsf1r9t7DeYhe52BN1goeBTkx-DV2js0H48LNdP0MYk2VXsFggaodqF99rlViYrXjMbBm3QnmelINao0Vo1Ib21viUJaHjbBro9zDWJSIP1mz3XY3pAiNxBJNBTznZoOSIXdvtNbHICjng7oueFriOzn_nv2R_XLDTDCsKS4HDmNt0tMgbyVs86lZH_xcS5GsiFm-i_NXDAVoMDaGigakOFEnedUqtSsfWhlBMT_NSoFG2Y-iCWO13RS8mnR9k4KTG-1ppuiFUZ7Wcnf-lL73YYD-rfa0JsTQSi8fbnEJCDfA4BGAokqXIVXyeTkFDZcx5Piid47xSZtq-TrccmgelChgAyOzDUkIpkOsRWwoKeWcA0p_MLHsEHF9qaQQWlkVqWsMXyKVd-IVSXbGECaF3xpFTGmA5OcSzg5cqP8YfaZecFq-LmyRRiq88B1Q=w1280-h597-no
&quot; border=&quot;0&quot; alt=&quot; photo inifModel_zpszslpm6g2.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 7 :: Number of Required Trees
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Next, we looked again at the obb error but this the we wanted to minimize the splits in a tree. For this, it can be seen in the image below that the error hits a minimum at roughly 25 splits.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/RuGgy4xTlEBkVWP0d8Wjz3rh4P4rn99upH0w_pw8_xKF0-IfvXh4uM5YuOu1_3uYkXT_Grkhmp5CnduhngmPG_RCGg-qhikMUny5Zk9AQbTR7F2Iy_wfWunfl5lmLHHjnghqwMc89GmDnjelfFjFIgbbfreH11WRjy3SxtfSDpKncuXscoS2mLgqaYWfNxvVfSHI72CjXJmer-KKncVYSlnbgVIfukGFP18Loos5FlmV3r6ariHnQ3swRg9Ev6oZFpK3N-YYX_7MqJtW8fmrfQdRS2iuRsq3LbdsePSzYt73Z1fyEbiui_69IHltNRUclk_boKuogfQmXMEKf3qUbdN8wcgGhGSU4EkUWjDH3NynkFPJV_d5BdNzMDXtGU3W19PdMRgo6pXWobjNCn32y2nfrHJomAtVHyNsTbvA_HBeXe2eCwGWeQMsAC7cllr6ovHsLS8MN0UjEadDbZhBGxIWfx1cCW1bLVmriU3VmJXyUU84UdpKuzxWjiMtDQH0FR2MT7aclKtwPuYH8T9VVbXkV5WICF-rFzyWSVeERwlBTnhbvZKvu3VGm17fm3-SHpEMuLeLO68ljlqyxWXB6PyuN9uy3nMxdZvU65HkXMU8s4BOiYrj-ZEXE3gbkI2F5cGTih2cs8Sv_NQu8l61hv1lPt7wi9s7s6cj25F1apzojw=w1280-h597-no
&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/RuGgy4xTlEBkVWP0d8Wjz3rh4P4rn99upH0w_pw8_xKF0-IfvXh4uM5YuOu1_3uYkXT_Grkhmp5CnduhngmPG_RCGg-qhikMUny5Zk9AQbTR7F2Iy_wfWunfl5lmLHHjnghqwMc89GmDnjelfFjFIgbbfreH11WRjy3SxtfSDpKncuXscoS2mLgqaYWfNxvVfSHI72CjXJmer-KKncVYSlnbgVIfukGFP18Loos5FlmV3r6ariHnQ3swRg9Ev6oZFpK3N-YYX_7MqJtW8fmrfQdRS2iuRsq3LbdsePSzYt73Z1fyEbiui_69IHltNRUclk_boKuogfQmXMEKf3qUbdN8wcgGhGSU4EkUWjDH3NynkFPJV_d5BdNzMDXtGU3W19PdMRgo6pXWobjNCn32y2nfrHJomAtVHyNsTbvA_HBeXe2eCwGWeQMsAC7cllr6ovHsLS8MN0UjEadDbZhBGxIWfx1cCW1bLVmriU3VmJXyUU84UdpKuzxWjiMtDQH0FR2MT7aclKtwPuYH8T9VVbXkV5WICF-rFzyWSVeERwlBTnhbvZKvu3VGm17fm3-SHpEMuLeLO68ljlqyxWXB6PyuN9uy3nMxdZvU65HkXMU8s4BOiYrj-ZEXE3gbkI2F5cGTih2cs8Sv_NQu8l61hv1lPt7wi9s7s6cj25F1apzojw=w1280-h597-no
&quot; border=&quot;0&quot; alt=&quot; photo inifModel_zpszslpm6g2.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 8 :: Miniminum Number of Splits 
&lt;/p&gt;

&lt;p&gt;So, a new learner was trained with 20 trees in the ensemble and a maximum of 25 splits in a single tree. The classification results for our simple ensemble are shown in Figure 9. As can be seen in the figure, the classification integrity of the learner was maintain as the complexity was decreased.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/cPWQdP3ygy-aTUiw2FKWLFzIdlxXh6aP9wRFoIZm5hWYTeLi-EFuqd0HIVjve0WwcxypxoF0ARiUepF0K_Jn5zvSG04z6yt47K5V80A36HP7sZFBmWmg8M3TrbLquK_JvWP5n9Rg6927bI7TQD74J4vh5Kh6yFD8hE8LxFYM2HFo0BJJnPjPNateMM86nrtpx9zrLODY0FxfJ1WkFuFdi_y17TB8SsXiovJO6WwflSA7J8ZtX6zJUCCHBMvhUfh-d4w1zNW9dkZVN2HFblzCDhQNbR2NCLyUKW2_NUDQYR39IbVYZLAE-QxNns87jUy6Iu91xh1l3s6Pva_GlSOSIvC7fF7Fs1F3zj8CLNnx2KaaebKOz6RsN9y8JYHwVQAuCG_N13ya_oXTDrdNWMaa_OKEPyjXOXw3I8uo_z4DkP_IJoQ3Qm1dk7erkLYdXr4cCdNuThpDi8mcvYhbbk6pprSwY8lU_82qD6Kit3-qw7B71Udqec1rTcJStfYaIdMn7xCJyGwlbNzHZ1OlMPcFkF_GT4LTgJs960U52u3JL5x9PEeRbA2n8ly2-c-fgBFAKUUjRMkP31Yc8BCD_EDFaNG162ZlW2p7ySvsPKfC8GvyvEmIUI0HEv39qgqcwDLFFEa9Viigtb21Rbj7gU6ex0wqZUHwd6UAVGWTVf8SucVvOQ=w1280-h597-no
&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/cPWQdP3ygy-aTUiw2FKWLFzIdlxXh6aP9wRFoIZm5hWYTeLi-EFuqd0HIVjve0WwcxypxoF0ARiUepF0K_Jn5zvSG04z6yt47K5V80A36HP7sZFBmWmg8M3TrbLquK_JvWP5n9Rg6927bI7TQD74J4vh5Kh6yFD8hE8LxFYM2HFo0BJJnPjPNateMM86nrtpx9zrLODY0FxfJ1WkFuFdi_y17TB8SsXiovJO6WwflSA7J8ZtX6zJUCCHBMvhUfh-d4w1zNW9dkZVN2HFblzCDhQNbR2NCLyUKW2_NUDQYR39IbVYZLAE-QxNns87jUy6Iu91xh1l3s6Pva_GlSOSIvC7fF7Fs1F3zj8CLNnx2KaaebKOz6RsN9y8JYHwVQAuCG_N13ya_oXTDrdNWMaa_OKEPyjXOXw3I8uo_z4DkP_IJoQ3Qm1dk7erkLYdXr4cCdNuThpDi8mcvYhbbk6pprSwY8lU_82qD6Kit3-qw7B71Udqec1rTcJStfYaIdMn7xCJyGwlbNzHZ1OlMPcFkF_GT4LTgJs960U52u3JL5x9PEeRbA2n8ly2-c-fgBFAKUUjRMkP31Yc8BCD_EDFaNG162ZlW2p7ySvsPKfC8GvyvEmIUI0HEv39qgqcwDLFFEa9Viigtb21Rbj7gU6ex0wqZUHwd6UAVGWTVf8SucVvOQ=w1280-h597-no
&quot; border=&quot;0&quot; alt=&quot; photo inifModel_zpszslpm6g2.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 9 :: Confusion Matrix for Ensemble 
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;additional-links-2&quot;&gt;Additional Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=gajUDdlBX3w&quot;&gt;Video Overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.cmu.edu/afs/cs/academic/class/10601-f10/lecture/lec16.pdf&quot;&gt;Good Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 25 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/tutorials/2017/07/25/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/tutorials/2017/07/25/Update.html</guid>
        
        <category>tutorials</category>
        
        <category>MLE</category>
        
        <category>Expectation Maximization</category>
        
        <category>Model Selection</category>
        
        
        <category>_projects</category>
        
        <category>tutorials</category>
        
      </item>
    
      <item>
        <title>Continued Testing of EM on Faulty Pose Graphs</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now, that the E.M. estimated covariance looks to be providing a reasonable estimate, we can begin testing it’s ability to robustly optimize and classify outliers. To visually represent the classification accuracy of the estimator, the confusion matrix will be utilized. A representation of the confusion matrix is provided in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/mYwoeM4tJwahrS_zZaVT56rYdJB0WSOJyMveaQDklBrJMTm8wxAsqVXD43BndSf61IGH5rTVWvBks1V5Xe5arxw9RVexzaA1sOCVlNpeCEBuZSvtp5v0A6X1naL-B1dW99SpZk_kC1NlxFhdGNgE2p-D7FxJDTdRiKXUS1Hq0DewuUIJRfOM_nWwiDF4jbkwaV2JtLX-ukaRkXoXLfhU0gE1o26F_hNcyYudq_MhFFHmU-Uw0QQyaX_2kFpBDF7jR_SVNLaNCXtQ3NHVYgph5gbGON_wnxqCx4KrJrEp7rNtBQdgjScaVlmpA7I0cA3aR6VO7FS1dcAkUNBSW0MPCC3UG1PZnIhNyqEf8YFLmdM3YIi51t35PTYKpDMF4amhevUGgGtg0OJOVhWuJ5BFHaz3HkbWL9XwS8D9ds8cwR6-FCqH2oqhTbCfP_Ods-ayiICOalRK3L4kB9Jw9C6HsCWfhicjj_HFSGmVzATQX92cMjtChU4AdRkKv9mEp7du_HniMTIBioxCq17bPAwIgr9I6hRxFMUjpywamLxxHi91gw3BzbPNdHS3nGv16bQ5elW26cZA4Ef9b_A_i3jlHCrypsUxloETR_5bnY8uouU0Zo0JRiQnpsXNoLXrFBUibKSfSQItBM1SDQGmNx_usbVKMHXfLUiLFj8q4BdcZaDmKg=w332-h288-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/mYwoeM4tJwahrS_zZaVT56rYdJB0WSOJyMveaQDklBrJMTm8wxAsqVXD43BndSf61IGH5rTVWvBks1V5Xe5arxw9RVexzaA1sOCVlNpeCEBuZSvtp5v0A6X1naL-B1dW99SpZk_kC1NlxFhdGNgE2p-D7FxJDTdRiKXUS1Hq0DewuUIJRfOM_nWwiDF4jbkwaV2JtLX-ukaRkXoXLfhU0gE1o26F_hNcyYudq_MhFFHmU-Uw0QQyaX_2kFpBDF7jR_SVNLaNCXtQ3NHVYgph5gbGON_wnxqCx4KrJrEp7rNtBQdgjScaVlmpA7I0cA3aR6VO7FS1dcAkUNBSW0MPCC3UG1PZnIhNyqEf8YFLmdM3YIi51t35PTYKpDMF4amhevUGgGtg0OJOVhWuJ5BFHaz3HkbWL9XwS8D9ds8cwR6-FCqH2oqhTbCfP_Ods-ayiICOalRK3L4kB9Jw9C6HsCWfhicjj_HFSGmVzATQX92cMjtChU4AdRkKv9mEp7du_HniMTIBioxCq17bPAwIgr9I6hRxFMUjpywamLxxHi91gw3BzbPNdHS3nGv16bQ5elW26cZA4Ef9b_A_i3jlHCrypsUxloETR_5bnY8uouU0Zo0JRiQnpsXNoLXrFBUibKSfSQItBM1SDQGmNx_usbVKMHXfLUiLFj8q4BdcZaDmKg=w332-h288-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: Confusion Matrix Definition    
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;10-false-constraints&quot;&gt;10 False Constraints&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;First, we will look at a graph that only contains 10 false constraints. The optimization of the pose graph can be seen in the video provided.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/1DsC7VLQXpE&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; align=&quot;center&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Additionally, the confusion matrix for the estimator is provided in Figure 2. As can be seen from the figure, estimator performed very well, with only one false negative.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/zXuhMn81ObHPt58obUAHXxMEo4Sg7LtiVQhBsVprRVDOaAb_YbV8H5o0WYUHzpVdJBdRBFMY6nt074EKKmXAcJHLxZ9T7BWvvRfqqPm8oEQnCOGMAXQdLEJPvQCs2T159py5gLNy0HLpQJwklulsA-mhOG7YDNwUn7vmTRaZ5EClws4GQ2IQcBgPQK9S-4kNndRRpwevJHvv-uK2hAlQ89SFeZETUUpoBmBWQVd_7hcHciQBYwdiXosLIEutstkSu9vpxefjw2Lk1ZrFX3U0eGr3KCCr9WcEWXnzluEcmLRhfXwcteAH5PmpFqpOoPglqf4hgWIpxsb7_ueBIAhjcAd-jn7VhMaNAEZ4-NglB0VCB6-9NASpSEjUN51B_kjMiXphi1kcO3qRexOztbjAGO2l37yNauobMyhTPQDSFgSWspPW0d2sJ3BtMWrdgUiNPfoKofEwrfqiT0L3yrybB6mNPMGu0In9sClaLSFpcLAh6m-CkFQKLJvE4rDmhZITxaxjJPBUYgxgYy4aDGZhbKcDtbJ8z8AfNNOZzp0vovD8OITFYfndpp2LS4rPYkCI3yq5DRGGIrZWZ2qky_gnTzdueLBCVwNq1idGDeEFOKT8TvdQlVZW-RF0tg2f4UuBOJYBIoL5SdDTAE7Phk4pb8MH2r29Su0V4fBCGgnovBGnUA=w1661-h960-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/zXuhMn81ObHPt58obUAHXxMEo4Sg7LtiVQhBsVprRVDOaAb_YbV8H5o0WYUHzpVdJBdRBFMY6nt074EKKmXAcJHLxZ9T7BWvvRfqqPm8oEQnCOGMAXQdLEJPvQCs2T159py5gLNy0HLpQJwklulsA-mhOG7YDNwUn7vmTRaZ5EClws4GQ2IQcBgPQK9S-4kNndRRpwevJHvv-uK2hAlQ89SFeZETUUpoBmBWQVd_7hcHciQBYwdiXosLIEutstkSu9vpxefjw2Lk1ZrFX3U0eGr3KCCr9WcEWXnzluEcmLRhfXwcteAH5PmpFqpOoPglqf4hgWIpxsb7_ueBIAhjcAd-jn7VhMaNAEZ4-NglB0VCB6-9NASpSEjUN51B_kjMiXphi1kcO3qRexOztbjAGO2l37yNauobMyhTPQDSFgSWspPW0d2sJ3BtMWrdgUiNPfoKofEwrfqiT0L3yrybB6mNPMGu0In9sClaLSFpcLAh6m-CkFQKLJvE4rDmhZITxaxjJPBUYgxgYy4aDGZhbKcDtbJ8z8AfNNOZzp0vovD8OITFYfndpp2LS4rPYkCI3yq5DRGGIrZWZ2qky_gnTzdueLBCVwNq1idGDeEFOKT8TvdQlVZW-RF0tg2f4UuBOJYBIoL5SdDTAE7Phk4pb8MH2r29Su0V4fBCGgnovBGnUA=w1661-h960-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 2 :: Confusion Matrix When 10 Faults are Present    
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;50-false-constraints&quot;&gt;50 False Constraints&lt;/h3&gt;

&lt;p&gt;Next, we will move onto a graph that contains 50 false constraints. Again, a video of the optimization process is provided in the video below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/-BSMKzQqgPc&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; align=&quot;center&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Again, the estimator was provided a very good classification of the inlier and outlier distribution. For this example there were no false negatives and only one false positive; however, this is the more detrimental of the two false classification because this allows erroneous measurement to be weighted the same as in inlier.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/b6azSSaIMPv9w_FBPUuWu5m_BscY5MRqRaSqWSt5GtvCLbcjnTKJKlAsRF22drD6rCrwDXLoga3mvZUDRu_FB4yEyGATgZKVGBcoW94U2eRzpyoW6oP4UXkwfsmlpUEvMWZegwpNYASngRZuBmKADjIJEPVmHHlUXlIyMn6nei1zOwxcG_Z_CkYH-xlYwTKbbLgPJkdSG9XfcEFTHnJQMqiOOJE8Yx1vxuszkEDBsejZAJbBk-z-QoK7HqvZTAirqwysfe9AujdiVr6GvyNy9SdOPlTQmWBXW5rRLenFkrsXcK-6qrwmWeMwGOcg_MtEXvwu5kOIJpnafzfIip0ExOk5eXrjc6NP11ITxmaC4qFPC08DtvRxJ-rpMISYpU9liEse6fYazbi-4DJp3MdKFTq8jzUlodrv7Q4HC1BnlMCgERhn0XxnaR2Hg1hj4gO1clLCoIxj9uAUEOe0pU4YlNK7nikQqsJPc0J_WegsvYiFhkiKXX1LoA0l6_NScE7NrHYTAuCRAnn17lKBoUDRUWVq-pWaTOOLeLJxGDN2mBNS0VUMuwaQvgx2WH7S-VpZzyEdiglaYuerKHQpZPJ5W9_s40-fcknS2QNAqLnBGz31uQ5u4S4ZtNG_tNGZyZK8A5nV5TY58wCcT7O-B4rpt_Sig83ZPWDnBg9g44P9G1Za_Q=w1661-h960-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/b6azSSaIMPv9w_FBPUuWu5m_BscY5MRqRaSqWSt5GtvCLbcjnTKJKlAsRF22drD6rCrwDXLoga3mvZUDRu_FB4yEyGATgZKVGBcoW94U2eRzpyoW6oP4UXkwfsmlpUEvMWZegwpNYASngRZuBmKADjIJEPVmHHlUXlIyMn6nei1zOwxcG_Z_CkYH-xlYwTKbbLgPJkdSG9XfcEFTHnJQMqiOOJE8Yx1vxuszkEDBsejZAJbBk-z-QoK7HqvZTAirqwysfe9AujdiVr6GvyNy9SdOPlTQmWBXW5rRLenFkrsXcK-6qrwmWeMwGOcg_MtEXvwu5kOIJpnafzfIip0ExOk5eXrjc6NP11ITxmaC4qFPC08DtvRxJ-rpMISYpU9liEse6fYazbi-4DJp3MdKFTq8jzUlodrv7Q4HC1BnlMCgERhn0XxnaR2Hg1hj4gO1clLCoIxj9uAUEOe0pU4YlNK7nikQqsJPc0J_WegsvYiFhkiKXX1LoA0l6_NScE7NrHYTAuCRAnn17lKBoUDRUWVq-pWaTOOLeLJxGDN2mBNS0VUMuwaQvgx2WH7S-VpZzyEdiglaYuerKHQpZPJ5W9_s40-fcknS2QNAqLnBGz31uQ5u4S4ZtNG_tNGZyZK8A5nV5TY58wCcT7O-B4rpt_Sig83ZPWDnBg9g44P9G1Za_Q=w1661-h960-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 3 :: Confusion Matrix When 50 Faults are Present    
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;100-false-constraints&quot;&gt;100 False Constraints&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we move to a graph that contains 100 false constaints. The optimization of the graph can be seen in the video below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/i-vMSbxLYvA&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; align=&quot;center&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As can be seen above, the optimize does not perform well when 100 false constraints are present. This is because the classifier inaccurately classified 10 measurements as inlier when they were erroneous measurements.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/XD2F4q7uRVIEbUI1Q-AbBuGXKJiIxaRdKbFh_UcXpp2S9ZHdg896hRqv8m7TXS3jMeOZtnF_wPWEJTBYSQG-2MNaXUwZBoYl_U_2Ue9Hm2rjx5l2qn7qVa3DEBbVXWmunVSpwie448J6flmNJLR7y_ayJzon_BKBBJWkYq4BZ3GIgnD-fFllW8Knxp1kTEXf19LzsXm-baxWmYiFaO-6g1fVpTG1AmUUzL-MHkKc-_xv40GPvAlomcOgC72Rydpq4I-pRP3BQGuWqWnAV3thUVjXXSzitPnLs6mOHrtrzohERfF8Fki9wUBvj40xSGB5haRB6-rRx8ET2OUsZL1eMb9X0E6gwyypJ56m2--btFLPKO8HA20x2GinMNY9YQBaUU3_HQDKpXsS4EQl02GYjrm2lXNJqzib9SDHu-X5PBLfPrVayBoMQQ2efwH8HW3T1H0Du4K_CgrYgWNGbWqTWm4Yy1hPRbsTthSiT0Co0Ml640Itp815skztH1YueWxDfAzjR-jmdfKygc77R2mgCHlTGp3V983u33Sa3UPgGWTb_dRKQ1KZv17zZTMgVzeXy4dBtnB7hHHdohQVGMJwOirCPq6QqCAgWxArSUGkght4iApxWIICgbGdOR7A8lS-I_RZAETicrhkIAfRyuIJHCw7iLO0HHgAMMOTtUcasAzUvQ=w1577-h911-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/XD2F4q7uRVIEbUI1Q-AbBuGXKJiIxaRdKbFh_UcXpp2S9ZHdg896hRqv8m7TXS3jMeOZtnF_wPWEJTBYSQG-2MNaXUwZBoYl_U_2Ue9Hm2rjx5l2qn7qVa3DEBbVXWmunVSpwie448J6flmNJLR7y_ayJzon_BKBBJWkYq4BZ3GIgnD-fFllW8Knxp1kTEXf19LzsXm-baxWmYiFaO-6g1fVpTG1AmUUzL-MHkKc-_xv40GPvAlomcOgC72Rydpq4I-pRP3BQGuWqWnAV3thUVjXXSzitPnLs6mOHrtrzohERfF8Fki9wUBvj40xSGB5haRB6-rRx8ET2OUsZL1eMb9X0E6gwyypJ56m2--btFLPKO8HA20x2GinMNY9YQBaUU3_HQDKpXsS4EQl02GYjrm2lXNJqzib9SDHu-X5PBLfPrVayBoMQQ2efwH8HW3T1H0Du4K_CgrYgWNGbWqTWm4Yy1hPRbsTthSiT0Co0Ml640Itp815skztH1YueWxDfAzjR-jmdfKygc77R2mgCHlTGp3V983u33Sa3UPgGWTb_dRKQ1KZv17zZTMgVzeXy4dBtnB7hHHdohQVGMJwOirCPq6QqCAgWxArSUGkght4iApxWIICgbGdOR7A8lS-I_RZAETicrhkIAfRyuIJHCw7iLO0HHgAMMOTtUcasAzUvQ=w1577-h911-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 4 :: Confusion Matrix When 50 Faults are Present    
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As, can be seen from the discussion above, the classifier does not performs as well as would be expected when sufficiently many outliers are added to the graph. To combat this, the optimization routine will be re-written to iterate between the E.M. covariance estimation step and the  Max-Mixtures optimization step. The new optimization routine is depicted in Figure 5.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/rreZUXFgydAv6svoRI_uqlCuCeymTtFlyW0Mo8liwoNIsd2uNOxsGQ-b3M8M3V8jnHNtUBvISAWud-f_sRsTlktMNwkWnvi5N1OFwkFIualh-xhAmg8CMteyU5_s5vAFqyX9MQDBqMZ7IyECKUGNJpDykij0ukw4M6UBA9lM1O73MFZPy4t8gSGSOKZmE4ScTicbwnoj4SC3RhHefcjeAxd8_F1xmuXX0vvE-YmhNTTZmjZrV88H_Gxw1VXCMPFv8e4_p-uyd2yni6ypRhpSV4SFfYU172uZ0MBTzc0u9m_YppDGZmp8XmQ7y8rjZqceiyiY-9tzwMtgHJ2voYgQU7GLExZAqWD0_fNhHBHntUj00OR3GEyXRuNl609-fQonju37qzdvlZ_b_QiyXL321CSk6t1vx4GAbPLqn_7uNPB8tKUMntmiuCzP1HKCeEQWdPY4Z7CIXGDBJrQC8OnhC9hVjTEKqRb9aNaGQ8-idj3z4ZTJltfU6pqov_vsAsNylL1TUpqa3FExdDVATQUrRoKQrc_BQnXt2Huudq-3Grt2OehMBI23NKd5_6AmRU9tYLXd5TPDFv520XGTS7-CzJ0bZiCn_V_TZnc9UArt-fzTxeVJPNq8ObBkXk7UT_CgR34EsLL6pFPGwHzSrIkb01r1dSDhOh_A-JjKggReJ7yWYg=w536-h960-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/rreZUXFgydAv6svoRI_uqlCuCeymTtFlyW0Mo8liwoNIsd2uNOxsGQ-b3M8M3V8jnHNtUBvISAWud-f_sRsTlktMNwkWnvi5N1OFwkFIualh-xhAmg8CMteyU5_s5vAFqyX9MQDBqMZ7IyECKUGNJpDykij0ukw4M6UBA9lM1O73MFZPy4t8gSGSOKZmE4ScTicbwnoj4SC3RhHefcjeAxd8_F1xmuXX0vvE-YmhNTTZmjZrV88H_Gxw1VXCMPFv8e4_p-uyd2yni6ypRhpSV4SFfYU172uZ0MBTzc0u9m_YppDGZmp8XmQ7y8rjZqceiyiY-9tzwMtgHJ2voYgQU7GLExZAqWD0_fNhHBHntUj00OR3GEyXRuNl609-fQonju37qzdvlZ_b_QiyXL321CSk6t1vx4GAbPLqn_7uNPB8tKUMntmiuCzP1HKCeEQWdPY4Z7CIXGDBJrQC8OnhC9hVjTEKqRb9aNaGQ8-idj3z4ZTJltfU6pqov_vsAsNylL1TUpqa3FExdDVATQUrRoKQrc_BQnXt2Huudq-3Grt2OehMBI23NKd5_6AmRU9tYLXd5TPDFv520XGTS7-CzJ0bZiCn_V_TZnc9UArt-fzTxeVJPNq8ObBkXk7UT_CgR34EsLL6pFPGwHzSrIkb01r1dSDhOh_A-JjKggReJ7yWYg=w536-h960-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 5 :: E.M. Optimization Flowchart    
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Additionally, we will scale the final estimated covariance by the Neyman–Pearson lemma, which will provide information about the likelihood of the observable being a false alarm.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 25 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/07/25/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/07/25/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Gausian Mixture Model</category>
        
        <category>max-mixtures</category>
        
        <category>E.M. Optimization</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Initial Processing Test of EM on Faulty Pose Graphs</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In the last update we looked into the sensitivity of the Dirichlet process to its hyper-parameter, $\alpha$. Today, we move to processing faulty pose-graphs with our modified max-mixtures model. To conduct the initial test, we will utilize the Manhattan 3500 pose graph, as depicted in Figure 1. Using this pose-graph, we add 50 false constraints to test the estimators ability to robustly optimize in the face of faulty measurements.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/5Cj-uK-5pzui25GzHKXZeG6dlW-M3fgMmpgQyuzm50YjuqFMqcSUz8L-xFPTbk72xGP2RkJEP_mtgP7a7Smzj_GOnY4kDgW7D9GqWXYHbCNspGqE9VSjDcro9NOGDx-1zEZ6upqO03GpGQiSr-BmUqYDsDMUuSxFyAWP9sB7vA3xhNCUuu9x_H389yHgut8OrJhSkRV5Lh5MR9nZIPEyGCwE2X2rxPUisMp5qSA2cJQKoq8HRK2hw3Tyo62JNqBl39Q7BLLy5a2IIGMdAQS__PkUB0byNHCKm4oeeZQBhITOFEcgeRHB4PS5woYqwlJnf48V-Dl6Oty_MNeyVPgRwlSksgYxQkQdxM5CAM-ammMrdSZRHYqK1vkR6EB_wHKPLBPALVVNK7VHH8CXyDEObMjljTxkor1d8ZgmiRlVfE9kqiEipK4S2Mpwz4VoP9SVMbvIVXds7SZ9rmUKXFV5r1lYms18LW0D8cWil3QUGDvvk4RaYJo1OmZEoTwMJndS_Xq4E7xHGNslm0TzH3EO4vfea5_4XrfUozQYnFz4U4mW9ZnfcG7c-jWHLb5Rb8d2kgMpP3MM4DVCccgLjfCDZMc_rHrPsbyEe1nm3FUklso-HWJs-VEJm1ZD=w608-h542-nosss&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/5Cj-uK-5pzui25GzHKXZeG6dlW-M3fgMmpgQyuzm50YjuqFMqcSUz8L-xFPTbk72xGP2RkJEP_mtgP7a7Smzj_GOnY4kDgW7D9GqWXYHbCNspGqE9VSjDcro9NOGDx-1zEZ6upqO03GpGQiSr-BmUqYDsDMUuSxFyAWP9sB7vA3xhNCUuu9x_H389yHgut8OrJhSkRV5Lh5MR9nZIPEyGCwE2X2rxPUisMp5qSA2cJQKoq8HRK2hw3Tyo62JNqBl39Q7BLLy5a2IIGMdAQS__PkUB0byNHCKm4oeeZQBhITOFEcgeRHB4PS5woYqwlJnf48V-Dl6Oty_MNeyVPgRwlSksgYxQkQdxM5CAM-ammMrdSZRHYqK1vkR6EB_wHKPLBPALVVNK7VHH8CXyDEObMjljTxkor1d8ZgmiRlVfE9kqiEipK4S2Mpwz4VoP9SVMbvIVXds7SZ9rmUKXFV5r1lYms18LW0D8cWil3QUGDvvk4RaYJo1OmZEoTwMJndS_Xq4E7xHGNslm0TzH3EO4vfea5_4XrfUozQYnFz4U4mW9ZnfcG7c-jWHLb5Rb8d2kgMpP3MM4DVCccgLjfCDZMc_rHrPsbyEe1nm3FUklso-HWJs-VEJm1ZD=w608-h542-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: Manhattan 3500 pose graph    
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As a reference, we first optimized the faulty pose graph using $L_2$ optimization. This is depicted in the video provided below. As video depicts — and as expected — traditional $L_2$ optimization does not perform well when presented with a faulty graph.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/pikWgy_pOoA&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; align=&quot;center&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Next, we optimize the graph using E.M. to estimate the covariance model and max-mixtures to select the most fitting component. When this model is employed, the optimized graph much more closely resembles what it would look like if no faults were present. A video showing the optimization is shown below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/rLqa63dV4TQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; align=&quot;center&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The code used to conduct this test can be found &lt;a href=&quot;https://github.com/watsonryan/summerAFIT/blob/master/bayes_gmm/examples/emMix2G2o.py&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h3&gt;

&lt;p&gt;Short term to-do list&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Compare to other robust models&lt;/li&gt;
  &lt;li&gt;Test on several graphs&lt;/li&gt;
  &lt;li&gt;Generate confusion matrix to check classifier accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 24 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/07/24/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/07/24/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Gausian Mixture Model</category>
        
        <category>max-mixtures</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Processing Oxford RobotCar Dataset</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;novatels-tightly-coupled-solution&quot;&gt;NovAtel’s Tightly-Coupled Solution&lt;/h1&gt;

&lt;p&gt;To begin, we can extract NovAtel’s tightly-coupled solution from the ${\it{INSPVA}}$ packet. This solution is depticted below as a ground trace in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/H2SJH2FfF4p_G-OcQWjUuXfiJ57oyU4hoALsuUbcWg7N0ip_4JmPquxyEfMHitb_5Nsvx58FY460GW1ELttU6uIhYTJyhvz9chugUYyE09935sa3Vb1aA61zYgcHWaCfIZMvXN1Tlav8JtNjW5wWz1m68F1b4NLxkBc1jqbviNE2KlnO7Gp9T5LmFxeu5_ujcvRI3mUHpfl2B45QIA98vPQ9rIbkviOuFgoiwDa-Sw-Li-T534uX8lC8zssUkxCh_Ifgk9zJmfxdlyN5dVGpHlgdtAPajFs1GEpzww2KNAVc0DVTZRQIWHs4rjzcaYRDER-ZuIjKopQ5xaxTWYUZojIq0kHxS855mihFS1sTYy93ibiVQoVyB1OjMmBJQ5PQb3qO3MCvkBvaYdjqO5etzT80PNjM67LiS-uaXoXUFwRB9FCZ6vlP_aSvpHe2b-BImNzn6aX7R4ukuEKn1ySUOig6Nq8rV6GQsLv5Kz3gS_Ivjbn5GEcbd0h9YxBd6rh0jrtef_JavI2PXeH0fLM70t5mDBx3VxLVkMhdKutO0nylNjamHyF_LNnq1O8Oy5qGIox2T22uXAvjHdOkmQZQnM2fphn-8v6UqI1O_pyPQTadfvx_OrhQWk1R=w1280-h751-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/H2SJH2FfF4p_G-OcQWjUuXfiJ57oyU4hoALsuUbcWg7N0ip_4JmPquxyEfMHitb_5Nsvx58FY460GW1ELttU6uIhYTJyhvz9chugUYyE09935sa3Vb1aA61zYgcHWaCfIZMvXN1Tlav8JtNjW5wWz1m68F1b4NLxkBc1jqbviNE2KlnO7Gp9T5LmFxeu5_ujcvRI3mUHpfl2B45QIA98vPQ9rIbkviOuFgoiwDa-Sw-Li-T534uX8lC8zssUkxCh_Ifgk9zJmfxdlyN5dVGpHlgdtAPajFs1GEpzww2KNAVc0DVTZRQIWHs4rjzcaYRDER-ZuIjKopQ5xaxTWYUZojIq0kHxS855mihFS1sTYy93ibiVQoVyB1OjMmBJQ5PQb3qO3MCvkBvaYdjqO5etzT80PNjM67LiS-uaXoXUFwRB9FCZ6vlP_aSvpHe2b-BImNzn6aX7R4ukuEKn1ySUOig6Nq8rV6GQsLv5Kz3gS_Ivjbn5GEcbd0h9YxBd6rh0jrtef_JavI2PXeH0fLM70t5mDBx3VxLVkMhdKutO0nylNjamHyF_LNnq1O8Oy5qGIox2T22uXAvjHdOkmQZQnM2fphn-8v6UqI1O_pyPQTadfvx_OrhQWk1R=w1280-h751-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: NovAtel's Tightly-Coupled Ground Trace   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Through visual inspection, this solutions looks good. However, to quanity the accuracy of the solution, a RTK solution will be generated.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;rtk-solution&quot;&gt;RTK Solution&lt;/h1&gt;

&lt;p&gt;To generate the RTK solution, which will be used as a “reference truth” for all later comparisions, the open-soure library, &lt;a href=&quot;http://www.rtklib.com/&quot;&gt;RTKLib&lt;/a&gt;, will be utilized. This allows use to incorporate precise obrit/clock products with aditional observablse provided via a basestation to iteratively solve for the platforms location. An overlay of the RTK ground trace is provided below in Figure 2.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/LKrowhz9Dn8CRPLNnUzLfkCNDNkLKouhLG55obTSBjIuP4xTTbjaIBjZF_VVwgsJ0CQTHfUD5uSldhrnBeHqOFPrFH2IuCB9OwZu2tkz2kxBRXt9F-JGT53qPExA1I3aBBlkGPbq8aybsggywh7yuJq2ax3ISQyCBBtmSlIaHtIiaxk69nz_2M11ffKX7npPtpE9uUpxHVp5gUBjSwL_yr2Fu41PclHSmf0MqxQKFZpKAGol5dcxhVyal3ecW9WfQAcN6W8oOzI310kERcTekZ2V_byPlA7sW1Uls1fiQyR0wZfApq-DQlxTRbuzQ6v30DasQdQAExXW1BqtfG1eq6ZbrrwLRG0FY83gY6Ov-6AfvW0mb4V0q3wUv2o9JE4mCJsiRcG1JnkcNGJWH4p9xebPJ1tkNc-QmRmprUiBlqsnF1OS5xxWDK3L916uGv7MZPbHBw5_DOSmvvCHHvMioNpnl5Sit_1kCx_iJB_RM9n5dh7Ua2gfXSK5IubtsY3ViNJBhxD81nWh4y56lD6S-6zDQ6h30rrJ63QEFLqbx7vPBT3NG1FcjP-HW-yJcB38KkDgL6KA3GU-WRUTtrqCNfH9Db2KZTc8ESc0nqzCqPpwhM71KbdTjSzqCisCSDE4HZXzRSvnrrbTuRnqQyycY5lIG6oKz6Plc7tRuX2HEzHCwQ=w1280-h751-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/LKrowhz9Dn8CRPLNnUzLfkCNDNkLKouhLG55obTSBjIuP4xTTbjaIBjZF_VVwgsJ0CQTHfUD5uSldhrnBeHqOFPrFH2IuCB9OwZu2tkz2kxBRXt9F-JGT53qPExA1I3aBBlkGPbq8aybsggywh7yuJq2ax3ISQyCBBtmSlIaHtIiaxk69nz_2M11ffKX7npPtpE9uUpxHVp5gUBjSwL_yr2Fu41PclHSmf0MqxQKFZpKAGol5dcxhVyal3ecW9WfQAcN6W8oOzI310kERcTekZ2V_byPlA7sW1Uls1fiQyR0wZfApq-DQlxTRbuzQ6v30DasQdQAExXW1BqtfG1eq6ZbrrwLRG0FY83gY6Ov-6AfvW0mb4V0q3wUv2o9JE4mCJsiRcG1JnkcNGJWH4p9xebPJ1tkNc-QmRmprUiBlqsnF1OS5xxWDK3L916uGv7MZPbHBw5_DOSmvvCHHvMioNpnl5Sit_1kCx_iJB_RM9n5dh7Ua2gfXSK5IubtsY3ViNJBhxD81nWh4y56lD6S-6zDQ6h30rrJ63QEFLqbx7vPBT3NG1FcjP-HW-yJcB38KkDgL6KA3GU-WRUTtrqCNfH9Db2KZTc8ESc0nqzCqPpwhM71KbdTjSzqCisCSDE4HZXzRSvnrrbTuRnqQyycY5lIG6oKz6Plc7tRuX2HEzHCwQ=w1280-h751-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: RTK Ground Trace   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;comparison-between-novatel-and-rtk&quot;&gt;Comparison Between NovAtel and RTK&lt;/h1&gt;

&lt;p&gt;Use the RTK solution generated by RTKLib, we can evaluate the estimate accuracy of the solution provided by the NovAtel SPAN system.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;processing-with-nasa-jpls-precise-point-position-libraries&quot;&gt;Processing With NASA JPL’s Precise Point Position Libraries&lt;/h1&gt;

&lt;p&gt;Now, we can processing the data using NASA JPL’s Precise Point Positioning (PPP) libraries.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gipsy---ii&quot;&gt;GIPSY - II&lt;/h2&gt;

&lt;p&gt;First, we will utilize NASA JPL’s GNSS-Inferred Positioning System and Orbit Analysis Simulation Software package (GIPSY-OASIS). This package has been the primary geodetic and positioning software for several NASA missions.&lt;/p&gt;

&lt;h4 id=&quot;pseudorange-only-solution&quot;&gt;Pseudorange Only Solution&lt;/h4&gt;

&lt;p&gt;To provide an initial trajectory for iterative processed PPP solution, we will initially generate a solution that utilizes only the pseudorange observables. The East/North ground trace of this solution is shown below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/RnrXXZydRZpoBmlWp0ZQ02W6iT8XLYvGy2WGOv6mO-4M0FWfom-WY1n5OltpJ94eMcIMyImN7rlqEbi731IPDizCyDmce8reLkJNq5gBvTs0RiAO95pdqSMXvTy02citypu9VH23lARx6PbD9rIMbZcAx9G1tB8nr1WSx7VZVkBN95BCWIc-nsxHYeeoGVz3zzqunyd_BnlSU1q5A9acZj2Unv80M0pG4KALUIxDcP8iS-hNIiWvrA0jMiBPnLlhGrgrIBfOxTBYV0WmL_Uei2duotyrsQRa3Q2jLtwi6nUxo98FxFxtzWq3OYkXEz7mj1Tenu5pp2Vl8Wg865Z-DsMux_fGFAZ7lutbF7MSw6HEyAgJB4RuiMytqkNLoYpV3tqn1fe64flaET_0ChCGm3oiS7sKHfN-bn6MwBCADUqWzuXBscsxsM4aPcGFUZ0JhVztuS1SK-VOniJmLuFTM_YeJLGtC6tXExFeP88NvJfMXYFMqoAS25J34iGl5M4UqY-YCo1Av9NeoU2vxZGQVNvehK0J9FIYp_B9C8kHRVXOsPX-3K3UnP9-w88FbeElRTnVjJDALv7Oyb7hLFgSkR_3urYtAhVzDy1BVYMvC3DPjXGdV-JRi21D=w720-h504-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/RnrXXZydRZpoBmlWp0ZQ02W6iT8XLYvGy2WGOv6mO-4M0FWfom-WY1n5OltpJ94eMcIMyImN7rlqEbi731IPDizCyDmce8reLkJNq5gBvTs0RiAO95pdqSMXvTy02citypu9VH23lARx6PbD9rIMbZcAx9G1tB8nr1WSx7VZVkBN95BCWIc-nsxHYeeoGVz3zzqunyd_BnlSU1q5A9acZj2Unv80M0pG4KALUIxDcP8iS-hNIiWvrA0jMiBPnLlhGrgrIBfOxTBYV0WmL_Uei2duotyrsQRa3Q2jLtwi6nUxo98FxFxtzWq3OYkXEz7mj1Tenu5pp2Vl8Wg865Z-DsMux_fGFAZ7lutbF7MSw6HEyAgJB4RuiMytqkNLoYpV3tqn1fe64flaET_0ChCGm3oiS7sKHfN-bn6MwBCADUqWzuXBscsxsM4aPcGFUZ0JhVztuS1SK-VOniJmLuFTM_YeJLGtC6tXExFeP88NvJfMXYFMqoAS25J34iGl5M4UqY-YCo1Av9NeoU2vxZGQVNvehK0J9FIYp_B9C8kHRVXOsPX-3K3UnP9-w88FbeElRTnVjJDALv7Oyb7hLFgSkR_3urYtAhVzDy1BVYMvC3DPjXGdV-JRi21D=w720-h504-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-XEN5fUgMoaSc_zpsliygmo1g.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 3 :: GIPSY Pseudorange only solution   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;iterative-ppp-solution&quot;&gt;Iterative PPP Solution&lt;/h4&gt;

&lt;h2 id=&quot;rtgx&quot;&gt;RTGx&lt;/h2&gt;

</description>
        <pubDate>Fri, 21 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/labupdates/2017/07/21/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/labupdates/2017/07/21/Update.html</guid>
        
        <category>labUpdates</category>
        
        <category>oxbot</category>
        
        <category>rtgx</category>
        
        <category>rtklib</category>
        
        <category>gipsy</category>
        
        
        <category>_projects</category>
        
        <category>labUpdates</category>
        
      </item>
    
      <item>
        <title>G.M.M. Sensitivity to Dirichlet Hyperparameter</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Yesterday we saw that the E.M. G.M.M algorithm approaches the true distribution if allocated a sufficient number of iterations. However, for the test conducted yesterday, the Dirichlet hyperparameter ( $\alpha$ ) — the tuning parameter on the categorical distribution, depicted in Figure 1 and Figure 2 — was fixed at $1e^{-6}$. Today, we would like to see the sensitivity of the estimator to the value of $\alpha$.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/beNGMOv6TqwTOY_ju-mw02bcOgF2TyG25jiZ6RxuwK-LlSzVuO9QLuoIDcTcDx8C4YWauAgmPhM9r2qQhiTS7JCSQDpA80o7r0u5xP1vdp9JXlreigEH7FmDYZJPOhV1lexinu787LaF32uaM1vftKDgKgftkX8dPCBZq2PMSB9ojpJCU4jRsoIn3NJ_Rm686Xf6vzvMWjfb6bejn1cSs6Gy9wrHGS4gg--NVpKiKqcjgFeESupfyzTIhSfFM0qGD9NHGLZ2KKqMtreltUBG-VGS0IoXkuTfBRJ7wJQuNyjlfo0LbiXpIpG_KNUf2koXN9yzQFEI37vAWTEgF7nt1YuvdeqxaiPfQ1KT7Xqqg74MH72ZMcqLrA9rAsL9h2QMntWdROnyPLE9etkqj26lNo58p5tzay_iPbIxKqEPOXOpBFJxfr0JdIdUsEpZpllDDvAEgcrLqK-_ZK75EqPzmFjQNljYHP7F17-b28bPIkB5q3cl64EZ1pcw2o7KqdU4TDwvar7r6MxUpBXbq5yqE2aZ1RUPe1lDLiKNCFvkNV4uPP68QIsc4QbWdCVOk47Btv6xava5FhzHjBboYYaEnQzgXzb6rkhzJZHJmPHwQMO3IObL7MGdUWuyn8_IigOBWi3QiVc5oAwDUZZVuk_Aa-OgPKPzvgLdLWl49SEeINeMww=w132-h232-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/beNGMOv6TqwTOY_ju-mw02bcOgF2TyG25jiZ6RxuwK-LlSzVuO9QLuoIDcTcDx8C4YWauAgmPhM9r2qQhiTS7JCSQDpA80o7r0u5xP1vdp9JXlreigEH7FmDYZJPOhV1lexinu787LaF32uaM1vftKDgKgftkX8dPCBZq2PMSB9ojpJCU4jRsoIn3NJ_Rm686Xf6vzvMWjfb6bejn1cSs6Gy9wrHGS4gg--NVpKiKqcjgFeESupfyzTIhSfFM0qGD9NHGLZ2KKqMtreltUBG-VGS0IoXkuTfBRJ7wJQuNyjlfo0LbiXpIpG_KNUf2koXN9yzQFEI37vAWTEgF7nt1YuvdeqxaiPfQ1KT7Xqqg74MH72ZMcqLrA9rAsL9h2QMntWdROnyPLE9etkqj26lNo58p5tzay_iPbIxKqEPOXOpBFJxfr0JdIdUsEpZpllDDvAEgcrLqK-_ZK75EqPzmFjQNljYHP7F17-b28bPIkB5q3cl64EZ1pcw2o7KqdU4TDwvar7r6MxUpBXbq5yqE2aZ1RUPe1lDLiKNCFvkNV4uPP68QIsc4QbWdCVOk47Btv6xava5FhzHjBboYYaEnQzgXzb6rkhzJZHJmPHwQMO3IObL7MGdUWuyn8_IigOBWi3QiVc5oAwDUZZVuk_Aa-OgPKPzvgLdLWl49SEeINeMww=w132-h232-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: Infinite Dirichlet Process    
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/1UQVXbLhYfSQIZSAJjebvrbFM9wzF2OyuTBcYh-Fy1rzIcSzk-6iSvAEOVW9zBXS206XIZJ_Ll0Ot42HEsLrgHBEih-OvXF0VEAEKSdZZQjaXIzSXQRimB9t6s_J2hI9w3NocS6d-HjptdeaWBbQwoSqzzl474xPrESQlkL8kEC2agcDtxQ94ZppbnVSIO5HbCRJb21UGGtsGVHK4TX7k9B_mO_LfcePLrEzRNh2BxDqjf_mC6fnzzFiEO6sJBXXVDS9B3PnCi_cW0xMGNTPnaHe-2dpYR1NnOOltmz6Y61lj5dZKzIRWYrheOeBg-v5czs9UUNz8dbbMo7rOWsrA-TaWSSt3lujs-zNHsmAOIhoZW3C7t0Eai0Ux4UkJlPvBTfDKOMSEw_VHGK0BmLHiAeSgtREbiqHaU1mUWxUJkIRy3qNpGprcQtkaz4qUlLuNVSu8Ja7IHKLeayEz1BnHSuHEqoYs44Hl4dZQhMWnVKceBQXfopeFAcR53yk-3Vx6xVuyHeY2p4LYGq-1KmbWxM86ECdkb-vlOqEpgPji3w32x0yPX6NoNzeBm68jqmcw0vO64018OunyAavFoOZc2DflvRfyIGwP6R7tieP6V0YuCuzn-9CriTiuMO-C1sb09Z2Tf1pExayTNXNlNLANhWb2rfCkFH3kXT8KlriCJwiVg=w1024-h293-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/1UQVXbLhYfSQIZSAJjebvrbFM9wzF2OyuTBcYh-Fy1rzIcSzk-6iSvAEOVW9zBXS206XIZJ_Ll0Ot42HEsLrgHBEih-OvXF0VEAEKSdZZQjaXIzSXQRimB9t6s_J2hI9w3NocS6d-HjptdeaWBbQwoSqzzl474xPrESQlkL8kEC2agcDtxQ94ZppbnVSIO5HbCRJb21UGGtsGVHK4TX7k9B_mO_LfcePLrEzRNh2BxDqjf_mC6fnzzFiEO6sJBXXVDS9B3PnCi_cW0xMGNTPnaHe-2dpYR1NnOOltmz6Y61lj5dZKzIRWYrheOeBg-v5czs9UUNz8dbbMo7rOWsrA-TaWSSt3lujs-zNHsmAOIhoZW3C7t0Eai0Ux4UkJlPvBTfDKOMSEw_VHGK0BmLHiAeSgtREbiqHaU1mUWxUJkIRy3qNpGprcQtkaz4qUlLuNVSu8Ja7IHKLeayEz1BnHSuHEqoYs44Hl4dZQhMWnVKceBQXfopeFAcR53yk-3Vx6xVuyHeY2p4LYGq-1KmbWxM86ECdkb-vlOqEpgPji3w32x0yPX6NoNzeBm68jqmcw0vO64018OunyAavFoOZc2DflvRfyIGwP6R7tieP6V0YuCuzn-9CriTiuMO-C1sb09Z2Tf1pExayTNXNlNLANhWb2rfCkFH3kXT8KlriCJwiVg=w1024-h293-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 2 :: Alpha Distribution Over Probability Simplex   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;To conduct this test, a simulated dataset was generated. This simulated dataset contains 10,000 inlier points and 3,000 outlier points. A scatter of the set is depicted in Figure 3, where the black points represent the inliers and the red points represent the outliers.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/eyvjigar9lYy1zrZ8ecWdJ8ZoCT8ZNTCv86KfXGSS9SjscK9h-DNQnP4OpSMGmoBpM-0OqUcKYvm89gyiAFlzlejxNw4yR0p-Id92-I3cs40RCk5rJZZGC0EXpha7v865HzSR7bwtKLvrJ1WA4TtaKIxCLz8PeqVAmRkh7pYYilbbfg1sAKgPj6kZsWPHNsna-Iu6ej-laDylDrQklBxVYAxr7oLz9f99jlEAFyZj7qCPkUsZClbHP_7OtDHnRHecD1zdstY-HQgsD1yxCdmxmoMzrvgktknxaDBh_y7rCBvvNb1MP9hqD59nDQbGBaiPMEt5UVC4iKWhpX48CY1lw5mnsezRvZUhlnHfgKKDKP7lQ3p6FhO3gqKUu-R3Sbu0rZDF9mecQrefesSpj5YxknTxZmrg2-oS7ZVaU2KzC_NYwRNS4eDKxj-KdxqYx-ITtqSVYUevGkvJRoRFFeyrqdOa7UhCoVaW865KrJZnOnVitePRKB9_bxOGZhrzmemXwRUrudn2CukTC5VMR_V3WrD-KdQVRKsRDFRFJu7sVIxWNRj8IjcOs4NyhHc2_9VbHETDa2-Gfu4W3CpcH9RcxYa8ju_Zwgw5YB9S4Xhw_HC-roS53dBHKfmSPp_t4ejADgYt__DAJX95urYrOdP2rebmC8401MhrXESHWZ4FZS_OA=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/eyvjigar9lYy1zrZ8ecWdJ8ZoCT8ZNTCv86KfXGSS9SjscK9h-DNQnP4OpSMGmoBpM-0OqUcKYvm89gyiAFlzlejxNw4yR0p-Id92-I3cs40RCk5rJZZGC0EXpha7v865HzSR7bwtKLvrJ1WA4TtaKIxCLz8PeqVAmRkh7pYYilbbfg1sAKgPj6kZsWPHNsna-Iu6ej-laDylDrQklBxVYAxr7oLz9f99jlEAFyZj7qCPkUsZClbHP_7OtDHnRHecD1zdstY-HQgsD1yxCdmxmoMzrvgktknxaDBh_y7rCBvvNb1MP9hqD59nDQbGBaiPMEt5UVC4iKWhpX48CY1lw5mnsezRvZUhlnHfgKKDKP7lQ3p6FhO3gqKUu-R3Sbu0rZDF9mecQrefesSpj5YxknTxZmrg2-oS7ZVaU2KzC_NYwRNS4eDKxj-KdxqYx-ITtqSVYUevGkvJRoRFFeyrqdOa7UhCoVaW865KrJZnOnVitePRKB9_bxOGZhrzmemXwRUrudn2CukTC5VMR_V3WrD-KdQVRKsRDFRFJu7sVIxWNRj8IjcOs4NyhHc2_9VbHETDa2-Gfu4W3CpcH9RcxYa8ju_Zwgw5YB9S4Xhw_HC-roS53dBHKfmSPp_t4ejADgYt__DAJX95urYrOdP2rebmC8401MhrXESHWZ4FZS_OA=w1024-h566-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 3 :: Simulated Distribution   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using the distribution shown above, the E.M. G.M.M algorithm was utilized with a varying $\alpha$ value to test the estimators sensitivity w.r.t $\alpha$. In Figure 4 the error of the estimated covariance w.r.t the $\alpha$ value is depicted. From this figure, it can be seen that the error tends to decreases as the value taken by the $\alpha$ parameter decreases.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/qYZHooMZpRK9gV0CLIaJ5nYizWwHlmvqADEHdv21sgoQqR2TH_btk7-und9gpIYR0e38gN6IyayREpc6jre-dLpOJRgWl5pROPsR0xYj_-BS2o4i-juQs3dP7f3rs78P3zXGAd41xtsnvYx7WQE3pFTi1ImGjsfmBaGHDPHUQTiwkzuIANv5d0FZlQdAWZC0ezhWv60scafM7cBQ0UwKXlJMHGN4eO3KSPAljG7tx-Cgs5d-NeYlEPCysPQUi4Gm0ETz1CClG65uXlzZtN5OqeuVN3QCi45yCWbdyqqdG4SDHsSMYazF1V2i61_AX2xpVxFkuTLKzWdZOUUyhBx9VPj5JqJeEzPpOrw_gx4gPG8803vAHcxDaoqJpVO8yEreoqoz-dTqYuQ14nR2BGBfgeFZcFNruDsZ3lfZUPHZx-Hi6lX-1RhR8sNIyR7dMYwPwUCl-1IjOZkU9YoHIG5yliDaH5h8j-WlnIvWBQLtsvec94e1DVXx_2mOjHmuMoMmyygzc6X9l4ybRE-taaWKIzl5kYirUWm3UjN2NXQTXunsz-dUXOv0Z_5EMa4vDQXCn6Nae0tJo0MHJomwIzFu3xuxyZKxLeKqOLMs_C0TeW0sZz-Gux8oQeJ3Pf7gvSoavQyrCBGjz_0dZao_P3b6H-i7ZlqE1JQBmNbe2Bg65iTMeA=w1680-h928-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/qYZHooMZpRK9gV0CLIaJ5nYizWwHlmvqADEHdv21sgoQqR2TH_btk7-und9gpIYR0e38gN6IyayREpc6jre-dLpOJRgWl5pROPsR0xYj_-BS2o4i-juQs3dP7f3rs78P3zXGAd41xtsnvYx7WQE3pFTi1ImGjsfmBaGHDPHUQTiwkzuIANv5d0FZlQdAWZC0ezhWv60scafM7cBQ0UwKXlJMHGN4eO3KSPAljG7tx-Cgs5d-NeYlEPCysPQUi4Gm0ETz1CClG65uXlzZtN5OqeuVN3QCi45yCWbdyqqdG4SDHsSMYazF1V2i61_AX2xpVxFkuTLKzWdZOUUyhBx9VPj5JqJeEzPpOrw_gx4gPG8803vAHcxDaoqJpVO8yEreoqoz-dTqYuQ14nR2BGBfgeFZcFNruDsZ3lfZUPHZx-Hi6lX-1RhR8sNIyR7dMYwPwUCl-1IjOZkU9YoHIG5yliDaH5h8j-WlnIvWBQLtsvec94e1DVXx_2mOjHmuMoMmyygzc6X9l4ybRE-taaWKIzl5kYirUWm3UjN2NXQTXunsz-dUXOv0Z_5EMa4vDQXCn6Nae0tJo0MHJomwIzFu3xuxyZKxLeKqOLMs_C0TeW0sZz-Gux8oQeJ3Pf7gvSoavQyrCBGjz_0dZao_P3b6H-i7ZlqE1JQBmNbe2Bg65iTMeA=w1680-h928-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 4 :: Estimated Inlier Covariance Error   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In addition to the estimator error, we can also look at the complexity of the mixture model selected for each $\alpha$ value. In Figure 5 the number of mixture components is shown w.r.t the value taken by the $\alpha$ parameter. From this figure it can be seen that the mixture model tends to only estimating one inlier and one outlier distribution as the $\alpha$ parameter decreases.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/VaoXNoidHDIL3q4C7QJUq1OYLiLQ3E7LJ2tmLXZ0AxZ5UKrFMxTBGNcDe0zRZI8yvjt_H_R7BVbvx5LtTrcqY0kxiE6xru18tK8EbbzG99PzuiMkK9zWF2ZRMpUIeV3EYAWGYBapys_sb6TGRmEfJwneR1d2RxRYl6mII0fXoscgn6ZMUpt_wQ3F1Kq8hziow_J7pzYwuHPtUUZHmIW2zpcPHnebVkYldM7syN857SW0biWKw-XSZlVQlNOAMkLrnryzYch7sfxtyuvK2q2l9HLSaKclptFE-y6bwhzvKhaqaKS5YeIAzm6HAEOyi1ZRV0bHmiYrIXg6uEwGjJQmM6Ohi_yA1t3aqBxW0t4GOgI-C1IyK90FdzNH34GJhp14cfWmSlB_CoHhRdwa2eTbc-J6e7AlXOapfUrZWh3jRUX2hJ2_XLb8XnKCy2G-6AemHgEYaWq6luG0IGMsjVGzgCWUPDEl2nxEvpphvKvCc7wSTvyW4jqWXU2_e8c8exyBdIJNfpqrlPQOXe80cJhE8NvQtXaOTWFRotEXTDig8V-eeDlQHe7ZPxXoy_AKhBs13URJHEH3kKEIxTlJFfp5lu7iyU0ZNuReNP1XnxtYLr60m7vEtrI3zdBICNab62P5CsxHpZWkKbAW-yyChfiPIuDPASrui4Ng4fxikJuwLdXmOw=w1680-h928-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/VaoXNoidHDIL3q4C7QJUq1OYLiLQ3E7LJ2tmLXZ0AxZ5UKrFMxTBGNcDe0zRZI8yvjt_H_R7BVbvx5LtTrcqY0kxiE6xru18tK8EbbzG99PzuiMkK9zWF2ZRMpUIeV3EYAWGYBapys_sb6TGRmEfJwneR1d2RxRYl6mII0fXoscgn6ZMUpt_wQ3F1Kq8hziow_J7pzYwuHPtUUZHmIW2zpcPHnebVkYldM7syN857SW0biWKw-XSZlVQlNOAMkLrnryzYch7sfxtyuvK2q2l9HLSaKclptFE-y6bwhzvKhaqaKS5YeIAzm6HAEOyi1ZRV0bHmiYrIXg6uEwGjJQmM6Ohi_yA1t3aqBxW0t4GOgI-C1IyK90FdzNH34GJhp14cfWmSlB_CoHhRdwa2eTbc-J6e7AlXOapfUrZWh3jRUX2hJ2_XLb8XnKCy2G-6AemHgEYaWq6luG0IGMsjVGzgCWUPDEl2nxEvpphvKvCc7wSTvyW4jqWXU2_e8c8exyBdIJNfpqrlPQOXe80cJhE8NvQtXaOTWFRotEXTDig8V-eeDlQHe7ZPxXoy_AKhBs13URJHEH3kKEIxTlJFfp5lu7iyU0ZNuReNP1XnxtYLr60m7vEtrI3zdBICNab62P5CsxHpZWkKbAW-yyChfiPIuDPASrui4Ng4fxikJuwLdXmOw=w1680-h928-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 5 :: Number of Mixture Components   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;From this simple experiment, we can see that our estimator is not incredibly sensitive to the value taken by the hyperparameter; however, optimal results ( w.r.t. estimator error and complexity ) are achieved by selecting a small $\alpha$ value. To validate this, later we will run a larger Monte-carlo style simulation to show that the trends hold over multiple trials.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h3&gt;

&lt;p&gt;Starting testing our mixture model on pose-graphs in G2O.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 20 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/07/20/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/07/20/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Gausian Mixture Model</category>
        
        <category>max-mixtures</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Validation of G.M.M Covariance Estimation Using Simulated Distributions</title>
        <description>&lt;p&gt;Yesterday, we tested the ability of our GMM to estimate the covariance of a residual distribution using the Manhattan 3500 pose-graph. Today, we will continue to validate our GMM by simulating a distribution to see if our GMM approaches the correct covariance as the number of iterations increases.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;first-test&quot;&gt;First Test&lt;/h3&gt;

&lt;p&gt;The first distribution that the GMM was tested on is depicted in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/eMy5-Cdp3htUZ1baVwDdHHFFWelaexhl4n8KlUsYtHtwKRdCMlv1I9RSGZmD8TpD8BZdOJyYCp3fqxntQKRsIqlSoJ2rUEkFh6c6nUtQh8nIr_RDdAHT_zaBvUnWd83oVkGADpjEdg=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/eMy5-Cdp3htUZ1baVwDdHHFFWelaexhl4n8KlUsYtHtwKRdCMlv1I9RSGZmD8TpD8BZdOJyYCp3fqxntQKRsIqlSoJ2rUEkFh6c6nUtQh8nIr_RDdAHT_zaBvUnWd83oVkGADpjEdg=w1024-h566-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: Residual Scatter ( Ratio of Inler/Outler = 0.01 ).   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using the distribution depicted in Figure 1, the GMM was utilized to provide a covariance estimate at different iteration values. The metric used to quantify the “closeness” of two covariance matricies was $ || P_{true} - P_{est} ||_{F}$.&lt;/p&gt;

&lt;p&gt;The convergence of the GMM as the number of iterations increases is shown in Figure 2.  Additionally, the final covariance estimate for the inlier distribution ( i.e. the covariance estimate when the number of iterations equaled fifty ) is shown in Figure 3.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/64yf_NF4CamayMwmgVkByCxEg1KTZeJA37x0mXBtpimDYgvQv2k7HseYfPtPxgls69trteEnhDKksUqN7hLgri7T3esWNxWF7pfHIh9bXHVGOx5bpP5_dW0I2_emE7jLn3IkgQPm1g=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/64yf_NF4CamayMwmgVkByCxEg1KTZeJA37x0mXBtpimDYgvQv2k7HseYfPtPxgls69trteEnhDKksUqN7hLgri7T3esWNxWF7pfHIh9bXHVGOx5bpP5_dW0I2_emE7jLn3IkgQPm1g=w1024-h566-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 2 :: Convergence of Covariance Estimate From GMM.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/-HghJkDadBemQSMrwTNZTeBG998S2eE1H0em1eogSbEfhSa4FopholQD_cRjWc78FxqJjTGZUmi6a-lacLNlygB9zViTpF9cFHCLj33UVt-7RucJhXI0--H9Jh9w1qZvr2-IKwXbeQ=w416-h67-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/-HghJkDadBemQSMrwTNZTeBG998S2eE1H0em1eogSbEfhSa4FopholQD_cRjWc78FxqJjTGZUmi6a-lacLNlygB9zViTpF9cFHCLj33UVt-7RucJhXI0--H9Jh9w1qZvr2-IKwXbeQ=w416-h67-no&quot; border=&quot;0&quot; alt=&quot; photo CodeCogsEqn_zpsaxae44q4.gif&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 3 :: Covariance Comparison.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;second-test&quot;&gt;Second Test&lt;/h3&gt;

&lt;p&gt;From the previous test, we have seen that the covariance estimate approaches the true covariance as the number of iterations of the GMM is allowed to increase. However, that test did not contain a high ratio of outliers. For this test, we increase the fraction of outliers in the distribution. The residual scatter for this test is depicted in Figure 4.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/YWuM0E7Ot6G2q7uMcufzgOpEt3was2X49sALk8y-cLXAUVlrPq1zzfR6GnhworD-dZ5VSeemYIrYBnpPa4K669Tv0_tip9KgykHAtHOZhV5Xms8s19oeDUrCClvI_yn1KV6YCHuO_Q=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/YWuM0E7Ot6G2q7uMcufzgOpEt3was2X49sALk8y-cLXAUVlrPq1zzfR6GnhworD-dZ5VSeemYIrYBnpPa4K669Tv0_tip9KgykHAtHOZhV5Xms8s19oeDUrCClvI_yn1KV6YCHuO_Q=w1024-h566-no&quot; border=&quot;0&quot; alt=&quot; photo simScatter2_zpsbdnst01e.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 4 :: Residual Scatter ( Ratio of Inler/Outler = 0.1 ).   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Again, the Frobenius norm was utilized to quantify the accuracy of our estimate. The accuracy of the estimate as the number of iterations increases is shown in Figure 5. This result again shows that the GMM approaches the true covariance estimate as the number of iterations increases.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/ke0DiuuYzz1vzPJGqfPaP6mXWcHNdoj4LLyUR_Xj37c4mBN-WYc817g8FqFUhv_BwR8zrYeM0lA8IlVapSTMr-jnbVvaa49HZOPvEOazWDz85ShOEV1yxkS0pgytK1ZJbQEsUJDDbw=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/ke0DiuuYzz1vzPJGqfPaP6mXWcHNdoj4LLyUR_Xj37c4mBN-WYc817g8FqFUhv_BwR8zrYeM0lA8IlVapSTMr-jnbVvaa49HZOPvEOazWDz85ShOEV1yxkS0pgytK1ZJbQEsUJDDbw=w1024-h566-no&quot; border=&quot;0&quot; alt=&quot; photo covEst2_zpsx2xkxi7d.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 5 :: Convergence of Covariance Estimate From GMM.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The final covariance estimate for the inlier distribution ( i.e., the covariance estimate when the number of iterations equaled fifty ) is shown in Figure 6.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/JzC7kdmP03YnJNhkUBlHyWWmqUOgc9rqY8qQdFskhXn2KPOOfHl8McaSF5Q214HAf-0diteosAn3_WA6SNjUQX_JD4Is4TEo-MLRJi6DjWuGLfiHYYJZ417279iDg6-puN7W-ZY9-A=w402-h67-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/JzC7kdmP03YnJNhkUBlHyWWmqUOgc9rqY8qQdFskhXn2KPOOfHl8McaSF5Q214HAf-0diteosAn3_WA6SNjUQX_JD4Is4TEo-MLRJi6DjWuGLfiHYYJZ417279iDg6-puN7W-ZY9-A=w402-h67-no&quot; border=&quot;0&quot; alt=&quot; photo CodeCogsEqn 1_zpszaabqtv0.gif&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 6 :: Covariance Comparison.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;final-test&quot;&gt;Final Test&lt;/h3&gt;

&lt;p&gt;For the final test, we increase the fraction of outliers to 30 percent. The scatter of the residuals is shown in Figure 7.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/pAlOV_lAfnluMKP0Dz2KIS5cqYO8l-6lR0pIErRkqrk9_ona65dYfCLisFoJYYDPox9_l-gK3mYl8aHkfjR11Dv4DWtA8PSFHEUk49KXjOMWXSQ3XxcFC4bIghGdgQ2j7GG8wB1sFg=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/pAlOV_lAfnluMKP0Dz2KIS5cqYO8l-6lR0pIErRkqrk9_ona65dYfCLisFoJYYDPox9_l-gK3mYl8aHkfjR11Dv4DWtA8PSFHEUk49KXjOMWXSQ3XxcFC4bIghGdgQ2j7GG8wB1sFg=w1024-h566-no&quot; border=&quot;0&quot; alt=&quot; photo simScatter3_zpsfinbvgo6.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 7 :: Residual Scatter ( Ratio of Inler/Outler = 0.3 ).   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The accuracy of the covariance estimate as the number of iterations provided to the GMM increases is shown in Figure 9. This shows a trend similar to the two previously conducted tests.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/s994_248IunbysgqHn-4Lu3wzEeaoIPzRQikwTdapfp6OjFJ1U1iUWb4KIq555yDy50hpQXm24fH6i0ZCJHKrXMYuhgkG0DDXLCOoMWnAKmVk1rZWdjzw6fNrU9JFhDILUSRsC_Ikg=w1024-h566-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/s994_248IunbysgqHn-4Lu3wzEeaoIPzRQikwTdapfp6OjFJ1U1iUWb4KIq555yDy50hpQXm24fH6i0ZCJHKrXMYuhgkG0DDXLCOoMWnAKmVk1rZWdjzw6fNrU9JFhDILUSRsC_Ikg=w1024-h566-no&quot; border=&quot;0&quot; alt=&quot; photo covEst3_zpslbhx8i9b.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 8 :: Convergence of Covariance Estimate From GMM.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The final covariance estimate for the inlier distribution is shown in Figure 9.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/-HghJkDadBemQSMrwTNZTeBG998S2eE1H0em1eogSbEfhSa4FopholQD_cRjWc78FxqJjTGZUmi6a-lacLNlygB9zViTpF9cFHCLj33UVt-7RucJhXI0--H9Jh9w1qZvr2-IKwXbeQ=w416-h67-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/-HghJkDadBemQSMrwTNZTeBG998S2eE1H0em1eogSbEfhSa4FopholQD_cRjWc78FxqJjTGZUmi6a-lacLNlygB9zViTpF9cFHCLj33UVt-7RucJhXI0--H9Jh9w1qZvr2-IKwXbeQ=w416-h67-no&quot; border=&quot;0&quot; alt=&quot; photo CodeCogsEqn 2_zpskgxg3vxq.gif&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 9 :: Covariance Comparison.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;Next, these mixture models will be provided to the max-mixtures robust noise model to see how well it can optimize when faced with faulty pose graphs&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 19 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/07/19/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/07/19/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Gausian Mixture Model</category>
        
        <category>max-mixtures</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Validation of G.M.M Covariance Estimation Using Manhattan Pose Graph</title>
        <description>&lt;p&gt;The previous two weeks have been spent, primarily, on porting code and tracking down bugs. Now, it looks like the G.M.M is working properly. To validate this, the Manhattan 3500 pose graph — as depicted in Figure 1 — was utilized.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/tVZLogjFEOfMGJk8tvuNYpixcBsL472LZYbhm8IRVGqCO74kD2XEV-hqqr3k4TdRYQ5WQhafKvsfCBcz4wLnAieV-S7ligGmJsYLDCxXwhghq8UtUEZe0ZB-uttdFlvte-8tdCENZA=w608-h542-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/tVZLogjFEOfMGJk8tvuNYpixcBsL472LZYbhm8IRVGqCO74kD2XEV-hqqr3k4TdRYQ5WQhafKvsfCBcz4wLnAieV-S7ligGmJsYLDCxXwhghq8UtUEZe0ZB-uttdFlvte-8tdCENZA=w608-h542-no&quot; border=&quot;0&quot; alt=&quot; photo M3500_eg2o_zpstu9zq4qo.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: Manhattan 3500 Pose-graph.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using the Manhattan 3500 pose-graph, 100 false constraints — as shown in Figure 2 — were added to see how well we can estimate the inlier/outlier distribution.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/_NHc35kLjMa3ZWSb8LHbLvol8KAd7TUYqv81-bWhhGOFspObggZrRLwZtqHSSZ9SUzTWaKkP2gnvuylOA-ZtKNAZ7O3faxofIset9bm9fp-caesdO_WilgO3npZ7tLAwP0Iw1ZkoJg=w977-h665-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/_NHc35kLjMa3ZWSb8LHbLvol8KAd7TUYqv81-bWhhGOFspObggZrRLwZtqHSSZ9SUzTWaKkP2gnvuylOA-ZtKNAZ7O3faxofIset9bm9fp-caesdO_WilgO3npZ7tLAwP0Iw1ZkoJg=w977-h665-no&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 2 :: Manhattan 3500 Pose-graph with 100 False Constraints.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using the graph represented in Figure 2, an initial optimization utilizing $L_2$ was conducted to extract the residuals. The residual scatter is shown in Figure 3. From this figure, a clear inlier cluster is shown in the center with the scattered residuals representing the false constraints.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/mRWRAjYrBjb-L2Mz8U81VESEnVJrUF1aXlOPyhB5PHlbqb3OjZJFbYBUydWkdjvx-VIEZPC7c-EVZeXvOCUIK_elVk83ZfKnsApC3Nv3JzCzP1tze7VjyJ1o25HWe_9ntOeQ9bjjHQ=w630-h332-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/mRWRAjYrBjb-L2Mz8U81VESEnVJrUF1aXlOPyhB5PHlbqb3OjZJFbYBUydWkdjvx-VIEZPC7c-EVZeXvOCUIK_elVk83ZfKnsApC3Nv3JzCzP1tze7VjyJ1o25HWe_9ntOeQ9bjjHQ=w630-h332-no&quot; border=&quot;0&quot; alt=&quot; photo residualScatter_zpsszsigx4x.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 3 :: Residual Scatter.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;With this set of residuals, we can utilize the G.M.M to estimate the inlier/outlier clusters. When Depicted in Figure 4 is the estimated inlier cluster. Through visual inspection, this looks like a fair estimate.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/o7Lu8J8021rsWXCfyBRub825QQ8quQNbh1I6cy14FtLUCp8qQtAu9wdsIPQXgv-UwBEcO8OPfiB-mJggBR-SqgbIHQTiaf0qmH-9boKPbPe5-KFbycIdjfJtfci7ahErgI7k1p0UTw=w630-h332-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/o7Lu8J8021rsWXCfyBRub825QQ8quQNbh1I6cy14FtLUCp8qQtAu9wdsIPQXgv-UwBEcO8OPfiB-mJggBR-SqgbIHQTiaf0qmH-9boKPbPe5-KFbycIdjfJtfci7ahErgI7k1p0UTw=w630-h332-no&quot; border=&quot;0&quot; alt=&quot; photo inlierCov_zpsc8e16igm.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 4 :: Inlier Covariance Estimate.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The next two figures shown the outlier covariance estimates provided by the G.M.M. Again, through visual inspection, these look like decent estimates of the residual covariance.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/bQNlojrVx85kI7WJ3xEhZtSmmhHI8j2ci_FAGygLsuHEGva4FNGs0eVFxZpwYZSjy1NERqkIAzB_bgY9v8a9sLd5X7h6PXnKZByPbiknK6oZVLvYuvVMBHO-FUPHx2T6JrkI2hURpg=w630-h332-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/bQNlojrVx85kI7WJ3xEhZtSmmhHI8j2ci_FAGygLsuHEGva4FNGs0eVFxZpwYZSjy1NERqkIAzB_bgY9v8a9sLd5X7h6PXnKZByPbiknK6oZVLvYuvVMBHO-FUPHx2T6JrkI2hURpg=w630-h332-no&quot; border=&quot;0&quot; alt=&quot; photo oulierCov_zpsjgzlpuc8.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 5 :: First Outlier Covariance.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/MGb3Oqq1pMNmRM0aHyGKv7py3DcAhIcMh6keG7ykZDZl6pRn9-ikXlXcq2NgLeJ3fqiL0hItqcZ2ctNbBJeeVXXZwlee8aLmxJx1Bc8OdsXsFp3OQS-He3h4cYWrRORlFCavekQcHw=w630-h349-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/MGb3Oqq1pMNmRM0aHyGKv7py3DcAhIcMh6keG7ykZDZl6pRn9-ikXlXcq2NgLeJ3fqiL0hItqcZ2ctNbBJeeVXXZwlee8aLmxJx1Bc8OdsXsFp3OQS-He3h4cYWrRORlFCavekQcHw=w630-h349-no&quot; border=&quot;0&quot; alt=&quot; photo outlier2_zpsexh9wgvg.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 6 :: Second Outlier Covariance.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;Next, I’m going to setup a simulation that will generate residual distributions. I’ll use the simulated distributions to validate that the estimate inlier covariance approaches the true distribution as the number of G.M.M. iterations increases.&lt;/p&gt;

</description>
        <pubDate>Tue, 18 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/07/18/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/07/18/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Gausian Mixture Model</category>
        
        <category>max-mixtures</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Innovation Adaptive Estimation Based on Inverse Wishart Distribution</title>
        <description>&lt;p&gt;For most INS integration applications the extended Kalman filter is the algorithm of choice. For the general application, a schematic overview of the algorithm is provided in Figure 1. From this figure it can be seen that the Kalman filter has two main steps: prediction and measurement update. In addition to the two main steps, the initialization of the Q, R matrices is incredible important for system convergence.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For most applications, the Kalman filter’s system noise covariance matrix, Q , and measurement noise covariance matrix, R , are determined during the development phase by laboratory measurements of the system, simulation and trials. However, there are some cases where this cannot be done. For example, if an INS/GNSS integration algorithm or INS calibration algorithm is designed for use with a range of different inertial sensors, the system noise covariance will not be known in advance of operation. Similarly, if a transfer alignment algorithm (Section 15.1) is designed for use on different aircraft and weapon stores without prior knowledge of the flexure and vibration environment, the measurement noise covariance will not be known in advance.   ** REF ::  Paul D. Groves Principles of GNSS, Inertial, and Multisensor Integrated Navigation Systems&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/z6YE6NmFB6MXf211uKJxf9fHApWcMURJh9XWLbu50viiXKc-0I6E0sFevPNxk4wYOhMoMsughAhn5171JbDf7dcSrZNGYhF1BEYS4F2iFCvmuZCqXHT8M3zRBPIp8gV93WL5Ml7aYg=w276-h489-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/z6YE6NmFB6MXf211uKJxf9fHApWcMURJh9XWLbu50viiXKc-0I6E0sFevPNxk4wYOhMoMsughAhn5171JbDf7dcSrZNGYhF1BEYS4F2iFCvmuZCqXHT8M3zRBPIp8gV93WL5Ml7aYg=w276-h489-no&quot; border=&quot;0&quot; alt=&quot; photo kfSchematic_zpsnbdnjnhw.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Figure 1 :: General Schematic of Extended Kalman Filter.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The traditional method to adaptively tune the Q and R matrices is known as Innovation Adaptive Estimation. The first step in this method is to calculate the measurement innovation covariance for the last n measurements, as show below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C_K = \frac{1}{n} \sum_{j=k-n}^k \delta z_j \delta z_j^T&lt;/script&gt;

&lt;p&gt;With this covariance estimate, the Q and R matricies can be updated.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_k = K_k C_k K_K^T&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_k = C_k - H_k P_K^- H_K^T&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;extension-to-traditional-iae&quot;&gt;Extension to Traditional IAE&lt;/h2&gt;

&lt;p&gt;Today, Clark proposed the idea of utilizing a similar approach to the one were implementing for robust pose-graph optimization to adaptively update the Q, R matrices? This approach will utilize the Gaussian Inverse Wishart distribution to estimate the innovation covariance matrix. The Gaussian Inverse Wishart distribution is defined below,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;GIW(\mu,\Sigma | m_o, \kappa_o, \nu_o, S_o) := \mathcal{N}(\mu | m_o, \frac{1}{\kappa_o}\Sigma) IW(\Sigma| S_o,\nu_o) ,&lt;/script&gt;

&lt;p&gt;which can be represented as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;GIW(\mu,\Sigma | m_o, \kappa_o, \nu_o, S_o) =  e^{-\frac{\kappa_o}{2}|| \mu - m_o||_{\Sigma}} e^{-\frac{1}{2} tr(\Sigma^{-1}S_o)} \frac{1}{Z_{GIW}(D,\kappa_o,\nu_o,S_o} |\Sigma|^{-\frac{\nu_o+D+2}{2}},&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_{GIW}(D,\kappa_o,\nu_o,S_o) = 2^{\frac{(\nu_o+1)D}{2}}\pi^{\frac{D(D+1)}{4}}\kappa_o^{-D/2} \|S_o\|^{-\nu_o/2} \prod_{d=1}^{D} \Gamma(\frac{\nu_o+1-d}{2})&lt;/script&gt;

&lt;h4 id=&quot;parameter-definition&quot;&gt;Parameter Definition&lt;/h4&gt;

&lt;p&gt;$m_o$ –&amp;gt; Prior mean for $\mu$&lt;/p&gt;

&lt;p&gt;$\kappa_o$ –&amp;gt; belief in $m_o$&lt;/p&gt;

&lt;p&gt;$S_o$ –&amp;gt; prior $\Sigma$&lt;/p&gt;

&lt;p&gt;$\nu_o$ –&amp;gt; belief in $S_o$&lt;/p&gt;

</description>
        <pubDate>Fri, 30 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/06/30/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/06/30/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>IAE</category>
        
        <category>Inverse Wishart</category>
        
        <category>Idea</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title>Comparison of Max-Mix Bayesian Mixture Model to M-Estimators</title>
        <description>&lt;h1 id=&quot;robust-noise-model-comparison&quot;&gt;Robust Noise Model Comparison&lt;/h1&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now that we have the BMM-DP working, we can begin testing it against other robust noise models on commonly used datasets. To start, we will test the BMM-DP  against six commonly used M-estimators. All comparisons shown below will have 100 false constraints added to each pose-graph.
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;manhattan-3500&quot;&gt;Manhattan 3500&lt;/h2&gt;

&lt;p&gt;To start the comparison we will utilize the commonly used Manhattan 3500 pose-graph. First, we optimized the pose-graph with $L_2$ optimization and the BMM-DP. The results are provided in Table 1. From this table it can be seen that the BMM-BP outperformed $L_2$ optimization with respect to RSS positioning error; however, that is not a surprise, so next we will compare the BMM-DP results to the results obtained by several M-estimators.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/WNKQyF9D3uFquId0Zedh8bsUYNT7mNJLh2vvRLAThbApSOBD9oukNBRwoZ6T3RImmleQN759xOVIRdjYQmWeSdjegK4ZCzjoZVH-0dxk-VVXijvluu6lxJsgQ5axApCoaRBR1KWHrw=w395-h78-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/WNKQyF9D3uFquId0Zedh8bsUYNT7mNJLh2vvRLAThbApSOBD9oukNBRwoZ6T3RImmleQN759xOVIRdjYQmWeSdjegK4ZCzjoZVH-0dxk-VVXijvluu6lxJsgQ5axApCoaRBR1KWHrw=w395-h78-no&quot; border=&quot;0&quot; alt=&quot; photo orgStats_zpsmtirkwkk.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Table 1 :: Median of RSS positioning error for Manhattan 3500 pose-graph Using $L_2$ and BMM-DP.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;From previous testing, it is known that all M-estimators are sensitive to the user specified kernel width. Due to this sensitivity, for this comparison, we ran each M-estimator with several kernel widths to find the optimal kernel width for each M-estimator with respect to this dataset. This is depicted in Figure 1, where is can be seen that there is great variability in the RSS positioning error with respect to the specified kernel width.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/hxCNOzoKyhdr46NM-Pk7N2bq-XLQGgfrQACsfOlJrZmcsxxyBFFAwsFvET4xZQ5ASN2ALaAEbmr6mLSGJiGm8IqGTOQWowVfHfQrZLrN4hiJDeEzQng4q6eCpn97_O8_65lXx8e6Nw=w630-h332-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/hxCNOzoKyhdr46NM-Pk7N2bq-XLQGgfrQACsfOlJrZmcsxxyBFFAwsFvET4xZQ5ASN2ALaAEbmr6mLSGJiGm8IqGTOQWowVfHfQrZLrN4hiJDeEzQng4q6eCpn97_O8_65lXx8e6Nw=w630-h332-no&quot; border=&quot;0&quot; alt=&quot; photo man3500_zpscx4shcce.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Median of RSS positioning error for Manhattan 3500 pose-graph.   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The optimal M-Estimator results are depicted in Table 2. From this table is can be seen that the Tukey kernel performed the best out of the M-estimators; however, the positioning error provided by the Tukey kernel is still substantially larger than the positioning error provided by the BMM-BP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/WqGZyw_A7fmTsgovfC6W4tDpcVpYHzqmfAZsX1WAWRFpBvrygxzkLAR7EmX-ylABJY6ODuypnMHLoyhLR_KCuS-jDtNDQMRQa_5eq_hZfvaeCVRokbxRCCAFUFm0Y09Kgsinrxc18Q=w467-h154-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/WqGZyw_A7fmTsgovfC6W4tDpcVpYHzqmfAZsX1WAWRFpBvrygxzkLAR7EmX-ylABJY6ODuypnMHLoyhLR_KCuS-jDtNDQMRQa_5eq_hZfvaeCVRokbxRCCAFUFm0Y09Kgsinrxc18Q=w467-h154-no&quot; border=&quot;0&quot; alt=&quot; photo stats_zpsfdirssj7.png&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Table 2 :: Median of RSS positioning error for Manhattan 3500 pose-graph using M-Estimators.   
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Now that we have this code setup we can do a similar comparison for other commonly used pose-graphs. Additionally, we need to test the BMM-DP against switchable constraints.&lt;/p&gt;

</description>
        <pubDate>Wed, 28 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/06/28/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/06/28/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>M-Estimator</category>
        
        <category>Switchable Constraints</category>
        
        <category>Mixture Model</category>
        
        <category>Dirichlet Process</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
      <item>
        <title> Continued Testing On Manhattan 3500</title>
        <description>&lt;h1 id=&quot;initial-testing&quot;&gt;Initial Testing&lt;/h1&gt;

&lt;p&gt;As mentioned in the previous update, the manhattan3500 data-set is being utilized to test the BMM DP– this pose-graph is depicted in Figure 1. Initially, we process the fault-free pose-graph and the pose-graph that has 100 false constraints added with traditional $L_2$ optimization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.lucacarlone.com/images/M3500_eg2o.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 1 :: Manhattan 3500 pose-graph
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The residual scatter plots for the fault-free graph and the graph with 100 false constraints using $L_2$ optimization are shown in  Figures 2 and 3, respectively. This plots depict what would be expected; both distributions have approximately zero mean with the  faulty graph residuals have a larger variance.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/BNZrx-XybVTee_hnOFM8fuZOKYtJrNFDffgYlUks3WQ9k5flqMijAxzjTMJTNAn-jFlZTo4AQ299j9DkmGzZDjeYe7yR7UwGU51_0o080rQIuE5845abbjAfuv3lRuzfW00JQFJaKw=w630-h156-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/BNZrx-XybVTee_hnOFM8fuZOKYtJrNFDffgYlUks3WQ9k5flqMijAxzjTMJTNAn-jFlZTo4AQ299j9DkmGzZDjeYe7yR7UwGU51_0o080rQIuE5845abbjAfuv3lRuzfW00JQFJaKw=w630-h156-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-kq4xebQrYv_zpsoavucrk8.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 2 :: Fault free residual scatter   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://lh3.googleusercontent.com/aH8ONLnws-m4PTRFJA734FyUS8XbDXObWCB7c_FXlmi91AwdDSQMbyMS8SV-C2V5m3f5XOv5LKPsqFSVXKEQV8pNKVLC4xy-nzLcv_hhsBSFpwUi6hMC-9Gh2PsPhwcxLL-B6dajXA=w630-h156-no&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/aH8ONLnws-m4PTRFJA734FyUS8XbDXObWCB7c_FXlmi91AwdDSQMbyMS8SV-C2V5m3f5XOv5LKPsqFSVXKEQV8pNKVLC4xy-nzLcv_hhsBSFpwUi6hMC-9Gh2PsPhwcxLL-B6dajXA=w630-h156-no&quot; border=&quot;0&quot; alt=&quot; photo imgonline-com-ua-twotoone-QXmCmNdSzLvkhWK_zpsavcmhecx.jpg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig 3 :: Pose graph residual scatter when 100 faulty constraints are added   
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using the faulty residuals, the BMM DP was tested. The BMM DP provided an inlier/outlier distribution mixture as shown below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_I = [ 0.04946242, -0.05057746, -0.0594329 ]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P_I =
  \begin{bmatrix}
    16.89882198 &amp;&amp; -0.90303439 &amp;&amp;  0.08602105 \\
   -0.90303439 &amp;&amp; 17.08403007  &amp;&amp; 0.06889149 \\
   0.08602105 &amp;&amp;  0.06889149 &amp;&amp; 10.95290421
  \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_O = [ 0.056539, -0.35273277, -0.83455116 ]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P_O =
  \begin{bmatrix}
     2087.43888892 &amp;&amp; -651.08477092 &amp;&amp;  198.82917436 \\
   -651.08477092 &amp;&amp; 1977.03231206  &amp;&amp; -92.72713996 \\
   198.82917436 &amp;&amp;  -92.72713996 &amp;&amp; 1264.0179354
  \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now, we can test the estimated mixture by feeding it into the Max-Mixture model to see how well it optimizes the pose-graph in comparison to $L_2$.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table class=&quot;mbtablestyle&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Optimization Stragety&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Total Graph Error – $\mathcal{X}^2$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$L_2$ with no faults&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;73.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$L_2$ with faults&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.964e+07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Max-Mix Using BMM-BP with Faults&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;65.33&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
From this table, we can see that the BMM-BP estimate mixture preforms very well in comparison to $L_2$.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Testing, testing, and more testing .. .&lt;/p&gt;
</description>
        <pubDate>Mon, 26 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/researchNotes/_projects/summerafit/2017/06/26/Update.html</link>
        <guid isPermaLink="true">http://localhost:4000/researchNotes/_projects/summerafit/2017/06/26/Update.html</guid>
        
        <category>summerAFIT</category>
        
        <category>Density Estimation</category>
        
        <category>Mixture Model</category>
        
        <category>Dirichlet Process</category>
        
        
        <category>_projects</category>
        
        <category>summerAFIT</category>
        
      </item>
    
  </channel>
</rss>
