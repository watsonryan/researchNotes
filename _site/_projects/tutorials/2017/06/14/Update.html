<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Dirichlet Process Mixture Model</title>
    <meta name="description" content="">

    <link rel="stylesheet" href="https://watsonryan.github.io/researchNotes/css/main.css">
    <link rel="canonical" href="https://watsonryan.github.io/researchNotes/_projects/tutorials/2017/06/14/Update.html">

<!-- meta robots -->
    

<!-- loading fonts -->
    <!-- Open Sans -->
    <link href="https://watsonryan.github.io/researchNotes/_assets/open_sans/style.css" rel="stylesheet">

    <!-- own icon font, created via https://icomoon.io/  -->
    <link href="https://watsonryan.github.io/researchNotes/_assets/lablogIcons/style.css" rel="stylesheet">

    <!-- Ubuntu Mono for code -->
    <link href="https://watsonryan.github.io/researchNotes/_assets/Ubuntu_Mono/style.css" rel='stylesheet' type='text/css'>

<!-- mathjax.org for math equations -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    imageFont: null,
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} ,
    menuSettings: { context: "Browser" }
    });
    </script>

    <script type="text/javascript"  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

<!-- highlight.js for code syntax highlighting-->
<link rel="stylesheet" href="https://watsonryan.github.io/researchNotes/_assets/highlight/styles/github.css">
<script src="https://watsonryan.github.io/researchNotes/_assets/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!--<link rel="stylesheet" href="http://yandex.st/highlightjs/7.3/styles/default.min.css">
<script src="http://yandex.st/highlightjs/7.3/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>-->


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/researchNotes/index.html">Research Notes</a>

  <nav  class="main-menu">
  <input type="checkbox" id="menu" />
  <label for="menu"></label>
  <div class="trigger">

    <a class="page-link" href="/researchNotes/index.html"><span style = "font-family: 'lablogIcons';">  &#xe605; </span> Research Notes</a>

    
      <a class="page-link" href="/researchNotes/tasks.html"><span style = "font-family: 'lablogIcons';">  &#xf046; </span> Tasks</a>
    

    
<a class="page-link" href="/researchNotes/tags.html"><span style = "font-family: 'lablogIcons';">  &#xe600; </span> Tags</a>
    

    
      <a class="page-link" href="/researchNotes/archive.html"><span style = "font-family: 'lablogIcons';">  &#xe602; </span> Archive</a>
    

    <br>

    
      <a class="page-link" href="/researchNotes/_projects/dirichletRobustPoseGraph/dirichletRobustPoseGraph.html">
        <span style = "font-family: 'lablogIcons';">  &#xe60a; </span>
        
        
        Pose Graph :: Dirichlet
      </a>
    
      <a class="page-link" href="/researchNotes/_projects/incrementalGraph/incrementalGraph.html">
        <span style = "font-family: 'lablogIcons';">  &#xe60a; </span>
        
        
        Incremental Robust Est.
      </a>
    
      <a class="page-link" href="/researchNotes/_projects/pollinatorBot/pollinatorBot.html">
        <span style = "font-family: 'lablogIcons';">  &#xe60a; </span>
        
        
        Pollinator Bot Updates
      </a>
    
      <a class="page-link" href="/researchNotes/_projects/robustEstDev/robustEstDev.html">
        <span style = "font-family: 'lablogIcons';">  &#xe60a; </span>
        
        
        Robust Est. Derivation
      </a>
    
      <a class="page-link" href="/researchNotes/_projects/robustGNSSGraph/robustGNSSGraph.html">
        <span style = "font-family: 'lablogIcons';">  &#xe60a; </span>
        
        
        Robust GNSS :: F.G.
      </a>
    
      <a class="page-link" href="/researchNotes/_projects/topologyRobustPoseGraph/topologyRobustPoseGraph.html">
        <span style = "font-family: 'lablogIcons';">  &#xe60a; </span>
        
        
        Pose Graph :: Topology
      </a>
    
      <a class="page-link" href="/researchNotes/_projects/tutorials/tutorials.html">
        <span style = "font-family: 'lablogIcons';">  &#xe60a; </span>
        
        
        Tutorials
      </a>
    

    

    <br>

    
    <div class="menu-content">
      

    </div>
    

    

  </div>

  </nav>


  </div>


</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Dirichlet Process Mixture Model</h1>
    <p class="post-meta">Jun 14, 2017 • Ryan Watson</p>

    
    <div>
     
       <a href="/researchNotes/tags.html#tutorials" title="Pages tagged tutorials" rel="tag" class = "tag"> tutorials </a>  &nbsp
     
       <a href="/researchNotes/tags.html#Dirichlet" title="Pages tagged Dirichlet" rel="tag" class = "tag"> Dirichlet </a>  &nbsp
     
       <a href="/researchNotes/tags.html#Density+Estimation" title="Pages tagged Density Estimation" rel="tag" class = "tag"> Density Estimation </a>  &nbsp
     
       <a href="/researchNotes/tags.html#Mixture+Model" title="Pages tagged Mixture Model" rel="tag" class = "tag"> Mixture Model </a>  &nbsp
     
    </div>
    

  </header>

  <article class="post-content">
    <h1 id="dirichlet-process-gaussian-mixture-model">Dirichlet Process Gaussian Mixture Model</h1>

<p>This tutorial is provided to give a brief overivew of the Dirichlet Process Gaussian Mixture Model ( DP GMM ) and prerequisite information.<br />
<br /></p>

<h1 id="prerequisite-information">Prerequisite Information</h1>

<p>Before diving into the DP-GMM, we will first discuss some prerequisite information. We will start by defining the multivariate Gaussian and its likelihood function. Next, a brief overview of the conjugate priors will be provided. Finally, we will discuss two commonly used distributions: the inverse-Wishart and the Dirichlet distribution.</p>

<h2 id="multivariate-gaussian">Multivariate Gaussian</h2>

<p>The multivariate Gaussian is simply a generalization of the univariate Guassian to higher dimensions. To define this slightly more formally, a vector a real-valued random variables, $\mathcal{X} = [ X_1, X_2 \ldots, X_n]$, has a Gaussian distribution with mean $\mu \in \mathcal{R}^n$ and covariance $\Sigma \in P^n_{++}$ — $P^n_{++}$ is a manifold composed of all symmetric positive definite nxn matricies (i.e., $P^n_{++} \in SO(n))$ — if it’s distribution can be charaterized by</p>

<script type="math/tex; mode=display">p(x | \mu,\Sigma) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp^{(-\frac{1}{2}|| x-\mu||_{\Sigma}^{2} )}.</script>

<h3 id="multivariate-gaussian-likelihood">Multivariate Gaussian Likelihood</h3>

<p>Give a mean $\mu$ and covariance $\Sigma$ we can calcuate the likelihod of a set of random vectors $\mathcal{X} = [X_1, X_2, \ldots, X_n]$ by,</p>

<script type="math/tex; mode=display">p(\mathcal{X} | \mu,\Sigma) = \prod^N_{n=1} \mathcal{N}(X_n|\mu,\Sigma),</script>

<p>which can be represents as</p>

<script type="math/tex; mode=display">p(\mathcal{X} | \mu,\Sigma) = \frac{1}{(2\pi)^{ND/2} (|\Sigma|^{N/2})}e^{-\frac{N}{2}|| \mu - \bar{x}||_{\Sigma}} e^{-\frac{1}{2} tr(\Sigma^{-1}S_{\bar{x}})},</script>

<p>where</p>

<script type="math/tex; mode=display">\bar{x} = \frac{1}{N} \sum^N_{n=1} x_n</script>

<script type="math/tex; mode=display">S_{\bar{x}} = (x_n - \bar{x})(x_n - \bar{x})^{T}.</script>

<h2 id="conjugate-prior">Conjugate Prior</h2>

<p>Conjugate priors are widely used because they simplify computation (i.e., they allow us to analytically integrate out latent variables and only sample parameters of interest through collapsed Gibbs sampling, which will be discussed in greater detail later).</p>

<p><strong>Def:</strong><br />
A family $\mathcal{F}$ of prior distributions, $p(\theta)$, is conjugate to a likelilhood, $p(\theta | \mathcal{D})$, if the posterior, $p(\theta | \mathcal{D})$,  is in $\mathcal{F}$.</p>

<p>With that in mind, we will next define the Gaussian inverse Wishart and the Dirichlet distribution, which are two commonly used conjugate priors for the multivariate Gaussian and the catagorical distributions, respectively.</p>

<h2 id="gaussian-inverse-wishart">Gaussian Inverse Wishart</h2>

<p>For the mean $\mu$ and covariance matrix $\Sigma$ of a multivariate Gaussian, the Gaussian-inverse-Wishart<br />
(GIW) prior is fully conjugate,</p>

<script type="math/tex; mode=display">GIW(\mu,\Sigma | m_o, \kappa_o, \nu_o, S_o) := \mathcal{N}(\mu | m_o, \frac{1}{\kappa_o}\Sigma) IW(\Sigma| S_o,\nu_o) ,</script>

<p>which can be represented as,</p>

<script type="math/tex; mode=display">GIW(\mu,\Sigma | m_o, \kappa_o, \nu_o, S_o) =  e^{-\frac{\kappa_o}{2}|| \mu - m_o||_{\Sigma}} e^{-\frac{1}{2} tr(\Sigma^{-1}S_o)} \frac{1}{Z_{GIW}(D,\kappa_o,\nu_o,S_o} |\Sigma|^{-\frac{\nu_o+D+2}{2}},</script>

<p>where</p>

<script type="math/tex; mode=display">Z_{GIW}(D,\kappa_o,\nu_o,S_o) = 2^{\frac{(\nu_o+1)D}{2}}\pi^{\frac{D(D+1)}{4}}\kappa_o^{-D/2} \|S_o\|^{-\nu_o/2} \prod_{d=1}^{D} \Gamma(\frac{\nu_o+1-d}{2})</script>

<h4 id="parameter-definition">Parameter Definition</h4>

<p>$m_o$ –&gt; Prior mean for $\mu$</p>

<p>$\kappa_o$ –&gt; belief in $m_o$</p>

<p>$S_o$ –&gt; prior $\Sigma$</p>

<p>$\nu_o$ –&gt; belief in $S_o$</p>

<h2 id="dirichlet-distribution">Dirichlet Distribution</h2>

<p>To define a conjugate prior for the multinomial distribution, the Dirichlet distribution is commonly utilized. The Dirichlet distribution is a distribution over possible parameter vectors for a multinomial distribution (e.g., the Beta distribution is a special case of the Dirichlet distribution when n = 2). The Dirichlet distribution defined as</p>

<script type="math/tex; mode=display">\theta \sim \text{Dir}(\alpha) \quad \text{if} \quad p(\theta | \alpha) = \frac{\Gamma(\sum_i^n(\alpha_i))}{\prod_i^n(\Gamma(\alpha_i))}\prod_{i=1}^{n}\theta_i^{\alpha_i-1} \mathcal{I}(\theta \in S),</script>

<p>where $\theta = (\theta_1, \ldots, \theta_n)$, $\alpha = (\alpha_1, \ldots, \alpha_n)$ s.t. $\alpha_i &gt; 0$, and $S$ is the probability simplex. A simplex is simply a generalization of the triangle to n-dimensional space (i.e., $S = \lbrace \alpha \in \mathcal{R}^n : \alpha_i \geq 0 : \sum_i^n \alpha_i = 1 \rbrace$ ).</p>

<p>With that brief description of the Dirichlet distribution, it is useful to visualize an example case. To do this, a simple python script that plots the Dirichlet density when n = 3 (i.e., a situation when three clusters are present in dataset) has been provided. The figure below shows the density when $\alpha = $ , [1,1,1], [5,5,5], and [2,2,8]. As can be seen, when $\alpha = $ [1,1,1] there is a uniform likelihood for each cluster, (i.e., there is no prior knowledge); however, as the $\alpha$’s very, the density can be seen to shift around the simplex.</p>

<p align="center">
<a href="https://lh3.googleusercontent.com/mTtXL49uTh_epEHCi1fKZa8eT3KBO_RAELbBkNSg-Os6RROyLqfMUZUGnXpBO54Cqo4KAADkLF2KLc8WvvrWHWwOPnNU8Vq05_kyyg6nTllcMa9p41Nw5TVeX0N8mnrYicYbPUL2xg=w630-h181-no" target="_blank"><img src="https://lh3.googleusercontent.com/mTtXL49uTh_epEHCi1fKZa8eT3KBO_RAELbBkNSg-Os6RROyLqfMUZUGnXpBO54Cqo4KAADkLF2KLc8WvvrWHWwOPnNU8Vq05_kyyg6nTllcMa9p41Nw5TVeX0N8mnrYicYbPUL2xg=w630-h181-no" border="0" alt=" photo dirDensity_zpsozji7b0u.png" /></a>
</p>
<p align="center">
Fig 1 :: Dirichlet density  
</p>
<p><br /></p>

<h1 id="mixture-models">Mixture Models</h1>

<p>With the overview provided above, we can now move to mixture models for data clustering. This section will start with a finite mixture model and move to an infinite mixture model to overcome the inherent difficultly of selecting the number of data partitions.</p>

<h2 id="finite-mixture-model">Finite Mixture Model</h2>

<p>The model that will be utilized for the finite Bayesian mixture model is shown in the figure below. For each observed data vector $x_i$ , we have a latent variable $z_i \in [1, 2, . . . , K]$ indicating which of the K components $x_i$ belongs to. $ \pi_i = P(z = k)$ is the prior probability that $x_i$ belongs to component $k_i$. Given $z_i = k$, $x_i$ is generated by the $k^{th}$ Gaussian mixture component with mean vector $\mu_k$ and covariance matrix $\Sigma_k$.</p>

<p align="center">
<a href="https://lh3.googleusercontent.com/9TFTxhxtKe87M01N6Oa1qNc_Q0jNrBwwZt5J_a5bBwHJibZsKWWTe6-Ad_2BpqOXzc_LwgHWy6xA-inQYBsa-Zjr7Wa0yXxevclhi8xk0SRWgevhCn2pDEJ14Eq6G0D5U3jXPmJSsA=w136-h235-no" target="_blank"><img src="https://lh3.googleusercontent.com/9TFTxhxtKe87M01N6Oa1qNc_Q0jNrBwwZt5J_a5bBwHJibZsKWWTe6-Ad_2BpqOXzc_LwgHWy6xA-inQYBsa-Zjr7Wa0yXxevclhi8xk0SRWgevhCn2pDEJ14Eq6G0D5U3jXPmJSsA=w136-h235-no" border="0" alt=" photo finiteMixture_zps8bj7ff6i.png" /></a>
</p>
<p align="center">
Fig 2 :: Finite Mixture Model  
</p>
<p><br /></p>

<p>We can provide a prior for our mixing weights, $\pi$ , using the Dirichlet distribution (i.e., $p(\pi | \alpha) = \text{Dir}(\pi | \alpha)$). We can also provide a prior for our mixture components using using the inverse Gaussian Wishart distribution  (i.e., $p(\mu_k, \Sigma_k | m_o, \kappa_o, \nu_o, S_o) = GIW( \mu_k, \Sigma_k | m_o, \kappa_o, \nu_o, S_o) )$.</p>

<h4 id="collapsed-gibbs-sampling">Collapsed Gibbs Sampling</h4>

<p>Because we selected $\pi | \alpha$ and $p(\mu_k,\Sigma_k | \beta)$ — where we define the hyper-parameter $\beta = (m_o,\kappa_o,\nu_o,S_o)$ — to be conjugate, we can analytically integrate out the parameters $\pi$, $\mu_k$, and $\Sigma_k$. This allows us to dramatically reduce our sampling space to component assignmetns $z$. This is represented by,</p>

<script type="math/tex; mode=display">P(z_i = k | z_{\ i}, \mathcal{X}, \alpha, \beta) \propto p(z_i = k | z_{\ i} \alpha) p (x_i | \mathcal{X}_{\ i}, z_i = k, z_{\ i}, \beta) ,</script>

<p>where</p>

<script type="math/tex; mode=display">p(z_i = k| z_{\ i}, \alpha) = \frac{ (N_k - 1) + \frac{\alpha}{K} }{N + \alpha -1}</script>

<p>and</p>

<script type="math/tex; mode=display">p (x_i | \mathcal{X}_{\ i}, z_i = k, z_{\ i}, \beta) = \int_{\mu_k} \int_{\Sigma_k} p(\mathcal{X}_k | \mu_k, \Sigma_K) p(\mu_k, \Sigma_k | \beta) d\mu_k d\Sigma_k .</script>

<p>Psudo code for collapsed Gibbs sampling for a finite GMM is given below.</p>

<p align="center">
<a href="https://lh3.googleusercontent.com/dJcyfLTXmQZn0RxJuTQEHv2IYacyGHC-HLWVb8n4UdZ8pMjvr8w8pxUhaIUlomjqWdJ8pHRNmUu_ey-73ZbTQ4EbBGSEGPbeD-bpnlcd7HWsUAY_yeLiaZC7mBZQxVt0Ly093axaZw=w630-h278-no" target="_blank"><img src="https://lh3.googleusercontent.com/dJcyfLTXmQZn0RxJuTQEHv2IYacyGHC-HLWVb8n4UdZ8pMjvr8w8pxUhaIUlomjqWdJ8pHRNmUu_ey-73ZbTQ4EbBGSEGPbeD-bpnlcd7HWsUAY_yeLiaZC7mBZQxVt0Ly093axaZw=w630-h278-no" border="0" alt=" photo algo1_zps40azobjo.png" /></a>
</p>
<p align="center">
Fig 3 :: Finite Mixture Algorithm  
</p>
<p><br /></p>

<h2 id="infinite-mixture-model">Infinite Mixture Model</h2>

<h4 id="chinese-restaurant-process">Chinese Restaurant Process</h4>

<p>Before we dive into the details of the infinite mixture model, we will first describe the Dirichlet process through the Chinese restaurant problem. This construct is a commonly used one is statistics. It is described as customers seating themselves in a resturant with an infinite number of tables.  This process has a <em>rich-get-richer</em> property, that is, tables with more people have a higher probabily of getting more people. The <em>rich-get-richer</em> property can be more formally defined as,</p>

<script type="math/tex; mode=display">p(z_i = k | z_{\ i} , \alpha ) =\left\{
                \begin{array}{ll}
                  \frac{N_k}{N+\alpha-1} \quad \text{if k is occupied} \\
                  \frac{\alpha}{N+\alpha-1} \quad \text{if k is a new table}
                \end{array}
              \right.</script>

<p>Now, we will move to the infinite mixture model. This model is closely related to the finite mixture model, with the exception being that the Dirichlet process is now used to define the mixing weight priors. This allows us to circimvent the issue of defining the number of partitions by allowing the model to select from an infinite number of partitions.</p>

<p align="center">
<a href="https://lh3.googleusercontent.com/eswxi4_Pb2f76WtmCb63z8NWjvAoaonYQkx9OzKmiyNzDtmMIa72mSCkQczSR71RsHNh6r4YrPJAmjGS8V3DKP0p7fhWcEc0MCxUTwkL0THapO4oh7FcEE7R-u1MgfCw2aVbpQCHfw=w132-h232-no" target="_blank"><img src="https://lh3.googleusercontent.com/eswxi4_Pb2f76WtmCb63z8NWjvAoaonYQkx9OzKmiyNzDtmMIa72mSCkQczSR71RsHNh6r4YrPJAmjGS8V3DKP0p7fhWcEc0MCxUTwkL0THapO4oh7FcEE7R-u1MgfCw2aVbpQCHfw=w132-h232-no" border="0" alt=" photo inifiteMixture_zpsybjiovvy.png" /></a>
</p>
<p align="center">
Fig 4 :: Infinite Mixture Model  
</p>
<p><br /></p>

<h4 id="collapsed-gibbs-sampling-1">Collapsed Gibbs Sampling</h4>

<p>Again, because we selected $p(\pi | \alpha)$ and $p(\mu_k,\Sigma_k | \beta)$ — where we define the hyper-parameter $\beta = (m_o,\kappa_o,\nu_o,S_o)$ — to be conjugate, we can analytically integrate out the parameters $\pi$, $\mu_k$, and $\Sigma_k$. This is represented by,</p>

<script type="math/tex; mode=display">P(z_i = k | z_{\ i}, \mathcal{X}, \alpha, \beta) \propto p(z_i = k | z_{\ i} \alpha) p (x_i | \mathcal{X}_{\ i}, z_i = k, z_{\ i}, \beta) ,</script>

<p>where,</p>

<script type="math/tex; mode=display">p(z | \alpha) = \frac{\alpha^K \prod_{k=1}^K (N_k-1)!}{\prod_{n=1}{N} ( i-1+\alpha}),</script>

<p>and</p>

<script type="math/tex; mode=display">p(x_i | \mathcal{X_{\ i}}, z_i = k, z_{\ i}, \beta) = p(x_i,\beta) = \int_{\mu}\int_{\Sigma} p(x_i | \mu, \Sigma)p(\mu,\Sigma | \beta)d\mu d\Sigma.</script>

<p>Psudo code for collapsed Gibbs sampling for a infinite GMM is given below.</p>

<p align="center">
<a href="https://lh3.googleusercontent.com/b3N9-r2dcZzGx0XCCzVhnpoKoB7MGFl-a94aVTP3Gybdu_g_ziBQC2DrNN8ucNAY6WkcGreDMLS18gOr5BFH8nTpYfkYXwzHBWna_3YUUNZitH9A7Mg9zhX7a851LpSKeYgOa56J2Q=w630-h350-no" target="_blank"><img src="https://lh3.googleusercontent.com/b3N9-r2dcZzGx0XCCzVhnpoKoB7MGFl-a94aVTP3Gybdu_g_ziBQC2DrNN8ucNAY6WkcGreDMLS18gOr5BFH8nTpYfkYXwzHBWna_3YUUNZitH9A7Mg9zhX7a851LpSKeYgOa56J2Q=w630-h350-no" border="0" alt=" photo inifModel_zpszslpm6g2.png" /></a>
</p>
<p align="center">
Fig 5 :: Infinite Mixture Algorithm  
</p>
<p><br /></p>

  </article>

</div>

<div class="related-posts">
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
</div>


<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-watsonryan-github-io-researchnotes.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



<div class="wrapper">


<div class = "turn" style = " right: 0px" >
 <a rel = "next" href="/researchNotes/_projects/dirichletrobustposegraph/2017/06/15/Update.html"> <span> &rsaquo; </span></a>
</div>





<div class = "turn" style = " left: 0px" >
 <a rel = "prev" href="/researchNotes/_projects/pollinatorbot/2017/06/14/Update.html"> <span> &lsaquo; </span></a>
</div>



</div>


      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Research Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Research Notes</li>
          <li><a href="mailto:rwatso12@gmail.com">rwatso12@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/watsonryan">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="$grey-color-light" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">watsonryan</span>
            </a>
          </li>
          

          


<li>
  <a href="/researchNotes/feed.xml">
    <span class="icon icon-feed">
      <svg viewBox="0 0 16 16">
      <circle cx="3" cy="13" r="2" fill="$grey-color-light" />
        <path fill="$grey-color-light" stroke="#000000" stroke-linecap="round" stroke-linejoin="round" d="M8.5,14.5"/>
        <path fill="$grey-color-light"  d="M8.708,15c-0.552,0-1-0.447-1-1c0-3.147-2.561-5.708-5.708-5.708c-0.552,0-1-0.448-1-1s0.448-1,1-1
          c4.25,0,7.708,3.458,7.708,7.708C9.708,14.553,9.261,15,8.708,15L8.708,15z"/>
        <path fill="$grey-color-light"  d="M13.469,14.999c-0.553,0-1-0.447-1-1C12.469,8.227,7.772,3.531,2,3.531c-0.552,0-1-0.448-1-1s0.448-1,1-1
          c6.875,0,12.469,5.593,12.469,12.468C14.469,14.552,14.021,14.999,13.469,14.999L13.469,14.999z"/>
      </svg>
    </span>
    <span class="feed"> RSS feed </span>
  </a>
</li>



        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text"></p>

      <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0;" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>

      <p class="text"><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Research Notes</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://watsonryan.github.io" property="cc:attributionName" rel="cc:attributionURL">Ryan Watson</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p>

      </div>



    </div>

  </div>

</footer>


  </body>

</html>
