<p><br /><br /></p>

<p>The purpose of this page is to provide a very brief intorduction to variation inference. This is very closely modeled after a <a href="https://www.youtube.com/watch?v=4toWtb7PRH4&amp;list=PLdk2fd27CQzSd1sQ3kBYL4vtv6GjXvPsE">series of 5 YouTube videos</a> on this subject.</p>

<p><br /></p>

<h2 id="what-we-will-need-from-information-theory">What we will need from information theory</h2>

<p>This section provides some commonly used equations that will be encontered when studying variational inference.</p>

<p>First, the information associated with an event $x$ is</p>

<script type="math/tex; mode=display">\mathcal{I} = -\text{log} \ P(x).</script>

<p>Now, if we have several events, we can calculate the weighted average of those events as,</p>

<script type="math/tex; mode=display">\mathcal{H} = - \sum_N \ P(x) \text{log} P(X) \ = \sum_N P(x) \mathcal{I}(x),</script>

<p>which is commonly refered to as the entropy of the system.</p>

<p>Finally, when we move from the discrete domain to a continous one, we talk about differenital entropy, which is simpy the modification of the above equation to the continous domain,</p>

<script type="math/tex; mode=display">\mathcal{H} =  \int_{\mathcal{D}} P(x) \mathcal{I}(x) dx</script>

<h2 id="an-introduction-to-kl-divergence">An introduction to KL-Divergence</h2>

<p>KL-Divergence is simply a measure of the distance between two probability distributions. If we have two distributions, then the KL-Diverence between them is written as,</p>

<script type="math/tex; mode=display">KL( P || Q )</script>

<p>where it should be noted that the KL-Divergence is not symmetric</p>

<script type="math/tex; mode=display">KL( P || Q) \neq KL( Q || P ).</script>

<p>One useful way to think about KL-Divergence is as a measure of relative entropy between two distributions,</p>

<script type="math/tex; mode=display">KL( P || Q ) = \sum_N ( P(x) \log( Q(x) ) ) - \mathcal{H}_{P(x)},</script>

<p>which can be manipulated into the form generally presented as,</p>

<script type="math/tex; mode=display">KL( P || Q ) = \sum_N P(x) \text{log} \frac{P(x)}{Q(x)} = - \sum_N P(x) \text{log} \frac{Q(x)}{P(x)}</script>

<h3 id="why-use-kl-divergence">Why use KL-Divergence</h3>

<p>Let’s say that we have an unkown distribution, $p(z|x)$, that we would like to estimate. We can use a new distribution, q(z), to estimate the desired distribution with KL-Divergence being the measure of closness.</p>

<script type="math/tex; mode=display">KL(p(z|x),q(z)) = - \sum_N q(z) \frac{p(z|x)}{q(z)}</script>

<p>Now, we can note that</p>

<script type="math/tex; mode=display">\frac{P(x,z)}{P(z)} = P(z|x),</script>

<p>so</p>

<script type="math/tex; mode=display">KL(p(z|x),q(z)) = -\sum_N q(z) \frac{P(x,z)}{p(x)} \frac{1}{q(z)}.</script>

<p>Now, manipulating the equation, we get</p>

<script type="math/tex; mode=display">KL(p(z|x),q(z)) = -\sum_N q(z) \frac{P(x,z)}{q(z)} \frac{1}{p(x)} = -\sum_N q(z) \big[ \text{log} \frac{P(x,z)}{q(z)} - \text{log} p(x) \big]</script>

<script type="math/tex; mode=display">KL(p(z|x),q(z)) = -\sum_N q(z) \frac{P(x,z)}{q(z)} + \text{log} P(x) \sum_N q(z)</script>

<script type="math/tex; mode=display">KL(p(z|x),q(z)) + \sum_N q(z) \frac{p(x,z)}{q(z)} = \text{log(p(x))}</script>

<p>Now, let’s call the second term on the left hand side of the equation above $\mathcal{L}$ for lower bound. Then, we can write the equation in it’s final form as,</p>

<script type="math/tex; mode=display">KL + \mathcal{L} = C</script>

<p>The equation above gives us the key insight provided by variational inference, (i.e., we can maximize this lower bound and gain the same results as trying to minimize the KL-Divergence when trying to approximate a conditional probability). And this is beneficial because the KL-divergnce contains a conditional probability which can be intractable while the lower bound contains the joint distribution which we can easily calculate.</p>

<blockquote><div style="background-color:#FFFF00; color:#000000; font-style: normal; font-family: Georgia;">
Goal :: <br /><br />
We want to find $q(z)$ such that

$$ \mathcal{L} = \sum_N q(z) \frac{p(x,z)}{q(z)} $$

is maximized.
</div></blockquote>

<p><br /><br /></p>
