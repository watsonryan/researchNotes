<h1 id="initial-testing">Initial Testing</h1>

<p>As one method to robustly optimize when confronted with erroneous data, we are 
testing clustering algorithms to learn the true residual distribution, which 
will — hopefully — allow us to properly de-weight faulty observables. All of 
the code used for this initial testing is housed 
<a href="https://github.com/watsonryan/summerAFIT/tree/master/bayes_gmm">HERE</a>.</p>

<h3 id="data-generation">Data Generation</h3>

<p>To begin testing the Gaussian Mixture Model with Dirichlet Process for outlier 
cluster, a simple 2D data-set was generated. This data-set can be see in Figure 1.</p>

<p align="center">
<a href="https://lh3.googleusercontent.com/n0IFvZrMP-A_nJlNaxaCBcmGvHWyKGqp1zrMpup7Nzs9kaFKouVzfhxSOGNcp9XUwZOeuJgApgCsalw6h2Skfm9UkbiUaCgamzxAvLirMA10fEJVaLz8QMbQkudxu-o9m0DY8DFehw=w630-h355-no" target="_blank"><img src="https://lh3.googleusercontent.com/n0IFvZrMP-A_nJlNaxaCBcmGvHWyKGqp1zrMpup7Nzs9kaFKouVzfhxSOGNcp9XUwZOeuJgApgCsalw6h2Skfm9UkbiUaCgamzxAvLirMA10fEJVaLz8QMbQkudxu-o9m0DY8DFehw=w630-h355-no" border="0" alt="true photo true_zpslveuybzz.png" /></a>
</p>
<p align="center">
Fig 1 :: Generated data-set  
</p>
<p><br /></p>

<h3 id="testing">Testing</h3>

<p>For an initial test, both the inlier and outlier distributions are sampled 
evenly (i.e., 100 data points were selected from each distribution). The clustering 
results are shown below in figure 2. What is interesting to note is that the number 
of clusters was not specified; however, the iterative algorithm correctly classified 
both distributions without adding addition partitions to the data-set.</p>

<p align="center">
<a href="https://lh3.googleusercontent.com/3vTA_STPi7iPA66wi1rjK7eBZzWYbs7Gto2drc_pVm92rJi1y_580LGqlWxWwkdga5e8MKCxuSeW6zFCfU46cLy-XyhS1hD1kkIokCHhKREk4NvNvnbJ_t9c4ltIHwZusB2fmyKAwA=w630-h344-no" target="_blank"><img src="https://lh3.googleusercontent.com/3vTA_STPi7iPA66wi1rjK7eBZzWYbs7Gto2drc_pVm92rJi1y_580LGqlWxWwkdga5e8MKCxuSeW6zFCfU46cLy-XyhS1hD1kkIokCHhKREk4NvNvnbJ_t9c4ltIHwZusB2fmyKAwA=w630-h344-no" border="0" alt=" photo igmmT1_zps3cxyalcv.png" /></a>
</p>
<p align="center">
Fig 2 :: DP GMM Initial Test  
</p>
<p><br /></p>

<p>We can also look at the run-time of the collapsed Gibbs sampler. For the case 
were our data-set is composed of 200 data points, the result is shown below in 
figure 3. This shows that the collapsed Gibbs sampling is fairly consistent, with
respect to time, over all iterations.</p>

<p align="center">
<a href="https://lh3.googleusercontent.com/mlQh9s_bSM6I_y6cEv75Yx_-xut8-FP3rOHDP-b_6Xsl_FqkQcNcKsNZiOnDRvVwv744wveL62qPCewznqmi8eWzZddF7yovcTEHCTrfjpzYXDOUbJLYabN9WkpDgprl5oQLdbOspw=w630-h312-no" target="_blank"><img src="https://lh3.googleusercontent.com/mlQh9s_bSM6I_y6cEv75Yx_-xut8-FP3rOHDP-b_6Xsl_FqkQcNcKsNZiOnDRvVwv744wveL62qPCewznqmi8eWzZddF7yovcTEHCTrfjpzYXDOUbJLYabN9WkpDgprl5oQLdbOspw=w630-h312-no" border="0" alt=" photo itialTime_zpshhv3qg1x.png" /></a>
</p>
<p align="center">
Fig 3 :: Collapsed Gibbs Sampling Initial Time Test  
</p>
<p><br /></p>

<p>However, were not necessarily interesting in the run-time of this implementation 
because it will need to be re-written later. So, something more beneficial to look 
at may be the mean iteration time as the size of the data-set grows. This is depicted 
in Figure 4, where a clear linear trend is shown between the data-set size 
and the iteration time.</p>

<p align="center">
<a href="https://lh3.googleusercontent.com/bNjcwx9Ffl32TyqeOuRKnfgRVn0b9mPIIXiO9LFrWJfvR_hblJrOJFciEZpyNn_QtAX0PraibhKz2Tu41CQFM3cr59ufSif35uDpu4Y0DtTiOjelGXDB0Z-gmLGMGy92D-MozRdyjw=w630-h312-no" target="_blank"><img src="https://lh3.googleusercontent.com/bNjcwx9Ffl32TyqeOuRKnfgRVn0b9mPIIXiO9LFrWJfvR_hblJrOJFciEZpyNn_QtAX0PraibhKz2Tu41CQFM3cr59ufSif35uDpu4Y0DtTiOjelGXDB0Z-gmLGMGy92D-MozRdyjw=w630-h312-no" border="0" alt=" photo timeComp_zpskemvd6qx.png" /></a>
</p>
<p align="center">
Fig 4 :: Mean Iteration Time of Gibbs Sampling  
</p>
<p><br /></p>

