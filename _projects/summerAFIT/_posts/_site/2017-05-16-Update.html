<h1 id="robust-noise-models">Robust Noise Models</h1>

<h2 id="traditional-m-estimators">Traditional M-Estimators</h2>

<p>Traditional M-Estimators have been research for many years. The <a href="http://sanghv.com/download/Soft/Machine%20Learning,%20Artificial%20Intelligence,%20Mathematics%20eBooks/math/statistics/robust%20statistics%20%282nd,%202009%29.pdf">first comprehensive book</a> on the subject was published by Huber in 1981.  The field of research related to M-estimation can be boiled down to reducing the influence of outliers by replacing the $L^2$ cost function with a modified cost function.</p>

<p>So, M-Estimators replace the traditional $L^2$ cost,</p>

<p align="center">
$$ min \sum_i e_i^2 $$ 
</p>

<p>with a modified cost function</p>
<p align="center">
$$ min \sum_i \rho(e_i) $$
</p>

<p>One of the most important qualities of an M-Estimator is that the weight approaches 0 as the residual approachs $\infty$. This function property is call redescending and be defined more formally as</p>

<p align="center">
$$ \lim_{e_i \to \infty} \rho'(e_i) = 0 $$
</p>

<p>A table of m-estimator cost functions is provided below. This table was taken from <a href="http://www2.informatik.uni-freiburg.de/~agarwal/resources/agarwal-thesis.pdf">this thesis</a>.</p>

<p align="center">
<a href="http://s1347.photobucket.com/user/rwatso12/media/m-estimators_zps3wyul65l.png.html" target="_blank"><img src="http://i1347.photobucket.com/albums/p701/rwatso12/m-estimators_zps3wyul65l.png" border="0" alt=" photo m-estimators_zps3wyul65l.png" /></a>
</p>

<p><br /></p>

<h2 id="graph-based-methods">Graph Based Methods</h2>

<h4 id="switch-constraints--sc">Switch Constraints  (S.C.)</h4>

<p>Switchable constraints was introduced by Sunderhauf in 2012 in <a href="https://www.tu-chemnitz.de/etit/proaut/mitarbeiter/rsrc/IROS12-switchableConstraints.pdf">this paper</a></p>

<blockquote>
  <p>The main idea behind the switchable constraints is that the topology of the factor graph that represents the pose graph SLAM problem, should be partially variable and subject to the optimization instead of being kept fixed. This way, edges representing outlier constraints can be removed from the graph during the optimization. This is achieved by augmenting the original problem and introducing an additional type of hidden variable: A switch variable is associated with each factor that could potentially represent an outlier. This additional variable acts as a multiplicative scaling factor on the information matrix associated with that constraint. Depending on the state of the switch variable (a value between 0 and 1), the resulting information matrix is either the original matrix (when the switch is equal to 1) or 0 (when the switch is 0) or something between both ends. Notice that if the switch variable is equal to 0, the associated constraint is completely removed and has no influence on the overall solution</p>
</blockquote>

<h4 id="dynamic-covariance-scaling--dcs">Dynamic Covariance Scaling  (D.C.S.)</h4>

<p>Dynamic Covariance Scaling was introduced in 2013 <a href="http://www2.informatik.uni-freiburg.de/~spinello/agarwalICRA13.pdf"> in this paper </a> as a closed-form solution to switch factors.</p>

<p>The DCS approach reduces the confidence of an observable by de-weighting the corresponding element in the information matrix by the scale factor</p>

<p align="center">
$$ s_{ij} = min(1,\frac{2\Phi}{\Phi + \mathcal{X}}) $$
</p>

<p>Ultimitely this is just the Geman-McClure weighting.</p>

<h4 id="max-mixtures--mm">Max-Mixtures  (M.M)</h4>

<p>Max-Mixtures was introduced in 2012 <a href="https://april.eecs.umich.edu/pdfs/olson2012rss.pdf"> in this paper </a>.</p>

<blockquote>
  <p>The Max-Mixture model consists of a front-end loop-closure and null hypotheses. The front-end loop-closure hypothesis represents the distribution of the inlier loop-closure constraints and the null hypothesis represents the distribution of the outlier loopconstraints. Each loop-closure constraint is verified against the hypotheses iteratively within the optimization loops and the weight associated with the most likely hypothesis is used to scale the Jacobian, residual and information matrix from that loop-closure constraint. In other words, the hypothesis testing acts as an “selector” to the weighting of the loop closure constraint.</p>
</blockquote>

<p><em>REF</em> $\rightarrow$ <a href="https://www.inf.ethz.ch/personal/pomarc/pubs/LeeIROS13b.pdf">this paper</a></p>

<h4 id="realizing-reversing-recovering--rrr">Realizing, Reversing, Recovering  (R.R.R)</h4>

<p>RRR was introduced in 2012 <a href="http://ai2-s2-pdfs.s3.amazonaws.com/1fca/9d6cbcccafbb6b1a2ee30cca5cc955eea1d6.pdf"> in this paper </a>. This was then extended to an incremental estimator in <a href="http://n.ethz.ch/~cesarc/files/IROS2012_latif.pdf"> this paper </a></p>

<blockquote>
  <p>RRR divides
loop closures into clusters based on topological similarity and then tries to find the largest subset of clusters than are consistent among themselves as well as with the underlying odometry. Consistency is considered in the chi-squared ($\mathcal{X}^2)$ sense. The algorithm first carries out consistency checks for each cluster in order to weed out incorrect links within it, followed by an intra-cluster consistency check. RRR is different from the previous algorithms as it explicitly requires convergence of the graph in order to verify the validity of loop closures. In contrast with SC and DCS, in RRR and MM loop closure decisions are not modeled as continuous variables but as discrete yes/no decisions that need to be made</p>
</blockquote>

<p><em>REF</em> $\rightarrow$ <a href="http://n.ethz.ch/~cesarc/files/IROS2014_latif.pdf">this paper</a></p>

<h4 id="comparison">Comparison</h4>

<p>A good paper comparing SC to DCS to RRR is provided by <a href="https://www.tu-chemnitz.de/etit/proaut/forschung/rsrc/ICRA12-comparisonRobustSLAM.pdf">Sunderhauf</a></p>

<p>The robust back-end formulation known as <a href="https://www.tu-chemnitz.de/etit/proaut/mitarbeiter/rsrc/IROS12-switchableConstraints.pdf">Switchable constraints</a> was introduced by  Sunderhauf in 2012.</p>

<p><br /><br /></p>

<h1 id="noise-model-testing">Noise Model Testing</h1>

<h3 id="data-set">Data-set</h3>

<h4 id="clean-data">Clean Data</h4>
<p>The <a href="http://rvsn.csail.mit.edu/graphoptim/eolson-graphoptim2006.pdf">Manhattan world with 3500 nodes</a> is a common graph optimization test data-set. The truth ground trace is shown in the figure below.</p>

<p><img src="http://www.lucacarlone.com/images/M3500_eg2o.jpg" alt="Man3500" /></p>
<p align="center">
True Manhattan wold 3500 ground truth 
</p>

<h4 id="adding-faults">Adding faults</h4>

<p>Gaussian noise with standard deviation 0.3rad is added to the relative orientation measurements.</p>

<h3 id="testing">Testing</h3>

<p>Using the erronous data, each of the noise models above will be tested. For all weighting functions, Levenberg Marquardt is utilized with a kernal width of 10.</p>

<h4 id="l2-cost-function">$L^2$ Cost Function</h4>
<p>First we will test the traditional $L^2$ minimization. A video of the optimizer going through 100 iterations is shown below.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Tb-hwRqCqkU" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h3 id="m-estimators">M-Estimators</h3>

<h4 id="cauchy-cost-function">Cauchy Cost Function</h4>

<p>Next, we will test the Cauchy weighting function. A video of the optimizer going through 100 iterations is shown below.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/OtSKsQpCzlk" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h4 id="geman-mcclure-cost-function">Geman-McClure Cost Function</h4>

<p>Now, a video of the optimization process with the Geman-McClure cost function.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/W7d3hNfppYc" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h4 id="saturaded-cost-function">Saturaded Cost Function</h4>

<p>Applying the saturated cost function to the optimization process</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/w7BsSpoQHQk" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h4 id="tukey-cost-function">Tukey Cost Function</h4>

<p>Applying the tukey cost function to the optimization process</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/f7lK3frlVPk" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h4 id="tukey-cost-function-1">Tukey Cost Function</h4>

<p>The last M-Estimator that will be tested is the Welcsh weighting. A video of the optimization process is shown below.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/lXHhbv2hFpM" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h3 id="robust-graphical-methods">Robust Graphical Methods</h3>

