<h1 id="robust-noise-models">Robust Noise Models</h1>

<h2 id="traditional-m-estimators">Traditional M-Estimators</h2>

<p>Traditional M-Estimators have been research for many years. The <a href="http://sanghv.com/download/Soft/Machine%20Learning,%20Artificial%20Intelligence,%20Mathematics%20eBooks/math/statistics/robust%20statistics%20%282nd,%202009%29.pdf">first comprehensive book</a> on the subject was published by Huber in 1981.  The field of research related to M-estimation can be boiled down to reducing the influence of outliers by replacing the $L^2$ cost function with a modified cost function.</p>

<p>So, M-Estimators replace the traditional $L^2$ cost,</p>

<p align="center">
$$ min \sum_i e_i^2 $$ 
</p>

<p>with a modified cost function</p>
<p align="center">
$$ min \sum_i \rho(e_i) $$
</p>

<p>One of the most important qualities of an M-Estimator is that the weight approaches 0 as the residual approachs $\infty$. This function property is call redescending and be defined more formally as</p>

<p align="center">
$$ \lim_{e_i \to \infty} \rho'(e_i) = 0 $$
</p>

<p>A table of m-estimator cost functions is provided below. This table was taken from <a href="http://www2.informatik.uni-freiburg.de/~agarwal/resources/agarwal-thesis.pdf">this thesis</a>.</p>

<p align="center">
<a href="http://s1347.photobucket.com/user/rwatso12/media/m-estimators_zps3wyul65l.png.html" target="_blank"><img src="http://i1347.photobucket.com/albums/p701/rwatso12/m-estimators_zps3wyul65l.png" border="0" alt=" photo m-estimators_zps3wyul65l.png" /></a>
</p>

<p><br /></p>

<h2 id="graph-based-methods">Graph Based Methods</h2>

<h4 id="switch-constraints--sc">Switch Constraints  (S.C.)</h4>

<p>Switchable constraints was introduced by Sunderhauf in 2012 in <a href="https://www.tu-chemnitz.de/etit/proaut/mitarbeiter/rsrc/IROS12-switchableConstraints.pdf">this paper</a></p>

<blockquote>
  <p>The main idea behind the switchable constraints is that the topology of the factor graph that represents the pose graph SLAM problem, should be partially variable and subject to the optimization instead of being kept fixed. This way, edges representing outlier constraints can be removed from the graph during the optimization. This is achieved by augmenting the original problem and introducing an additional type of hidden variable: A switch variable is associated with each factor that could potentially represent an outlier. This additional variable acts as a multiplicative scaling factor on the information matrix associated with that constraint. Depending on the state of the switch variable (a value between 0 and 1), the resulting information matrix is either the original matrix (when the switch is equal to 1) or 0 (when the switch is 0) or something between both ends. Notice that if the switch variable is equal to 0, the associated constraint is completely removed and has no influence on the overall solution</p>
</blockquote>

<h4 id="dynamic-covariance-scaling--dcs">Dynamic Covariance Scaling  (D.C.S.)</h4>

<p>Dynamic Covariance Scaling was introduced in 2013 <a href="http://www2.informatik.uni-freiburg.de/~spinello/agarwalICRA13.pdf"> in this paper </a> as a closed-form solution to switch factors.</p>

<p>The DCS approach reduces the confidence of an observable by de-weighting the corresponding element in the information matrix by the scale factor</p>

<p align="center">
$$ s_{ij} = min(1,\frac{2\Phi}{\Phi + \mathcal{X}}) $$
</p>

<p>Ultimitely this is just the Geman-McClure weighting.</p>

<h4 id="max-mixtures--mm">Max-Mixtures  (M.M)</h4>

<p>Max-Mixtures was introduced in 2012 <a href="https://april.eecs.umich.edu/pdfs/olson2012rss.pdf"> in this paper </a>.</p>

<h4 id="realizing-reversing-recovering--rrr">Realizing, Reversing, Recovering  (R.R.R)</h4>

<h4 id="comparison">Comparison</h4>

<p>A good paper comparing SC to DCS to RRR is provided by <a href="https://www.tu-chemnitz.de/etit/proaut/forschung/rsrc/ICRA12-comparisonRobustSLAM.pdf">Sunderhauf</a></p>

<p>The robust back-end formulation known as <a href="https://www.tu-chemnitz.de/etit/proaut/mitarbeiter/rsrc/IROS12-switchableConstraints.pdf">Switchable constraints</a> was introduced by  Sunderhauf in 2012.</p>

<p><br /><br /></p>

<h1 id="noise-model-testing">Noise Model Testing</h1>

<h3 id="data-set">Data-set</h3>

<h4 id="clean-data">Clean Data</h4>
<p>The <a href="http://rvsn.csail.mit.edu/graphoptim/eolson-graphoptim2006.pdf">Manhattan world with 3500 nodes</a> is a common graph optimization test data-set. The truth ground trace is shown in the figure below.</p>

<p><img src="http://www.lucacarlone.com/images/M3500_eg2o.jpg" alt="Man3500" /></p>
<p align="center">
True Manhattan wold 3500 ground truth 
</p>

<h4 id="adding-faults">Adding faults</h4>

<p>Gaussian noise with standard deviation 0.3rad is added to the relative orientation measurements.</p>

<h3 id="testing">Testing</h3>

<p>Using the erronous data, each of the noise models above will be tested. For all weighting functions, Levenberg Marquardt is utilized with a kernal width of 10.</p>

<h4 id="l2-cost-function">$L^2$ Cost Function</h4>
<p>First we will test the traditional $L^2$ minimization. A video of the optimizer going through 100 iterations is shown below.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Tb-hwRqCqkU" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h3 id="m-estimators">M-Estimators</h3>

<h4 id="cauchy-cost-function">Cauchy Cost Function</h4>

<p>Next, we will test the Cauchy weighting function. A video of the optimizer going through 100 iterations is shown below.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/OtSKsQpCzlk" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h4 id="geman-mcclure-cost-function">Geman-McClure Cost Function</h4>

<p>Now, a video of the optimization process with the Geman-McClure cost function.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/W7d3hNfppYc" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h4 id="saturaded-cost-function">Saturaded Cost Function</h4>

<p>Applying the saturated cost function to the optimization process</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/w7BsSpoQHQk" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h4 id="tukey-cost-function">Tukey Cost Function</h4>

<p>Applying the tukey cost function to the optimization process</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/f7lK3frlVPk" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h4 id="tukey-cost-function-1">Tukey Cost Function</h4>

<p>The last M-Estimator that will be tested is the Welcsh weighting. A video of the optimization process is shown below.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/lXHhbv2hFpM" frameborder="0" allowfullscreen="" align="center"></iframe>
<p><br /><br /></p>

<h3 id="robust-graphical-methods">Robust Graphical Methods</h3>

